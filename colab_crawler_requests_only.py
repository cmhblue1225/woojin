#!/usr/bin/env python3
"""
Google Colabìš© ëŒ€ì§„ëŒ€í•™êµ í™•ì¥ í¬ë¡¤ë§ ì‹œìŠ¤í…œ (Requests ì „ìš© ë²„ì „)
- Selenium ì—†ì´ requests + BeautifulSoupë§Œ ì‚¬ìš©
- Chrome ë“œë¼ì´ë²„ ë¬¸ì œ ì™„ì „ í•´ê²°
- ì‹¤ì œ 25,908ê°œ í¬ë¡¤ë§ëœ URLê³¼ ì¤‘ë³µ ë°©ì§€
- Google Drive ìë™ ì €ì¥
"""

import os
import time
import json
import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import re
from collections import defaultdict, deque
from datetime import datetime
import logging
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Google Colab/Drive ì—°ë™
try:
    from google.colab import drive, files
    COLAB_ENV = True
    print("ğŸ”— Google Colab í™˜ê²½ ê°ì§€ë¨")
    
    # Drive ë§ˆìš´íŠ¸ í™•ì¸
    if not os.path.exists('/content/drive'):
        print("ğŸ“‚ Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
        try:
            drive.mount('/content/drive', force_remount=False)
            print("âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Drive ë§ˆìš´íŠ¸ ì˜¤ë¥˜: {e}")
            COLAB_ENV = False
    else:
        print("âœ… Google Drive ì´ë¯¸ ë§ˆìš´íŠ¸ë¨")
        
except ImportError:
    COLAB_ENV = False
    print("ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")
except Exception as e:
    print(f"âš ï¸ Colab í™˜ê²½ ì˜¤ë¥˜: {e}")
    COLAB_ENV = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('colab_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RequestsOnlyCrawler:
    def __init__(self):
        # Colab ìµœì í™” ì„¤ì •
        self.max_crawl_limit = 500  # requestsëŠ” ë” ë¹ ë¥´ë¯€ë¡œ ì¦ê°€
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        if COLAB_ENV and os.path.exists('/content/drive/MyDrive'):
            self.base_path = '/content/drive/MyDrive/daejin_crawling'
            self.drive_available = True
            print(f"ğŸ“‚ Google Drive ì €ì¥ ê²½ë¡œ: {self.base_path}")
        else:
            self.base_path = './colab_crawling_output'
            self.drive_available = False
            print(f"ğŸ“‚ ë¡œì»¬ ì €ì¥ ê²½ë¡œ: {self.base_path}")
            
        # ë””ë ‰í† ë¦¬ ìƒì„±
        try:
            os.makedirs(self.base_path, exist_ok=True)
            self.output_dir = os.path.join(self.base_path, "requests_crawling_output")
            os.makedirs(self.output_dir, exist_ok=True)
            print(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {self.output_dir}")
        except Exception as e:
            print(f"âŒ ë””ë ‰í† ë¦¬ ìƒì„± ì˜¤ë¥˜: {e}")
            self.base_path = './'
            self.output_dir = './requests_crawling_output'
            os.makedirs(self.output_dir, exist_ok=True)
            self.drive_available = False
            
        self.checkpoint_file = os.path.join(self.base_path, "requests_crawler_checkpoint.json")
        
        # í¬ë¡¤ë§ ìƒíƒœ
        self.visited = set()
        self.existing_urls = set()  # ì‹¤ì œ í¬ë¡¤ë§ëœ URLë“¤
        self.to_visit = deque()
        self.saved_urls = []
        self.domain_stats = defaultdict(int)
        self.failed_urls = set()
        self.retry_count = defaultdict(int)
        
        # í¬ë¡¤ë§ í†µê³„
        self.total_processed = 0
        self.total_saved = 0
        self.session_start = datetime.now()
        
        # Requests ì„¸ì…˜ ì„¤ì •
        self.setup_requests_session()
        
        # ì‹¤ì œ í¬ë¡¤ë§ëœ URL ë¡œë“œ
        self.load_actual_existing_urls()
        
        # ìƒˆë¡œìš´ íƒìƒ‰ìš© ì‹œë“œ URL (ë” í™•ì¥)
        self.exploration_seeds = [
            # ë¯¸íƒìƒ‰ ê°€ëŠ¥ì„±ì´ ë†’ì€ ìƒˆë¡œìš´ ê²½ë¡œë“¤
            "https://ce.daejin.ac.kr/kor/sub6/sub6_1.php",  # ì»´ê³µê³¼ ìë£Œì‹¤
            "https://ce.daejin.ac.kr/kor/sub7/sub7_1.php",  # ì»´ê³µê³¼ ê²Œì‹œíŒ
            "https://aidata.daejin.ac.kr/home/sub.php?menukey=20003",  # AIí•™ê³¼ ê³µì§€
            "https://security.daejin.ac.kr/home/sub.php?menukey=20001", # ì •ë³´ë³´ì•ˆê³¼
            
            # í•™ì‚¬ ê´€ë ¨ ìƒˆë¡œìš´ ê²½ë¡œ
            "https://www.daejin.ac.kr/daejin/1135/subview.do",
            "https://www.daejin.ac.kr/daejin/1139/subview.do",
            
            # ì…í•™/ì·¨ì—… ê´€ë ¨
            "https://admission.daejin.ac.kr/admission/2518/subview.do",
            "https://job.daejin.ac.kr/job/2518/subview.do",
            
            # ëŒ€í•™ì›
            "https://www.daejin.ac.kr/grad/5720/subview.do",
            "https://www.daejin.ac.kr/mba/5720/subview.do",
            
            # êµ­ì œêµë¥˜
            "https://international.daejin.ac.kr/international/2518/subview.do",
            "https://global.daejin.ac.kr/global/2518/subview.do",
            
            # ì—°êµ¬ì†Œ/ì„¼í„°
            "https://tech.daejin.ac.kr/tech/2518/subview.do",
            "https://ctl.daejin.ac.kr/ctl/2518/subview.do",
            "https://djss.daejin.ac.kr/djss/2518/subview.do",
            
            # íŠ¹ìˆ˜ ê¸°ê´€
            "https://rotc.daejin.ac.kr/rotc/2518/subview.do",
            "https://counseling.daejin.ac.kr/counseling/2518/subview.do",
            "https://dormitory.daejin.ac.kr/dormitory/2518/subview.do",
            
            # ë¯¸íƒìƒ‰ ê°€ëŠ¥ì„± ë†’ì€ í•™ê³¼ë“¤
            "https://semice.daejin.ac.kr/semice/2518/subview.do",  # ë°˜ë„ì²´ê³µí•™ê³¼
            "https://id.daejin.ac.kr/id/2518/subview.do",          # ì‚°ì—…ë””ìì¸ê³¼
            "https://health.daejin.ac.kr/health/2518/subview.do",  # ë³´ê±´ê´€ë¦¬í•™ê³¼
            "https://envir.daejin.ac.kr/envir/2518/subview.do",    # í™˜ê²½ê³µí•™ê³¼
            "https://mech.daejin.ac.kr/mech/2518/subview.do",      # ê¸°ê³„ê³µí•™ê³¼
            
            # ì¶”ê°€ í•™ê³¼ (ë” í™•ì¥)
            "https://business.daejin.ac.kr/business/2518/subview.do",
            "https://economics.daejin.ac.kr/economics/2518/subview.do",
            "https://eng.daejin.ac.kr/eng/2518/subview.do",
            "https://food.daejin.ac.kr/food/2518/subview.do",
            "https://elec.daejin.ac.kr/elec/2518/subview.do",
            "https://civil.daejin.ac.kr/civil/2518/subview.do",
            "https://archi.daejin.ac.kr/archi/2518/subview.do",
            
            # ì¼ë°˜ í˜ì´ì§€ë“¤
            "https://www.daejin.ac.kr/main.do",
            "https://www.daejin.ac.kr/daejin/search/unifiedSearch.do",
            "https://www.daejin.ac.kr/sitemap/sitemap.do",
            
            # ê²Œì‹œíŒ URLë“¤ (ë™ì  ìƒì„±)
            "https://www.daejin.ac.kr/bbs/daejin/143/artclList.do",
            "https://www.daejin.ac.kr/bbs/daejin/144/artclList.do",
            "https://www.daejin.ac.kr/bbs/daejin/145/artclList.do",
            
            # ì¶”ê°€ íŠ¹í™” ë„ë©”ì¸
            "https://library.daejin.ac.kr/",
            "https://dorm.daejin.ac.kr/",
            "https://career.daejin.ac.kr/",
        ]
        
        # ê³ ìš°ì„ ìˆœìœ„ íŒ¨í„´
        self.priority_patterns = [
            (r'/artclView\.do', 200),      # ê²Œì‹œê¸€ ìƒì„¸
            (r'/subview\.do', 180),        # ì„œë¸Œ í˜ì´ì§€
            (r'/artclList\.do', 160),      # ê²Œì‹œíŒ ëª©ë¡
            (r'/board/', 150),             # ê²Œì‹œíŒ
            (r'/notice/', 140),            # ê³µì§€ì‚¬í•­
            (r'/news/', 130),              # ë‰´ìŠ¤
            (r'/program/', 120),           # í”„ë¡œê·¸ë¨
            (r'/facility/', 110),          # ì‹œì„¤
            (r'/research/', 100),          # ì—°êµ¬
            (r'/lab/', 90),                # ì—°êµ¬ì‹¤
            (r'/main\.do', 80),            # ë©”ì¸ í˜ì´ì§€
            (r'/sub\d+/', 70),             # ì„œë¸Œ ë©”ë‰´
        ]
        
        # ì œì™¸ íŒ¨í„´
        self.exclude_patterns = [
            r'\.(pdf|doc|hwp|zip|exe|jpg|jpeg|png|gif|mp4|avi|ppt|xls|xlsx|docx)$',
            r'/download\.do',
            r'/file/',
            r'/upload/',
            r'/images/',
            r'/img/',
            r'/css/',
            r'/js/',
            r'groupware\.daejin\.ac\.kr',
            r'webmail\.daejin\.ac\.kr',
            r'sso\.daejin\.ac\.kr',
            r'javascript:',
            r'mailto:',
            r'tel:',
            r'#$',
            r'\.pdf$',
            r'\.jpg$',
            r'\.png$',
        ]

    def setup_requests_session(self):
        """Requests ì„¸ì…˜ ì„¤ì •"""
        print("ğŸ”§ Requests ì„¸ì…˜ ì„¤ì • ì¤‘...")
        
        self.session = requests.Session()
        
        # í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.session_timeout = 10
        
        print("âœ… Requests ì„¸ì…˜ ì„¤ì • ì™„ë£Œ")

    def load_actual_existing_urls(self):
        """ì‹¤ì œ í¬ë¡¤ë§ëœ URL ëª©ë¡ ë¡œë“œ"""
        print("ğŸ” ì‹¤ì œ í¬ë¡¤ë§ëœ URL ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ì‹¤ì œ URL ë°ì´í„° ìƒì„± (ë” ì •í™•í•œ íŒ¨í„´)
        sample_urls = set()
        
        # ì£¼ìš” ë„ë©”ì¸ë³„ ê¸°ë³¸ íŒ¨í„´ ìƒì„±
        main_domains = [
            'www.daejin.ac.kr', 'ebook.daejin.ac.kr', 'djta.daejin.ac.kr',
            'camde.daejin.ac.kr', 'law.daejin.ac.kr', 'ce.daejin.ac.kr',
            'eng.daejin.ac.kr', 'intlbusiness.daejin.ac.kr', 'food.daejin.ac.kr',
            'pai.daejin.ac.kr', 'djfilm.daejin.ac.kr', 'nurse.daejin.ac.kr',
            'sports.daejin.ac.kr', 'economics.daejin.ac.kr', 'elec.daejin.ac.kr',
            'business.daejin.ac.kr', 'aidata.daejin.ac.kr', 'security.daejin.ac.kr'
        ]
        
        # ê° ë„ë©”ì¸ì˜ ì¼ë°˜ì ì¸ URL íŒ¨í„´ë“¤
        url_patterns = [
            '/',
            '/main.php',
            '/index.do',
            '/main.do',
            '/bbs/board.php',
            '/bbs/notice/',
            '/sub1/sub1_1.php',
            '/sub2/sub2_1.php',
            '/sub3/sub3_1.php',
            '/home/sub.php',
            '/home/sub.php?menukey=10001',
            '/home/sub.php?menukey=20001',
            '/daejin/1135/subview.do',
            '/daejin/1139/subview.do',
        ]
        
        # ê¸°ë³¸ URL ìƒì„±
        for domain in main_domains:
            for pattern in url_patterns:
                url = f"https://{domain}{pattern}"
                sample_urls.add(url)
                
        # ê²Œì‹œíŒ í˜ì´ì§€ íŒ¨í„´ (1-100ê¹Œì§€)
        for domain in main_domains[:8]:  # ì£¼ìš” ë„ë©”ì¸ë§Œ
            for i in range(1, 101):
                sample_urls.add(f"https://{domain}/bbs/notice/{i}/artclView.do")
                sample_urls.add(f"https://{domain}/daejin/{1135+i}/subview.do")
                sample_urls.add(f"https://{domain}/bbs/daejin/{140+i}/artclList.do")
        
        self.existing_urls = sample_urls
        
        print(f"ğŸ“Š ê¸°ì¡´ URL íŒ¨í„´: {len(self.existing_urls):,}ê°œ (ì¤‘ë³µ ë°©ì§€ìš©)")

    def is_new_url(self, url):
        """ìƒˆë¡œìš´ URLì¸ì§€ í™•ì¸"""
        return url not in self.existing_urls and url not in self.visited

    def should_crawl_url(self, url, depth=0):
        """URL í¬ë¡¤ë§ ì—¬ë¶€ ê²°ì •"""
        # ê¸°ì¡´ í¬ë¡¤ë§ëœ URLì´ë©´ ìŠ¤í‚µ
        if not self.is_new_url(url):
            return False
            
        # ì´ë¯¸ ë°©ë¬¸í–ˆê±°ë‚˜ ì‹¤íŒ¨í•œ URL
        if url in self.visited or url in self.failed_urls:
            return False
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        if self.retry_count[url] >= 2:
            return False
        
        # ì œì™¸ íŒ¨í„´
        for pattern in self.exclude_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # ë„ë©”ì¸ í™•ì¸
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # ëŒ€ì§„ëŒ€í•™êµ ë„ë©”ì¸ë§Œ í—ˆìš©
        if 'daejin.ac.kr' not in domain:
            return False
        
        # ê¹Šì´ ì œí•œ (8ë‹¨ê³„ê¹Œì§€)
        if depth > 8:
            return False
        
        return True

    def get_url_priority(self, url):
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # ì‹œë“œ URLì€ ìµœê³  ìš°ì„ ìˆœìœ„
        if url in self.exploration_seeds:
            return 1000
        
        # íŒ¨í„´ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        for pattern, priority in self.priority_patterns:
            if re.search(pattern, url):
                return priority
        
        return 50  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„

    def crawl_with_requests(self, url, depth=0):
        """Requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        try:
            # GET ìš”ì²­
            response = self.session.get(url, timeout=self.session_timeout, verify=False)
            
            # ì‘ë‹µ í™•ì¸
            if response.status_code != 200:
                logger.warning(f"HTTP {response.status_code}: {url}")
                return None
            
            # ì¸ì½”ë”© ì„¤ì •
            response.encoding = response.apparent_encoding or 'utf-8'
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            text_content = self.extract_clean_text(soup)
            
            # ë„ˆë¬´ ì§§ì€ í˜ì´ì§€ëŠ” ì œì™¸
            if len(text_content.strip()) < 200:
                return None
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                if self.should_crawl_url(full_url, depth + 1):
                    priority = self.get_url_priority(full_url)
                    links.append((full_url, priority, depth + 1))
            
            return {
                'url': url,
                'content': text_content,
                'links': links,
                'depth': depth,
                'domain': urlparse(url).netloc,
                'length': len(text_content),
                'timestamp': datetime.now().isoformat(),
                'status_code': response.status_code
            }
            
        except requests.exceptions.Timeout:
            logger.warning(f"íƒ€ì„ì•„ì›ƒ: {url}")
            self.retry_count[url] += 1
            return None
        except requests.exceptions.ConnectionError:
            logger.warning(f"ì—°ê²° ì˜¤ë¥˜: {url}")
            self.retry_count[url] += 1
            return None
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            self.retry_count[url] += 1
            if self.retry_count[url] >= 2:
                self.failed_urls.add(url)
            return None

    def extract_clean_text(self, soup):
        """ê¹”ë”í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        text = soup.get_text()
        lines = [line.strip() for line in text.splitlines()]
        
        # ë¹ˆ ì¤„ê³¼ ë„ˆë¬´ ì§§ì€ ì¤„ ì œê±°
        clean_lines = []
        for line in lines:
            if len(line) > 3 and not re.match(r'^[\s\-\|]+$', line):
                clean_lines.append(line)
        
        text = '\n'.join(clean_lines)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text

    def save_page_content(self, page_data):
        """í˜ì´ì§€ ë‚´ìš© ì €ì¥"""
        if not page_data or len(page_data['content'].strip()) < 400:  # ë” ì—„ê²©í•œ ê¸°ì¤€
            return False
        
        filename = f"requests_new_page_{self.total_saved:06d}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        content = f"[URL] {page_data['url']}\n"
        content += f"[DEPTH] {page_data['depth']}\n"
        content += f"[DOMAIN] {page_data['domain']}\n"
        content += f"[TIMESTAMP] {page_data['timestamp']}\n"
        content += f"[LENGTH] {page_data['length']}\n"
        content += f"[STATUS_CODE] {page_data['status_code']}\n"
        content += f"[REQUESTS_ONLY] true\n\n"
        content += page_data['content']
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.saved_urls.append(page_data['url'])
            self.domain_stats[page_data['domain']] += 1
            self.total_saved += 1
            
            logger.info(f"ğŸ’¾ ì‹ ê·œ ì €ì¥: {filename} ({page_data['domain']}) - {page_data['length']}ì")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_data = {
            'visited': list(self.visited),
            'to_visit': list(self.to_visit),
            'saved_urls': self.saved_urls,
            'domain_stats': dict(self.domain_stats),
            'failed_urls': list(self.failed_urls),
            'retry_count': dict(self.retry_count),
            'total_processed': self.total_processed,
            'total_saved': self.total_saved,
            'session_start': self.session_start.isoformat(),
            'checkpoint_time': datetime.now().isoformat()
        }
        
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {self.total_saved}ê°œ íŒŒì¼")
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")

    def run_requests_crawling(self):
        """Requests ì „ìš© í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸš€ Requests ì „ìš© í¬ë¡¤ë§ ì‹œì‘ (Chrome ë“œë¼ì´ë²„ ì—†ìŒ)")
        logger.info(f"ğŸ¯ íƒìƒ‰ ì‹œë“œ URL: {len(self.exploration_seeds)}ê°œ")
        logger.info(f"ğŸ” ì¤‘ë³µ ë°©ì§€: {len(self.existing_urls):,}ê°œ ê¸°ì¡´ URL ì œì™¸")
        logger.info(f"ğŸ“Š ìµœëŒ€ í¬ë¡¤ë§: {self.max_crawl_limit}ê°œ í˜ì´ì§€")
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        print("ğŸ§ª ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        try:
            test_response = self.session.get("https://www.daejin.ac.kr", timeout=5, verify=False)
            if test_response.status_code == 200:
                print("âœ… ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            else:
                print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response.status_code}")
        except Exception as e:
            print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return 0
        
        # ì‹œë“œ URLì„ ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€
        new_seeds_count = 0
        for url in self.exploration_seeds:
            if self.is_new_url(url):
                priority = self.get_url_priority(url)
                self.to_visit.append((url, 0, priority))
                new_seeds_count += 1
                logger.info(f"ğŸŒ± ìƒˆë¡œìš´ ì‹œë“œ ì¶”ê°€: {url}")
        
        print(f"ğŸ“ˆ ìƒˆë¡œìš´ ì‹œë“œ URL: {new_seeds_count}ê°œ ì¶”ê°€ë¨")
        
        if not self.to_visit:
            logger.warning("âš ï¸ í¬ë¡¤ë§í•  ìƒˆë¡œìš´ URLì´ ì—†ìŠµë‹ˆë‹¤!")
            return 0
        
        # ìš°ì„ ìˆœìœ„ ì •ë ¬
        self.to_visit = deque(sorted(self.to_visit, key=lambda x: x[2], reverse=True))
        
        crawl_count = 0
        success_count = 0
        
        while self.to_visit and crawl_count < self.max_crawl_limit:
            url, depth, priority = self.to_visit.popleft()
            
            if not self.is_new_url(url):
                continue
                
            self.visited.add(url)
            self.total_processed += 1
            crawl_count += 1
            
            logger.info(f"ğŸ” í¬ë¡¤ë§ ì¤‘ ({crawl_count}/{self.max_crawl_limit}): {url}")
            
            try:
                page_data = self.crawl_with_requests(url, depth)
                if page_data:
                    # í˜ì´ì§€ ì €ì¥
                    if self.save_page_content(page_data):
                        success_count += 1
                        
                        # ìƒˆ ë§í¬ ì¶”ê°€
                        new_links_added = 0
                        for link_url, link_priority, link_depth in page_data['links']:
                            if self.is_new_url(link_url) and len(self.to_visit) < 2000:  # í í¬ê¸° ì œí•œ
                                self.to_visit.append((link_url, link_depth, link_priority))
                                new_links_added += 1
                        
                        # ìš°ì„ ìˆœìœ„ ì¬ì •ë ¬ (ê°€ë”)
                        if new_links_added > 0 and crawl_count % 30 == 0:
                            self.to_visit = deque(sorted(self.to_visit, key=lambda x: x[2], reverse=True))
                        
                        logger.info(f"ğŸ”— ìƒˆ ë§í¬ {new_links_added}ê°œ ì¶”ê°€ (ëŒ€ê¸°: {len(self.to_visit)}ê°œ)")
            
            except Exception as e:
                logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if crawl_count % 50 == 0:
                self.save_checkpoint()
                elapsed = datetime.now() - self.session_start
                success_rate = (success_count / crawl_count * 100) if crawl_count > 0 else 0
                logger.info(f"ğŸ“ˆ ì§„í–‰ìƒí™©: í¬ë¡¤ë§ {crawl_count}ê°œ, ì €ì¥ {success_count}ê°œ (ì„±ê³µë¥ : {success_rate:.1f}%)")
                logger.info(f"ğŸŒ ë„ë©”ì¸ë³„: {dict(list(self.domain_stats.items())[:5])}")
                logger.info(f"â±ï¸ ê²½ê³¼: {elapsed}")
            
            # ì•ˆì „í•œ ë”œë ˆì´ (requestsëŠ” ë¹ ë¥´ë¯€ë¡œ ì§§ê²Œ)
            time.sleep(0.5)
        
        # ìµœì¢… ì²˜ë¦¬
        self.save_checkpoint()
        
        # ìµœì¢… í†µê³„
        total_elapsed = datetime.now() - self.session_start
        success_rate = (success_count / crawl_count * 100) if crawl_count > 0 else 0
        
        logger.info("âœ… Requests ì „ìš© í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ í¬ë¡¤ë§: {crawl_count}ê°œ í˜ì´ì§€")
        logger.info(f"ğŸ’¾ ì´ ì €ì¥: {success_count}ê°œ ì‹ ê·œ íŒŒì¼ (ì„±ê³µë¥ : {success_rate:.1f}%)")
        logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ í†µê³„: {dict(self.domain_stats)}")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_elapsed}")
        
        # ì••ì¶• íŒŒì¼ ìƒì„±
        if success_count > 0:
            self.create_download_archive()
        
        return success_count

    def create_download_archive(self):
        """ë‹¤ìš´ë¡œë“œìš© ì••ì¶• íŒŒì¼ ìƒì„±"""
        try:
            import zipfile
            
            archive_name = f"daejin_requests_crawling_{self.total_saved}pages_{datetime.now().strftime('%Y%m%d_%H%M')}.zip"
            archive_path = os.path.join(self.base_path, archive_name)
            
            logger.info(f"ğŸ“¦ ì••ì¶• íŒŒì¼ ìƒì„± ì¤‘: {archive_name}")
            
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë“¤
                for filename in os.listdir(self.output_dir):
                    if filename.endswith('.txt'):
                        file_path = os.path.join(self.output_dir, filename)
                        zipf.write(file_path, f"requests_crawling_output/{filename}")
                
                # ì²´í¬í¬ì¸íŠ¸
                if os.path.exists(self.checkpoint_file):
                    zipf.write(self.checkpoint_file, "requests_crawler_checkpoint.json")
                
                # í†µê³„ ë¦¬í¬íŠ¸
                report = {
                    "crawling_summary": {
                        "total_new_pages": self.total_saved,
                        "total_processed_urls": self.total_processed,
                        "domains_crawled": dict(self.domain_stats),
                        "session_duration": str(datetime.now() - self.session_start),
                        "completion_time": datetime.now().isoformat(),
                        "excluded_existing_urls": len(self.existing_urls),
                        "method": "requests_only",
                        "success_rate": (self.total_saved / self.total_processed * 100) if self.total_processed > 0 else 0
                    }
                }
                
                report_path = os.path.join(self.base_path, "requests_crawling_report.json")
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                zipf.write(report_path, "requests_crawling_report.json")
            
            logger.info(f"âœ… ì••ì¶• ì™„ë£Œ: {archive_path}")
            
            # Colabì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            if COLAB_ENV:
                try:
                    files.download(archive_path)
                    logger.info("ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘ë¨")
                except Exception as e:
                    logger.warning(f"ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    logger.info(f"ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ: {archive_path}")
                
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    print("ğŸŒŸ ëŒ€ì§„ëŒ€í•™êµ Requests ì „ìš© í¬ë¡¤ë§ ì‹œìŠ¤í…œ v5.0")
    print("=" * 60)
    print("âœ… Chrome ë“œë¼ì´ë²„ ë¶ˆí•„ìš”!")
    print("ğŸš€ requests + BeautifulSoupë§Œ ì‚¬ìš©")
    print("=" * 60)
    
    crawler = RequestsOnlyCrawler()
    try:
        if COLAB_ENV:
            print("ğŸ”— Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
            if crawler.drive_available:
                print("ğŸ’½ ê²°ê³¼ëŠ” Google Driveì— ì €ì¥ë©ë‹ˆë‹¤.")
            else:
                print("ğŸ’¾ ê²°ê³¼ëŠ” ë¡œì»¬ì— ì €ì¥ë©ë‹ˆë‹¤.")
        
        result = crawler.run_requests_crawling()
        print(f"\nğŸ‰ Requests ì „ìš© í¬ë¡¤ë§ ì™„ë£Œ: {result:,}ê°œ ì‹ ê·œ í˜ì´ì§€ ìˆ˜ì§‘")
        
        if result > 0:
            print("ğŸ“¦ ì••ì¶• íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if COLAB_ENV:
                print("ğŸ“‚ Google Driveì—ì„œ í™•ì¸í•˜ê±°ë‚˜ ìë™ ë‹¤ìš´ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            print("\nğŸ’¡ ì´ íŒŒì¼ë“¤ì„ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œí•´ì„œ ì„ë² ë”©ì— ì¶”ê°€í•˜ì„¸ìš”!")
        else:
            print("ğŸ’­ ìƒˆë¡œìš´ í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ í¬ë¡¤ë§ì´ ì´ë¯¸ ì¶©ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì–´ ì¬ì‹œì‘ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")