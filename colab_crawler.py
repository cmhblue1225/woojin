#!/usr/bin/env python3
"""
Google Colabìš© ëŒ€ì§„ëŒ€í•™êµ í™•ì¥ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
- ê¸°ì¡´ 20,231ê°œ í˜ì´ì§€ì™€ ì¤‘ë³µ ë°©ì§€
- Google Drive ìë™ ì €ì¥
- ì¤‘ë‹¨ í›„ ì¬ì‹œì‘ ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ
"""

import os
import time
import json
import asyncio
import aiohttp
import requests
from urllib.parse import urlparse, urljoin, parse_qs
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, deque
import hashlib
from datetime import datetime
import logging
import signal
import sys

# Google Colab/Drive ì—°ë™
try:
    from google.colab import drive, files
    COLAB_ENV = True
    print("ğŸ”— Google Colab í™˜ê²½ ê°ì§€ë¨")
except ImportError:
    COLAB_ENV = False
    print("ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('colab_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ColabAdvancedCrawler:
    def __init__(self):
        # Colab ìµœì í™” ì„¤ì •
        self.max_workers = 8 if COLAB_ENV else 4
        self.max_concurrent_requests = 30 if COLAB_ENV else 15
        self.max_selenium_instances = 4 if COLAB_ENV else 2
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        if COLAB_ENV:
            # Google Drive ë§ˆìš´íŠ¸
            drive.mount('/content/drive')
            self.base_path = '/content/drive/MyDrive/daejin_crawling'
            os.makedirs(self.base_path, exist_ok=True)
        else:
            self.base_path = './colab_crawling_output'
            
        self.output_dir = os.path.join(self.base_path, "new_crawling_output")
        self.checkpoint_file = os.path.join(self.base_path, "colab_crawler_checkpoint.json")
        self.visited_urls_file = os.path.join(self.base_path, "existing_urls.json")
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        # í¬ë¡¤ë§ ìƒíƒœ
        self.visited = set()
        self.existing_urls = set()  # ê¸°ì¡´ í¬ë¡¤ë§ëœ URLë“¤
        self.to_visit = deque()
        self.priority_queue = deque()
        self.saved_texts = []
        self.saved_urls = []
        self.url_depths = {}
        self.domain_stats = defaultdict(int)
        self.failed_urls = set()
        self.retry_count = defaultdict(int)
        
        # í¬ë¡¤ë§ í†µê³„
        self.start_time = datetime.now()
        self.total_processed = 0
        self.total_saved = 0
        self.session_start = datetime.now()
        
        # ìƒíƒœ ì €ì¥ í”Œë˜ê·¸
        self.should_stop = False
        self.checkpoint_interval = 50  # 50ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        # ì‹ í˜¸ ì²˜ë¦¬ ì„¤ì •
        if not COLAB_ENV:
            signal.signal(signal.SIGINT, self.signal_handler)
            signal.signal(signal.SIGTERM, self.signal_handler)
        
        # ê¸°ì¡´ URL ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)
        self.load_existing_urls()
        
        # í™•ì¥ëœ ì‹œë“œ URL (ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ìƒˆë¡œìš´ ê²½ë¡œ ì¤‘ì‹¬)
        self.new_exploration_seeds = [
            # ê¹Šì€ íƒìƒ‰ìš© ìƒˆë¡œìš´ ì‹œë“œ
            "https://www.daejin.ac.kr/sitemap/sitemap.do",  # ì‚¬ì´íŠ¸ë§µ
            "https://www.daejin.ac.kr/daejin/search/unifiedSearch.do",  # í†µí•©ê²€ìƒ‰
            
            # ê° í•™ê³¼ì˜ ê¹Šì€ ë©”ë‰´ë“¤
            "https://ce.daejin.ac.kr/sub6/sub6_1.php",  # ì»´ê³µê³¼ ìë£Œì‹¤
            "https://ce.daejin.ac.kr/sub7/sub7_1.php",  # ì»´ê³µê³¼ ì»¤ë®¤ë‹ˆí‹°
            "https://aidata.daejin.ac.kr/board/list",    # AIë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ê³¼ ê²Œì‹œíŒ
            "https://security.daejin.ac.kr/board/list",  # ì •ë³´ë³´ì•ˆê³¼ ê²Œì‹œíŒ
            
            # ì˜ì–´ ë²„ì „ ì‚¬ì´íŠ¸ë“¤
            "https://eng.daejin.ac.kr/sub/sub6_1.php",
            "https://history.daejin.ac.kr/sub/sub5_1.php",
            "https://literature.daejin.ac.kr/sub/sub4_1.php",
            
            # ëŒ€í•™ì› ë° íŠ¹ìˆ˜ ê³¼ì •
            "https://www.daejin.ac.kr/grad/index.do",
            "https://www.daejin.ac.kr/mba/index.do",
            "https://www.daejin.ac.kr/special/index.do",
            
            # ì—°êµ¬ê¸°ê´€
            "https://www.daejin.ac.kr/research/index.do",
            "https://tech.daejin.ac.kr/tech/index.do",
            
            # ë¶€ì„¤ê¸°ê´€ë“¤ì˜ ê¹Šì€ ë©”ë‰´
            "https://admission.daejin.ac.kr/admission/notice/list.do",
            "https://job.daejin.ac.kr/job/notice/list.do",
            "https://ctl.daejin.ac.kr/ctl/notice/list.do",
            "https://international.daejin.ac.kr/international/notice/list.do",
            
            # ëª¨ë°”ì¼ ë²„ì „ (ë‹¤ë¥¸ ì •ë³´ ì œê³µ ê°€ëŠ¥)
            "https://m.daejin.ac.kr/",
            
            # RSS ë° API ì—”ë“œí¬ì¸íŠ¸ë“¤
            "https://www.daejin.ac.kr/daejin/rss.do",
            
            # íŠ¹ë³„ í”„ë¡œê·¸ë¨ë“¤
            "https://rotc.daejin.ac.kr/rotc/program/list.do",
            "https://counseling.daejin.ac.kr/counseling/program/list.do",
            
            # ë„ì„œê´€ì˜ ê¹Šì€ ë©”ë‰´ë“¤
            "https://library.daejin.ac.kr/search/search.mir",
            "https://library.daejin.ac.kr/info/info.mir",
            
            # ê¸°ìˆ™ì‚¬ ìƒì„¸ ì •ë³´
            "https://dormitory.daejin.ac.kr/dormitory/facility/list.do",
            "https://dormitory.daejin.ac.kr/dormitory/notice/list.do",
        ]
        
        # ìƒˆë¡œìš´ íƒìƒ‰ ì „ëµ (ê¸°ì¡´ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” ì˜ì—­ ì§‘ì¤‘)
        self.new_exploration_strategies = {
            # ë©”ì¸ ì‚¬ì´íŠ¸ì˜ ë¯¸íƒìƒ‰ ì˜ì—­ ì§‘ì¤‘
            'www.daejin.ac.kr': {'max_depth': 60, 'priority': 1000},
            
            # ê¸°ì¡´ì— ìƒëŒ€ì ìœ¼ë¡œ ì ê²Œ í¬ë¡¤ë§ëœ ë„ë©”ì¸ë“¤ ìš°ì„ 
            'semice.daejin.ac.kr': {'max_depth': 50, 'priority': 950},  # ë°˜ë„ì²´ê³µí•™ê³¼
            'id.daejin.ac.kr': {'max_depth': 50, 'priority': 940},      # ì‚°ì—…ë””ìì¸ê³¼
            'food.daejin.ac.kr': {'max_depth': 45, 'priority': 930},    # ì‹í’ˆì˜ì–‘í•™ê³¼
            'health.daejin.ac.kr': {'max_depth': 45, 'priority': 920},  # ë³´ê±´ê´€ë¦¬í•™ê³¼
            'envir.daejin.ac.kr': {'max_depth': 45, 'priority': 910},   # í™˜ê²½ê³µí•™ê³¼
            'mech.daejin.ac.kr': {'max_depth': 45, 'priority': 900},    # ê¸°ê³„ê³µí•™ê³¼
            'child.daejin.ac.kr': {'max_depth': 40, 'priority': 890},   # ì•„ë™í•™ê³¼
            'welfare.daejin.ac.kr': {'max_depth': 40, 'priority': 880}, # ì‚¬íšŒë³µì§€í•™ê³¼
            'camde.daejin.ac.kr': {'max_depth': 40, 'priority': 870},   # ë¬¸í™”ì½˜í…ì¸ í•™ê³¼
            'korean.daejin.ac.kr': {'max_depth': 40, 'priority': 860},  # í•œêµ­ì–´êµìœ¡ê³¼
            
            # ìƒˆë¡œìš´ íŒ¨í„´ íƒìƒ‰
            'default': {'max_depth': 35, 'priority': 500},
            
            # ë„ì„œê´€ì€ ì—¬ì „íˆ ì œí•œ
            'library.daejin.ac.kr': {'max_depth': 8, 'priority': 200},
        }
        
        # ìƒˆë¡œìš´ ê³ ìš°ì„ ìˆœìœ„ íŒ¨í„´ (ê¸°ì¡´ê³¼ ë‹¤ë¥¸ íŒ¨í„´ ì¶”ê°€)
        self.new_priority_patterns = [
            (r'/program/', 200),                        # í”„ë¡œê·¸ë¨ ì •ë³´
            (r'/facility/', 190),                       # ì‹œì„¤ ì •ë³´
            (r'/equipment/', 180),                      # ì¥ë¹„/ì‹œì„¤
            (r'/lab/', 170),                           # ì—°êµ¬ì‹¤ ì •ë³´
            (r'/research/', 160),                      # ì—°êµ¬ ê´€ë ¨
            (r'/publication/', 150),                   # ì¶œíŒë¬¼/ë…¼ë¬¸
            (r'/seminar/', 140),                       # ì„¸ë¯¸ë‚˜/íŠ¹ê°•
            (r'/conference/', 135),                    # í•™ìˆ ëŒ€íšŒ
            (r'/workshop/', 130),                      # ì›Œí¬ìƒµ
            (r'/competition/', 125),                   # ê²½ì§„ëŒ€íšŒ
            (r'/award/', 120),                         # ìˆ˜ìƒë‚´ì—­
            (r'/achievement/', 115),                   # ì„±ê³¼/ì—…ì 
            (r'/alumni/', 110),                        # ë™ë¬¸/ì¡¸ì—…ìƒ
            (r'/industry/', 105),                      # ì‚°í•™í˜‘ë ¥
            (r'/cooperation/', 100),                   # í˜‘ë ¥ì‚¬ì—…
            (r'/exchange/', 95),                       # êµí™˜/êµë¥˜
            (r'/international/', 90),                  # êµ­ì œí™”
            (r'/global/', 85),                         # ê¸€ë¡œë²Œ
            (r'/partnership/', 80),                    # íŒŒíŠ¸ë„ˆì‹­
            (r'/mou/', 75),                           # ì—…ë¬´í˜‘ì•½
        ]
        
        # ê¸°ì¡´ ì œì™¸ íŒ¨í„´ì— ìƒˆë¡œìš´ íŒ¨í„´ ì¶”ê°€
        self.exclude_patterns = [
            r'\\.(pdf|doc|hwp|zip|exe|jpg|png|gif|mp4|avi|ppt|xls)$',
            r'/download\\.do',
            r'/file/',
            r'/upload/',
            r'groupware\\.daejin\\.ac\\.kr',
            r'webmail\\.daejin\\.ac\\.kr',
            r'sso\\.daejin\\.ac\\.kr/login',
            r'#none$',
            r'#this$',
            r'javascript:',
            r'mailto:',
            r'tel:',
            r'/admin/',           # ê´€ë¦¬ì í˜ì´ì§€
            r'/private/',         # ë¹„ê³µê°œ í˜ì´ì§€
            r'/temp/',           # ì„ì‹œ í˜ì´ì§€
            r'/test/',           # í…ŒìŠ¤íŠ¸ í˜ì´ì§€
        ]
        
        # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ ì‹œë„
        self.load_checkpoint()

    def load_existing_urls(self):
        """ê¸°ì¡´ í¬ë¡¤ë§ëœ URL ëª©ë¡ ë¡œë“œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€"""
        logger.info("ğŸ” ê¸°ì¡´ í¬ë¡¤ë§ URL ë¶„ì„ ì¤‘...")
        
        # ê¸°ì¡´ í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ URL ì¶”ì¶œ (ë¡œì»¬ì—ì„œ ìƒì„±í•œ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
        existing_crawling_paths = [
            "/Users/minhyuk/Desktop/ìš°ì§„ë´‡/crawlingTest/unlimited_crawling_output",
            "/Users/minhyuk/Desktop/ìš°ì§„ë´‡/crawlingTest/enhanced_output",
            "/Users/minhyuk/Desktop/ìš°ì§„ë´‡/crawlingTest/enhanced_strategic_output"
        ]
        
        url_count = 0
        for path in existing_crawling_paths:
            if os.path.exists(path):
                try:
                    for filename in os.listdir(path):
                        if filename.endswith('.txt'):
                            filepath = os.path.join(path, filename)
                            with open(filepath, 'r', encoding='utf-8') as f:
                                lines = f.readlines()
                                if lines and lines[0].startswith('[URL]'):
                                    url = lines[0].replace('[URL]', '').strip()
                                    self.existing_urls.add(url)
                                    url_count += 1
                except Exception as e:
                    logger.warning(f"ê¸°ì¡´ URL ë¡œë“œ ì˜¤ë¥˜ {path}: {e}")
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ visited URL ë¡œë“œ
        if os.path.exists(self.visited_urls_file):
            try:
                with open(self.visited_urls_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    additional_urls = set(data.get('urls', []))
                    self.existing_urls.update(additional_urls)
                    url_count += len(additional_urls)
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ URL íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        logger.info(f"ğŸ“Š ê¸°ì¡´ í¬ë¡¤ë§ URL: {url_count:,}ê°œ (ì¤‘ë³µ ì œê±° í›„: {len(self.existing_urls):,}ê°œ)")
        
        # ê¸°ì¡´ URLì„ íŒŒì¼ë¡œ ì €ì¥ (ë°±ì—… ë° ê³µìœ ìš©)
        try:
            with open(self.visited_urls_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'total_count': len(self.existing_urls),
                    'last_updated': datetime.now().isoformat(),
                    'urls': list(self.existing_urls)
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ URL ì €ì¥ ì˜¤ë¥˜: {e}")

    def signal_handler(self, signum, frame):
        """ì‹ í˜¸ ì²˜ë¦¬ (Ctrl+C ë“±)"""
        logger.info(f"\\nâš ï¸ ì‹ í˜¸ {signum} ìˆ˜ì‹ . ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘...")
        self.should_stop = True
        self.save_checkpoint()

    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_data = {
            'visited': list(self.visited),
            'to_visit': list(self.to_visit),
            'priority_queue': list(self.priority_queue),
            'saved_urls': self.saved_urls,
            'url_depths': self.url_depths,
            'domain_stats': dict(self.domain_stats),
            'failed_urls': list(self.failed_urls),
            'retry_count': dict(self.retry_count),
            'total_processed': self.total_processed,
            'total_saved': self.total_saved,
            'session_start': self.session_start.isoformat(),
            'checkpoint_time': datetime.now().isoformat(),
            'existing_urls_count': len(self.existing_urls)
        }
        
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {self.total_saved}ê°œ ì‹ ê·œ íŒŒì¼")
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")

    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if not os.path.exists(self.checkpoint_file):
            logger.info("ğŸ†• ìƒˆë¡œìš´ í™•ì¥ í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘")
            return
        
        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            self.visited = set(checkpoint_data.get('visited', []))
            self.to_visit = deque(checkpoint_data.get('to_visit', []))
            self.priority_queue = deque(checkpoint_data.get('priority_queue', []))
            self.saved_urls = checkpoint_data.get('saved_urls', [])
            self.url_depths = checkpoint_data.get('url_depths', {})
            self.domain_stats = defaultdict(int, checkpoint_data.get('domain_stats', {}))
            self.failed_urls = set(checkpoint_data.get('failed_urls', []))
            self.retry_count = defaultdict(int, checkpoint_data.get('retry_count', {}))
            self.total_processed = checkpoint_data.get('total_processed', 0)
            self.total_saved = checkpoint_data.get('total_saved', 0)
            
            if 'session_start' in checkpoint_data:
                self.session_start = datetime.fromisoformat(checkpoint_data['session_start'])
            
            logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {self.total_saved}ê°œ ì‹ ê·œ íŒŒì¼, {len(self.visited)}ê°œ ë°©ë¬¸")
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            logger.info("ğŸ†• ìƒˆë¡œìš´ ì„¸ì…˜ìœ¼ë¡œ ì‹œì‘")

    def is_new_url(self, url):
        """ìƒˆë¡œìš´ URLì¸ì§€ í™•ì¸ (ê¸°ì¡´ í¬ë¡¤ë§ê³¼ ì¤‘ë³µ ë°©ì§€)"""
        return url not in self.existing_urls and url not in self.visited

    def should_crawl_url(self, url, depth):
        """URL í¬ë¡¤ë§ ì—¬ë¶€ ê²°ì • (ê¸°ì¡´ ë¡œì§ + ì¤‘ë³µ ë°©ì§€)"""
        # ê¸°ì¡´ í¬ë¡¤ë§ëœ URLì´ë©´ ìŠ¤í‚µ
        if not self.is_new_url(url):
            return False
            
        # ì´ë¯¸ ë°©ë¬¸í–ˆê±°ë‚˜ ì‹¤íŒ¨í•œ URL
        if url in self.visited or url in self.failed_urls:
            return False
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        if self.retry_count[url] >= 3:
            return False
        
        # ì œì™¸ íŒ¨í„´
        for pattern in self.exclude_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # ë„ë©”ì¸ í™•ì¸
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # ëŒ€ì§„ëŒ€í•™êµ ë„ë©”ì¸ë§Œ í—ˆìš©
        if 'daejin.ac.kr' not in domain:
            return False
        
        # ë„ë©”ì¸ë³„ ê¹Šì´ ì œí•œ
        strategy = self.new_exploration_strategies.get(domain, self.new_exploration_strategies['default'])
        if depth > strategy['max_depth']:
            return False
        
        return True

    def get_url_priority(self, url):
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # ì‹œë“œ URLì€ ìµœê³  ìš°ì„ ìˆœìœ„
        if url in self.new_exploration_seeds:
            return 2000
        
        # ìƒˆë¡œìš´ íŒ¨í„´ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        for pattern, priority in self.new_priority_patterns:
            if re.search(pattern, url):
                return priority + 1000  # ë² ì´ìŠ¤ ìš°ì„ ìˆœìœ„ ì¶”ê°€
        
        # ë„ë©”ì¸ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        parsed = urlparse(url)
        domain = parsed.netloc
        strategy = self.new_exploration_strategies.get(domain, self.new_exploration_strategies['default'])
        
        return strategy['priority']

    async def crawl_with_selenium(self, url, depth=0):
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§ (Colab ìµœì í™”)"""
        driver = None
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-extensions')
            options.add_argument('--disable-plugins')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
            
            # Colab í™˜ê²½ì—ì„œëŠ” Chrome ê²½ë¡œ ëª…ì‹œ
            if COLAB_ENV:
                options.binary_location = '/usr/bin/google-chrome'
                
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(25)
            
            driver.get(url)
            time.sleep(2)  # ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            content = driver.page_source
            soup = BeautifulSoup(content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            text_content = self.extract_clean_text(soup)
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                if self.should_crawl_url(full_url, depth + 1):
                    priority = self.get_url_priority(full_url)
                    links.append((full_url, priority, depth + 1))
            
            driver.quit()
            
            return {
                'url': url,
                'content': text_content,
                'links': links,
                'depth': depth,
                'domain': urlparse(url).netloc,
                'length': len(text_content),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Selenium í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            self.retry_count[url] += 1
            if self.retry_count[url] >= 3:
                self.failed_urls.add(url)
            
            if driver:
                try:
                    driver.quit()
                except:
                    pass
            return None

    def extract_clean_text(self, soup):
        """ê¹”ë”í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        text = soup.get_text()
        lines = [line.strip() for line in text.splitlines()]
        text = '\\n'.join(line for line in lines if line and len(line) > 2)
        
        return text

    def save_page_content(self, page_data):
        """í˜ì´ì§€ ë‚´ìš© ì €ì¥"""
        if not page_data or len(page_data['content'].strip()) < 200:  # ë” ì—„ê²©í•œ ê¸°ì¤€
            return False
        
        filename = f"new_page_{self.total_saved:06d}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        content = f"[URL] {page_data['url']}\\n"
        content += f"[DEPTH] {page_data['depth']}\\n"
        content += f"[DOMAIN] {page_data['domain']}\\n"
        content += f"[TIMESTAMP] {page_data['timestamp']}\\n"
        content += f"[LENGTH] {page_data['length']}\\n"
        content += f"[NEW_CRAWLING] true\\n\\n"  # ì‹ ê·œ í¬ë¡¤ë§ í‘œì‹œ
        content += page_data['content']
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.saved_urls.append(page_data['url'])
            self.domain_stats[page_data['domain']] += 1
            self.total_saved += 1
            
            logger.info(f"ğŸ’¾ ì‹ ê·œ ì €ì¥: {filename} ({page_data['domain']}) - {page_data['length']}ì")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    async def run_extended_crawling(self):
        """í™•ì¥ í¬ë¡¤ë§ ì‹¤í–‰ (ê¸°ì¡´ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ í˜ì´ì§€ë§Œ)"""
        logger.info("ğŸš€ í™•ì¥ í¬ë¡¤ë§ ì‹œì‘ (ê¸°ì¡´ 20,231ê°œ í˜ì´ì§€ì™€ ì¤‘ë³µ ë°©ì§€)")
        logger.info(f"ğŸ¯ ìƒˆë¡œìš´ ì‹œë“œ URL: {len(self.new_exploration_seeds)}ê°œ")
        logger.info(f"ğŸ“Š ê¸°ì¡´ ì§„í–‰: {self.total_saved}ê°œ ì‹ ê·œ ì €ì¥ë¨")
        logger.info(f"ğŸ” ì¤‘ë³µ ë°©ì§€: {len(self.existing_urls):,}ê°œ ê¸°ì¡´ URL ì œì™¸")
        
        # ìƒˆë¡œìš´ ì‹œë“œ URL ì¶”ê°€ (ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ê²ƒë§Œ)
        for url in self.new_exploration_seeds:
            if self.is_new_url(url):
                priority = self.get_url_priority(url)
                self.priority_queue.append((url, 0, priority))
                logger.info(f"ğŸŒ± ìƒˆë¡œìš´ ì‹œë“œ ì¶”ê°€: {url}")
        
        with ThreadPoolExecutor(max_workers=self.max_selenium_instances) as executor:
            while not self.should_stop and (self.priority_queue or self.to_visit):
                # ìš°ì„ ìˆœìœ„ íì—ì„œ ì²˜ë¦¬í•  URL ì„ íƒ
                current_batch = []
                
                # ìš°ì„ ìˆœìœ„ í ìš°ì„  ì²˜ë¦¬
                while self.priority_queue and len(current_batch) < self.max_selenium_instances:
                    url, depth, priority = self.priority_queue.popleft()
                    if self.is_new_url(url):
                        current_batch.append((url, depth, priority))
                
                # ì¼ë°˜ íì—ì„œ ì¶”ê°€
                while self.to_visit and len(current_batch) < self.max_selenium_instances:
                    url, depth, priority = self.to_visit.popleft()
                    if self.is_new_url(url):
                        current_batch.append((url, depth, priority))
                
                if not current_batch:
                    logger.info("ğŸ“ ëª¨ë“  ì‹ ê·œ URL ì²˜ë¦¬ ì™„ë£Œ")
                    break
                
                # ë³‘ë ¬ ì²˜ë¦¬
                futures = []
                for url, depth, priority in current_batch:
                    if self.is_new_url(url):
                        future = executor.submit(asyncio.run, self.crawl_with_selenium(url, depth))
                        futures.append(future)
                        self.visited.add(url)
                        self.total_processed += 1
                
                # ê²°ê³¼ ì²˜ë¦¬
                for future in as_completed(futures):
                    try:
                        page_data = future.result()
                        if page_data:
                            # í˜ì´ì§€ ì €ì¥
                            self.save_page_content(page_data)
                            
                            # ìƒˆ ë§í¬ë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¶„ë¥˜ (ì‹ ê·œë§Œ)
                            for link_url, link_priority, link_depth in page_data['links']:
                                if self.is_new_url(link_url) and link_url not in self.failed_urls:
                                    if link_priority >= 1000:  # ë†’ì€ ìš°ì„ ìˆœìœ„
                                        self.priority_queue.append((link_url, link_depth, link_priority))
                                    else:  # ì¼ë°˜ ìš°ì„ ìˆœìœ„
                                        self.to_visit.append((link_url, link_depth, link_priority))
                    
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
                # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if self.total_saved % self.checkpoint_interval == 0 and self.total_saved > 0:
                    self.save_checkpoint()
                    
                # ì£¼ê¸°ì  ìƒíƒœ ë³´ê³ 
                if self.total_processed % 25 == 0:
                    elapsed = datetime.now() - self.session_start
                    logger.info(f"ğŸ“ˆ ì§„í–‰ìƒí™©: ì²˜ë¦¬ {self.total_processed:,}ê°œ, ì‹ ê·œ ì €ì¥ {self.total_saved:,}ê°œ")
                    logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ ì‹ ê·œ: {dict(list(self.domain_stats.items())[:5])}...")
                    logger.info(f"ğŸ“‹ ëŒ€ê¸° ì¤‘: ìš°ì„ ìˆœìœ„ {len(self.priority_queue)}ê°œ, ì¼ë°˜ {len(self.to_visit)}ê°œ")
                    logger.info(f"â±ï¸ ê²½ê³¼ ì‹œê°„: {elapsed}")
                
                # ì†ë„ ì¡°ì ˆ
                await asyncio.sleep(0.5)  # Colabì—ì„œëŠ” ë” ì•ˆì „í•˜ê²Œ
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        self.save_checkpoint()
        
        # ìµœì¢… í†µê³„
        total_elapsed = datetime.now() - self.session_start
        logger.info("âœ… í™•ì¥ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ ì²˜ë¦¬: {self.total_processed:,}ê°œ ì‹ ê·œ URL")
        logger.info(f"ğŸ’¾ ì´ ì €ì¥: {self.total_saved:,}ê°œ ì‹ ê·œ í˜ì´ì§€")
        logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ í†µê³„: {dict(self.domain_stats)}")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_elapsed}")
        logger.info(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {self.output_dir}/")
        
        # Colabì—ì„œëŠ” ì••ì¶• íŒŒì¼ ìƒì„±
        if COLAB_ENV and self.total_saved > 0:
            self.create_download_archive()
        
        return self.total_saved

    def create_download_archive(self):
        """Colabì—ì„œ ë‹¤ìš´ë¡œë“œìš© ì••ì¶• íŒŒì¼ ìƒì„±"""
        try:
            import zipfile
            import shutil
            
            archive_name = f"daejin_new_crawling_{self.total_saved}pages_{datetime.now().strftime('%Y%m%d_%H%M')}.zip"
            archive_path = os.path.join(self.base_path, archive_name)
            
            logger.info(f"ğŸ“¦ ì••ì¶• íŒŒì¼ ìƒì„± ì¤‘: {archive_name}")
            
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë“¤
                for filename in os.listdir(self.output_dir):
                    if filename.endswith('.txt'):
                        file_path = os.path.join(self.output_dir, filename)
                        zipf.write(file_path, f"new_crawling_output/{filename}")
                
                # ì²´í¬í¬ì¸íŠ¸ ë° ë©”íƒ€ë°ì´í„°
                zipf.write(self.checkpoint_file, "colab_crawler_checkpoint.json")
                zipf.write(self.visited_urls_file, "existing_urls.json")
                
                # í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
                report = {
                    "crawling_summary": {
                        "total_new_pages": self.total_saved,
                        "total_processed_urls": self.total_processed,
                        "domains_crawled": dict(self.domain_stats),
                        "session_duration": str(datetime.now() - self.session_start),
                        "completion_time": datetime.now().isoformat()
                    }
                }
                
                report_path = os.path.join(self.base_path, "crawling_report.json")
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                zipf.write(report_path, "crawling_report.json")
            
            logger.info(f"âœ… ì••ì¶• ì™„ë£Œ: {archive_path}")
            
            # Colabì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            if COLAB_ENV:
                files.download(archive_path)
                logger.info("ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘ë¨")
                
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    print("ğŸŒŸ ëŒ€ì§„ëŒ€í•™êµ í™•ì¥ í¬ë¡¤ë§ ì‹œìŠ¤í…œ v2.0")
    print("=" * 60)
    
    crawler = ColabAdvancedCrawler()
    try:
        if COLAB_ENV:
            print("ğŸ”— Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
            print("ğŸ’½ ê²°ê³¼ëŠ” Google Driveì— ìë™ ì €ì¥ë©ë‹ˆë‹¤.")
        
        result = asyncio.run(crawler.run_extended_crawling())
        print(f"\\nğŸ‰ í™•ì¥ í¬ë¡¤ë§ ì™„ë£Œ: {result:,}ê°œ ì‹ ê·œ í˜ì´ì§€ ìˆ˜ì§‘")
        
        if COLAB_ENV:
            print("ğŸ“¦ ì••ì¶• íŒŒì¼ì´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
            print("ğŸ“‚ Google Driveì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì–´ ì¬ì‹œì‘ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    except Exception as e:
        print(f"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")