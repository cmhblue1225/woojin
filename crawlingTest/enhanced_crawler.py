#!/usr/bin/env python3
"""
ëŒ€ì§„ëŒ€í•™êµ í™ˆí˜ì´ì§€ ê³ ì„±ëŠ¥ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
- MacBook Pro M4 Pro ìµœì í™” (14-core CPU, 20-core GPU)
- ì¤‘ë‹¨/ì¬ì‹œì‘ ì§€ì›
- ë¶ˆí•„ìš”í•œ ë„¤ë¹„ê²Œì´ì…˜ ë°ì´í„° ì œê±°
- ê¹Šì´ ì œí•œ ë° ë„ë©”ì¸ë³„ ì „ëµ
- RAG í†µí•© ì¤€ë¹„
"""

import os
import time
import json
import asyncio
import aiohttp
import requests
from urllib.parse import urlparse, urljoin, parse_qs
from bs4 import BeautifulSoup
import pdfplumber
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import multiprocessing as mp
from collections import defaultdict
import hashlib
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedCrawler:
    def __init__(self):
        # M4 Pro ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ëŒ€ëŸ‰ í¬ë¡¤ë§ ìµœì í™”)
        self.max_workers = min(12, mp.cpu_count())  # ì›Œì»¤ ìˆ˜ ì¦ê°€ (8â†’12)
        self.max_concurrent_requests = 30  # ë™ì‹œ HTTP ìš”ì²­ ìˆ˜ ì¦ê°€ (20â†’30)
        self.max_selenium_instances = 4    # Selenium ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì¦ê°€ (3â†’4)
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = "enhanced_output"
        self.state_file = "enhanced_crawler_state.json"
        self.error_log = os.path.join(self.output_dir, "enhanced_error_log.txt")
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        # í¬ë¡¤ë§ ìƒíƒœ
        self.visited = set()
        self.to_visit = set()
        self.saved_texts = []
        self.saved_urls = []
        self.url_depths = {}  # URLë³„ ê¹Šì´ ì¶”ì 
        self.domain_stats = defaultdict(int)  # ë„ë©”ì¸ë³„ í†µê³„
        
        # ìš°ì„ ìˆœìœ„ URL íŒ¨í„´ (ê²Œì‹œíŒ ìµœì í™”)
        self.priority_patterns = [
            r'/bbs/.*/artclView\.do',      # ê²Œì‹œíŒ ê²Œì‹œë¬¼ (ìµœê³  ìš°ì„ ìˆœìœ„)
            r'/bbs/.*/',                   # ê²Œì‹œíŒ ëª©ë¡
            r'/board/.*/',                 # ê²Œì‹œíŒ ê´€ë ¨
            r'/notice/',                   # ê³µì§€ì‚¬í•­
            r'/subview\.do',               # ì„œë¸Œí˜ì´ì§€
            r'/info/',                     # ì •ë³´ í˜ì´ì§€
            r'/curriculum/',               # êµìœ¡ê³¼ì •
            r'/professor/',                # êµìˆ˜ì§„
            r'/faculty/',                  # êµìˆ˜ì§„
            r'/student/',                  # í•™ìƒ ê´€ë ¨
            r'/admission/',                # ì…í•™ ê´€ë ¨
            r'/scholarship/',              # ì¥í•™ ê´€ë ¨
            r'/employment/',               # ì·¨ì—… ê´€ë ¨
            r'/research/',                 # ì—°êµ¬ ê´€ë ¨
        ]
        
        # ë„ë©”ì¸ë³„ ê¹Šì´ ì œí•œ (ëŒ€í­ í™•ëŒ€)
        self.domain_depth_limits = {
            'library.daejin.ac.kr': 0,    # ë„ì„œê´€ ì™„ì „ ì œì™¸
            'www.daejin.ac.kr': 15,       # ë©”ì¸ ì‚¬ì´íŠ¸ ë” ê¹Šê²Œ
            'ce.daejin.ac.kr': 12,        # ì»´í“¨í„°ê³µí•™ê³¼
            'law.daejin.ac.kr': 12,       # ë²•í•™ê³¼
            'eng.daejin.ac.kr': 12,       # ì˜ì–´ì˜ë¬¸í•™ê³¼
            'sm.daejin.ac.kr': 12,        # ê²½ì˜í•™ê³¼
            'admission.daejin.ac.kr': 10, # ì…í•™ì²˜
            'job.daejin.ac.kr': 10,       # ì·¨ì—…ì§„ë¡œì²˜
            'default': 10                 # ê¸°ë³¸ ê¹Šì´ ëŒ€í­ í™•ëŒ€
        }
        
        # ëª¨ë“  ëŒ€ì§„ëŒ€í•™êµ í•™ê³¼ ë° ê¸°ê´€ ì‹œì‘ URL
        self.start_urls = [
            # ë©”ì¸ ì‚¬ì´íŠ¸
            "https://www.daejin.ac.kr/sites/daejin/index.do",
            "https://www.daejin.ac.kr/daejin/874/subview.do",  # í•™ì‚¬ì •ë³´
            "https://www.daejin.ac.kr/daejin/873/subview.do",  # ì…í•™ì •ë³´
            "https://www.daejin.ac.kr/daejin/970/subview.do",  # ê³µì§€ì‚¬í•­
            
            # ê³µê³¼ëŒ€í•™
            "https://ce.daejin.ac.kr/ce/index.do",           # ì»´í“¨í„°ê³µí•™ê³¼
            "https://ee.daejin.ac.kr/ee/index.do",           # ì „ìì „ê¸°ê³µí•™ë¶€
            "https://ie.daejin.ac.kr/ie/index.do",           # ì‚°ì—…ê²½ì˜ê³µí•™ê³¼
            "https://chemeng.daejin.ac.kr/chemeng/index.do", # í™”í•™ê³µí•™ê³¼
            "https://civil.daejin.ac.kr/civil/index.do",     # ê±´ì„¤ì‹œìŠ¤í…œê³µí•™ê³¼
            "https://envir.daejin.ac.kr/envir/index.do",     # í™˜ê²½ê³µí•™ê³¼
            "https://arcon.daejin.ac.kr/arcon/index.do",     # ê±´ì¶•ê³µí•™ê³¼
            "https://architecture.daejin.ac.kr/architecture/index.do", # ê±´ì¶•í•™ê³¼
            
            # ê²½ìƒëŒ€í•™
            "https://sm.daejin.ac.kr/sm/index.do",           # ê²½ì˜í•™ê³¼
            "https://economics.daejin.ac.kr/economics/index.do", # ê²½ì œí•™ê³¼
            "https://trade.daejin.ac.kr/trade/index.do",     # êµ­ì œí†µìƒí•™ê³¼
            "https://intlbusiness.daejin.ac.kr/intlbusiness/index.do", # ê¸€ë¡œë²Œê²½ì˜í•™ê³¼
            "https://business.daejin.ac.kr/business/index.do", # ê²½ì˜ì •ë³´í•™ê³¼
            
            # ì¸ë¬¸ëŒ€í•™
            "https://eng.daejin.ac.kr/eng/index.do",         # ì˜ì–´ì˜ë¬¸í•™ê³¼
            "https://china.daejin.ac.kr/china/index.do",     # ì¤‘êµ­í•™ê³¼
            "https://japan.daejin.ac.kr/japan/index.do",     # ì¼ë³¸í•™ê³¼
            "https://korea.daejin.ac.kr/korea/index.do",     # êµ­ì–´êµ­ë¬¸í•™ê³¼
            "https://koreanstudies.daejin.ac.kr/koreanstudies/index.do", # í•œêµ­ì‚¬í•™ê³¼
            "https://creativewriting.daejin.ac.kr/creativewriting/index.do", # ë¬¸ì˜ˆì°½ì‘í•™ê³¼
            
            # ë²•ì •ëŒ€í•™
            "https://law.daejin.ac.kr/law/index.do",         # ê³µê³µì¸ì¬ë²•í•™ê³¼
            "https://pai.daejin.ac.kr/pai/index.do",         # í–‰ì •í•™ê³¼
            
            # ì‚¬íšŒê³¼í•™ëŒ€í•™
            "https://unification.daejin.ac.kr/unification/index.do", # í†µì¼í•™ê³¼
            "https://media.daejin.ac.kr/media/index.do",     # ë¯¸ë””ì–´ì»¤ë®¤ë‹ˆì¼€ì´ì…˜í•™ê³¼
            "https://djss.daejin.ac.kr/djss/index.do",       # ì‚¬íšŒë³µì§€í•™ê³¼
            
            # ì˜ˆìˆ ëŒ€í•™
            "https://music.daejin.ac.kr/music/index.do",     # ìŒì•…í•™ê³¼
            "https://arte.daejin.ac.kr/arte/index.do",       # íšŒí™”ê³¼
            "https://design.daejin.ac.kr/design/index.do",   # ì‚°ì—…ë””ìì¸í•™ê³¼
            "https://djfilm.daejin.ac.kr/djfilm/index.do",   # ì˜í™”ì˜ìƒí•™ê³¼
            
            # ìƒëª…ë³´ê±´ëŒ€í•™
            "https://food.daejin.ac.kr/food/index.do",       # ì‹í’ˆì˜ì–‘í•™ê³¼
            "https://sports.daejin.ac.kr/sports/index.do",   # ìŠ¤í¬ì¸ ê³¼í•™ê³¼
            "https://nurse.daejin.ac.kr/nurse/index.do",     # ê°„í˜¸í•™ê³¼
            "https://health.daejin.ac.kr/health/index.do",   # ë³´ê±´ê´€ë¦¬í•™ê³¼
            "https://bms.daejin.ac.kr/bms/index.do",         # ì˜ìƒëª…ë¶„ìê³¼í•™ê³¼
            
            # êµì–‘ëŒ€í•™
            "https://liberal.daejin.ac.kr/liberal/index.do", # êµì–‘ëŒ€í•™
            "https://personality.daejin.ac.kr/personality/index.do", # ì¸ì„±êµìœ¡ì„¼í„°
            
            # ëŒ€í•™ì›
            "https://grad.daejin.ac.kr/grad/index.do",       # ì¼ë°˜ëŒ€í•™ì›
            "https://dmz.daejin.ac.kr/dmz/index.do",         # DMZêµ­ì œëŒ€í•™ì›
            
            # íŠ¹ìˆ˜ê¸°ê´€
            "https://daesoon.daejin.ac.kr/daesoon/index.do", # ëŒ€ìˆœì¢…í•™ê³¼
            "https://rotc.daejin.ac.kr/rotc/index.do",       # í•™êµ°ë‹¨
            "https://aidata.daejin.ac.kr/aidata/index.do",   # AIë°ì´í„°ì‚¬ì´ì–¸ìŠ¤í•™ê³¼
            "https://swcg.daejin.ac.kr/swcg/index.do",       # SWìœµí•©ëŒ€í•™
            
            # ë¶€ì†ê¸°ê´€
            "https://admission.daejin.ac.kr/admission/index.do", # ì…í•™ì²˜
            "https://job.daejin.ac.kr/job/index.do",         # ì·¨ì—…ì§„ë¡œì²˜
            "https://counseling.daejin.ac.kr/counseling/index.do", # í•™ìƒìƒë‹´ì„¼í„°
            "https://dormitory.daejin.ac.kr/dormitory/index.do", # ìƒí™œê´€
            "https://global.daejin.ac.kr/global/index.do",   # êµ­ì œêµë¥˜ì›
            "https://ctl.daejin.ac.kr/ctl/index.do",         # êµìˆ˜í•™ìŠµì§€ì›ì„¼í„°
            "https://cce.daejin.ac.kr/cce/index.do",         # í‰ìƒêµìœ¡ì›
        ]
        
        self.load_state()

    def load_state(self):
        """ì´ì „ í¬ë¡¤ë§ ìƒíƒœ ë¡œë“œ"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, "r", encoding="utf-8") as f:
                    state = json.load(f)
                    self.visited.update(state.get("visited", []))
                    self.to_visit.update(state.get("to_visit", []))
                    self.saved_texts.extend(state.get("saved_texts", []))
                    self.saved_urls.extend(state.get("saved_urls", []))
                    self.url_depths.update(state.get("url_depths", {}))
                    
                logger.info(f"ğŸ”„ ìƒíƒœ ë³µì›: ë°©ë¬¸ {len(self.visited)}ê°œ, ëŒ€ê¸° {len(self.to_visit)}ê°œ")
            except Exception as e:
                logger.error(f"ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ì‹œì‘ URL ì¶”ê°€ (ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ê²ƒë§Œ)
        new_start_urls = set(self.start_urls) - self.visited
        self.to_visit.update(new_start_urls)
        
        # ì‹œì‘ URL ê¹Šì´ ì„¤ì •
        for url in self.start_urls:
            if url not in self.url_depths:
                self.url_depths[url] = 0

    def save_state(self):
        """í˜„ì¬ í¬ë¡¤ë§ ìƒíƒœ ì €ì¥"""
        state = {
            "visited": list(self.visited),
            "to_visit": list(self.to_visit),
            "saved_texts": self.saved_texts,
            "saved_urls": self.saved_urls,
            "url_depths": self.url_depths,
            "last_saved": datetime.now().isoformat(),
            "stats": dict(self.domain_stats)
        }
        
        try:
            with open(self.state_file, "w", encoding="utf-8") as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ìƒíƒœ ì €ì¥: ë°©ë¬¸ {len(self.visited)}ê°œ, ëŒ€ê¸° {len(self.to_visit)}ê°œ")
        except Exception as e:
            logger.error(f"ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

    def create_selenium_driver(self):
        """ìµœì í™”ëœ Selenium ë“œë¼ì´ë²„ ìƒì„±"""
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-web-security")
        options.add_argument("--disable-features=VizDisplayCompositor")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-plugins")
        options.add_argument("--disable-images")  # ì´ë¯¸ì§€ ë¡œë“œ ì•ˆí•¨
        options.add_argument("--disable-javascript")  # JS ì‹¤í–‰ ì•ˆí•¨ (í•„ìš”ì‹œ ì œê±°)
        options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36")
        
        # ì„±ëŠ¥ ìµœì í™”
        prefs = {
            "profile.managed_default_content_settings.images": 2,  # ì´ë¯¸ì§€ ì°¨ë‹¨
            "profile.default_content_setting_values.notifications": 2,  # ì•Œë¦¼ ì°¨ë‹¨
        }
        options.add_experimental_option("prefs", prefs)
        
        return webdriver.Chrome(options=options)

    def is_valid_url(self, url, current_depth=0):
        """URL ìœ íš¨ì„± ë° í¬ë¡¤ë§ ì¡°ê±´ ê²€ì‚¬"""
        try:
            parsed = urlparse(url)
            
            # ê¸°ë³¸ í•„í„°ë§
            if not parsed.scheme in ("http", "https"):
                return False
            if not "daejin.ac.kr" in parsed.netloc:
                return False
            if re.search(r'\\.(pdf|jpg|png|gif|doc|docx|zip|mp4|avi)$', url, re.IGNORECASE):
                return False
            if url in self.visited:
                return False
                
            # ê¹Šì´ ì œí•œ í™•ì¸
            domain = parsed.netloc
            max_depth = self.domain_depth_limits.get(domain, self.domain_depth_limits['default'])
            if current_depth > max_depth:
                return False
                
            # ë¶ˆí•„ìš”í•œ URL íŒ¨í„´ ì œì™¸ (ë„ì„œê´€ ì¶”ê°€)
            exclude_patterns = [
                r'#',  # ì•µì»¤ ë§í¬
                r'javascript:',  # JavaScript ë§í¬
                r'mailto:',  # ì´ë©”ì¼ ë§í¬
                r'/login',  # ë¡œê·¸ì¸ í˜ì´ì§€
                r'/admin',  # ê´€ë¦¬ì í˜ì´ì§€
                r'/popup',  # íŒì—… í˜ì´ì§€
                r'\\.(css|js)$',  # CSS, JS íŒŒì¼
                r'library\.daejin\.ac\.kr',  # ë„ì„œê´€ ì™„ì „ ì œì™¸
            ]
            
            for pattern in exclude_patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return False
            
            return True
            
        except Exception:
            return False

    def get_url_priority(self, url):
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ìš°ì„ )"""
        priority = 0
        
        # ìš°ì„ ìˆœìœ„ íŒ¨í„´ ë§¤ì¹­
        for i, pattern in enumerate(self.priority_patterns):
            if re.search(pattern, url, re.IGNORECASE):
                priority += (len(self.priority_patterns) - i) * 10
                break
        
        # ê¹Šì´ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ (ì–•ì„ìˆ˜ë¡ ë†’ìŒ)
        depth = self.url_depths.get(url, 0)
        priority += max(0, 10 - depth)
        
        # íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì‹œ ìš°ì„ ìˆœìœ„ ì¦ê°€ (í™•ì¥)
        important_keywords = [
            'ê³µì§€', 'notice', 'í•™ì‚¬', 'ìˆ˜ê°•', 'ì…í•™', 'ì¡¸ì—…', 'ì¥í•™',
            'êµìˆ˜', 'professor', 'êµìœ¡ê³¼ì •', 'curriculum', 'ì—°êµ¬',
            'í•™ìƒ', 'student', 'ì·¨ì—…', 'employment', 'job',
            'ì‹¤ìŠµ', 'í”„ë¡œì íŠ¸', 'project', 'ì„¸ë¯¸ë‚˜', 'seminar',
            'í•™íšŒ', 'conference', 'ì›Œí¬ìƒµ', 'workshop'
        ]
        for keyword in important_keywords:
            if keyword in url.lower():
                priority += 5
        
        return priority

    def clean_text_advanced(self, html, url):
        """í–¥ìƒëœ í…ìŠ¤íŠ¸ ì •ì œ"""
        if url.lower().endswith('.pdf'):
            return self.extract_pdf_text(html)
        
        try:
            soup = BeautifulSoup(html, "html.parser")
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (í™•ì¥ëœ ëª©ë¡)
            remove_selectors = [
                "header", "nav", "footer", "aside",
                ".navigation", ".nav", ".menu", ".sidebar",
                ".header", ".footer", ".top-menu", ".breadcrumb",
                ".sitemap", ".quickmenu", ".quick-menu",
                ".popup", ".modal", ".overlay",
                ".advertisement", ".banner", ".ad",
                ".social", ".share", ".sns",
                "script", "style", "noscript", "meta", "link",
                ".hidden", ".invisible", "[style*='display: none']",
                ".pagination", ".paging",
                ".button", ".btn", "input", "form",
                ".fnctId", ".imageSlide",  # ëŒ€ì§„ëŒ€ íŠ¹ì • ìš”ì†Œë“¤
            ]
            
            for selector in remove_selectors:
                for element in soup.select(selector):
                    element.extract()
            
            # í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            remove_class_patterns = [
                r'menu', r'nav', r'header', r'footer', r'sidebar',
                r'popup', r'modal', r'overlay', r'banner', r'ad',
                r'social', r'share', r'btn', r'button', r'pagination'
            ]
            
            for tag in soup.find_all(True):
                if tag.get('class'):
                    class_str = ' '.join(tag.get('class')).lower()
                    for pattern in remove_class_patterns:
                        if re.search(pattern, class_str):
                            tag.extract()
                            break
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text(separator="\\n", strip=True)
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            lines = text.split('\\n')
            cleaned_lines = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ë¶ˆí•„ìš”í•œ ë¼ì¸ íŒ¨í„´ ì œê±°
                skip_patterns = [
                    r'^\\d+$',  # ìˆ«ìë§Œ ìˆëŠ” ë¼ì¸
                    r'^\\s*[\\d\\.]+\\s*$',  # í˜ì´ì§€ ë²ˆí˜¸
                    r'fnctId=',  # ëŒ€ì§„ëŒ€ íŠ¹ì • í•¨ìˆ˜ ID
                    r'imageSlide',  # ì´ë¯¸ì§€ ìŠ¬ë¼ì´ë“œ ê´€ë ¨
                    r'^(ì´ì „|ë‹¤ìŒ|ì²˜ìŒ|ë§ˆì§€ë§‰)$',  # ë„¤ë¹„ê²Œì´ì…˜ í…ìŠ¤íŠ¸
                    r'^(HOME|home|ë©”ë‰´|MENU)$',  # ë©”ë‰´ ê´€ë ¨
                    r'^\\s*(ë¡œê·¸ì¸|LOGIN|íšŒì›ê°€ì…)\\s*$',  # ë¡œê·¸ì¸ ê´€ë ¨
                ]
                
                skip_line = False
                for pattern in skip_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        skip_line = True
                        break
                
                if not skip_line and len(line) > 2:  # ë„ˆë¬´ ì§§ì€ ë¼ì¸ ì œì™¸
                    cleaned_lines.append(line)
            
            # ì¤‘ë³µ ë¼ì¸ ì œê±°
            unique_lines = []
            seen = set()
            for line in cleaned_lines:
                if line not in seen:
                    unique_lines.append(line)
                    seen.add(line)
            
            final_text = '\\n'.join(unique_lines)
            
            # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ê¸°ì¤€ ì™„í™”)
            if len(final_text.strip()) < 30:
                return ""
                
            return final_text.strip()
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì •ì œ ì‹¤íŒ¨ {url}: {e}")
            return ""

    def extract_pdf_text(self, content):
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # contentê°€ bytesì¸ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì²˜ë¦¬
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as tmp:
                tmp.write(content)
                tmp_path = tmp.name
            
            text = ""
            with pdfplumber.open(tmp_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\\n"
            
            os.unlink(tmp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            return text.strip()
            
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ""

    def is_duplicate_content(self, text):
        """ê³ ê¸‰ ì¤‘ë³µ ì½˜í…ì¸  ê°ì§€"""
        if not self.saved_texts or len(text) < 100:
            return False
        
        try:
            # í…ìŠ¤íŠ¸ í•´ì‹œ ê¸°ë°˜ ë¹ ë¥¸ ê²€ì‚¬
            text_hash = hashlib.md5(text.encode()).hexdigest()
            text_hashes = [hashlib.md5(t.encode()).hexdigest() for t in self.saved_texts[-100:]]  # ìµœê·¼ 100ê°œë§Œ ë¹„êµ
            
            if text_hash in text_hashes:
                return True
            
            # TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬ (ìƒ˜í”Œë§)
            sample_texts = self.saved_texts[-50:] if len(self.saved_texts) > 50 else self.saved_texts
            if sample_texts:
                try:
                    vectorizer = TfidfVectorizer(max_features=1000).fit_transform([text] + sample_texts)
                    similarities = cosine_similarity(vectorizer[0:1], vectorizer[1:])[0]
                    return any(similarity > 0.90 for similarity in similarities)
                except:
                    return False
            
            return False
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return False

    def extract_links(self, soup, base_url, current_depth):
        """ë§í¬ ì¶”ì¶œ ë° ìš°ì„ ìˆœìœ„ ì •ë ¬"""
        links = set()
        
        for tag in soup.find_all("a", href=True):
            href = tag["href"]
            absolute_url = urljoin(base_url, href)
            
            if self.is_valid_url(absolute_url, current_depth + 1):
                links.add(absolute_url)
                self.url_depths[absolute_url] = current_depth + 1
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬
        prioritized_links = sorted(links, key=self.get_url_priority, reverse=True)
        return set(prioritized_links[:200])  # ìƒìœ„ 200ê°œë¡œ í™•ëŒ€

    async def crawl_page_async(self, session, url):
        """ë¹„ë™ê¸° í˜ì´ì§€ í¬ë¡¤ë§ (HTTP ìš”ì²­ìš©)"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with session.get(url, timeout=timeout) as response:
                if response.status == 200:
                    content = await response.text()
                    return content
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return None
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            return None

    def crawl_page_selenium(self, driver, url):
        """Selenium ê¸°ë°˜ í˜ì´ì§€ í¬ë¡¤ë§ (JavaScript í•„ìš”í•œ í˜ì´ì§€ìš©)"""
        try:
            driver.set_page_load_timeout(60)  # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ 60ì´ˆ
            driver.implicitly_wait(10)        # ì•”ì‹œì  ëŒ€ê¸° 10ì´ˆ
            
            driver.get(url)
            WebDriverWait(driver, 30).until(  # ëŒ€ê¸° ì‹œê°„ 15â†’30ì´ˆ
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            # í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
            time.sleep(3)  # ì•ˆì „í•œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
            return driver.page_source
        except Exception as e:
            logger.error(f"Selenium í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            return None

    def process_url(self, url_data):
        """ë‹¨ì¼ URL ì²˜ë¦¬"""
        url, use_selenium = url_data
        
        if url in self.visited:
            return None
        
        try:
            logger.info(f"ğŸ” í¬ë¡¤ë§: {url}")
            
            # ë„ë©”ì¸ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            domain = urlparse(url).netloc
            self.domain_stats[domain] += 1
            
            content = None
            
            if use_selenium:
                # JavaScriptê°€ í•„ìš”í•œ í˜ì´ì§€ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
                driver = self.create_selenium_driver()
                try:
                    content = self.crawl_page_selenium(driver, url)
                    if not content:
                        # ì²« ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨ ì‹œ HTTPë¡œ ì¬ì‹œë„
                        logger.warning(f"Selenium ì‹¤íŒ¨, HTTPë¡œ ì¬ì‹œë„: {url}")
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                        }
                        response = requests.get(url, headers=headers, timeout=30)
                        if response.status_code == 200:
                            content = response.text
                finally:
                    driver.quit()
            else:
                # ì¼ë°˜ HTTP ìš”ì²­
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=30)
                if response.status_code == 200:
                    content = response.text
            
            if not content:
                return None
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            cleaned_text = self.clean_text_advanced(content, url)
            
            if not cleaned_text or len(cleaned_text) < 50:
                logger.warning(f"âš ï¸  í…ìŠ¤íŠ¸ ë¶€ì¡±: {url}")
                return None
            
            # ì¤‘ë³µ ê²€ì‚¬
            if self.is_duplicate_content(cleaned_text):
                logger.info(f"ğŸ“‹ ì¤‘ë³µ ì½˜í…ì¸ : {url}")
                return None
            
            # ë§í¬ ì¶”ì¶œ
            soup = BeautifulSoup(content, "html.parser")
            current_depth = self.url_depths.get(url, 0)
            new_links = self.extract_links(soup, url, current_depth)
            
            return {
                'url': url,
                'text': cleaned_text,
                'links': new_links,
                'depth': current_depth
            }
            
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ {url}: {e}")
            return None

    def save_page(self, data, index):
        """í˜ì´ì§€ ë°ì´í„° ì €ì¥"""
        filename = os.path.join(self.output_dir, f"page_{index:05d}.txt")
        
        try:
            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"[URL] {data['url']}\\n")
                f.write(f"[DEPTH] {data['depth']}\\n")
                f.write(f"[DOMAIN] {urlparse(data['url']).netloc}\\n")
                f.write(f"[TIMESTAMP] {datetime.now().isoformat()}\\n")
                f.write(f"[LENGTH] {len(data['text'])}\\n\\n")
                f.write(data['text'])
            
            self.saved_texts.append(data['text'])
            self.saved_urls.append(data['url'])
            
            logger.info(f"âœ… ì €ì¥: {data['url']} ({len(data['text'])}ì)")
            
        except Exception as e:
            logger.error(f"ì €ì¥ ì‹¤íŒ¨ {data['url']}: {e}")

    def run(self):
        """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"ğŸš€ ê³ ì„±ëŠ¥ í¬ë¡¤ë§ ì‹œì‘ - {self.max_workers}ê°œ ì›Œì»¤ ì‚¬ìš©")
        
        page_index = len(self.saved_urls)
        
        try:
            while self.to_visit:
                # URL ìš°ì„ ìˆœìœ„ ì •ë ¬
                sorted_urls = sorted(list(self.to_visit), key=self.get_url_priority, reverse=True)
                
                # ë°°ì¹˜ í¬ê¸° (M4 Pro ì„±ëŠ¥ ìµœì í™”)
                batch_size = min(self.max_workers * 2, len(sorted_urls))
                current_batch = sorted_urls[:batch_size]
                
                # ì²˜ë¦¬í•  URLì„ to_visitì—ì„œ ì œê±°
                for url in current_batch:
                    self.to_visit.remove(url)
                
                # JavaScript í•„ìš” ì—¬ë¶€ íŒë‹¨
                url_tasks = []
                for url in current_batch:
                    # íŠ¹ì • íŒ¨í„´ì€ Selenium ì‚¬ìš©
                    use_selenium = any(pattern in url for pattern in [
                        'artclView.do', 'subview.do', 'board', 'bbs'
                    ])
                    url_tasks.append((url, use_selenium))
                
                # ë³‘ë ¬ ì²˜ë¦¬
                results = []
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_url = {executor.submit(self.process_url, task): task[0] 
                                   for task in url_tasks}
                    
                    for future in as_completed(future_to_url):
                        url = future_to_url[future]
                        try:
                            result = future.result()
                            if result:
                                results.append(result)
                                
                                # ë°©ë¬¸ ì²˜ë¦¬
                                self.visited.add(result['url'])
                                
                                # ìƒˆ ë§í¬ ì¶”ê°€
                                new_links = result['links'] - self.visited
                                self.to_visit.update(new_links)
                                
                                # í˜ì´ì§€ ì €ì¥
                                self.save_page(result, page_index)
                                page_index += 1
                                
                        except Exception as e:
                            logger.error(f"ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨ {url}: {e}")
                        
                        # ë°©ë¬¸ ì²˜ë¦¬ (ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„)
                        self.visited.add(url)
                
                # ì£¼ê¸°ì  ìƒíƒœ ì €ì¥
                if page_index % 20 == 0:
                    self.save_state()
                    logger.info(f"ğŸ“Š ì§„í–‰ ìƒí™©: ì €ì¥ {page_index}ê°œ, ëŒ€ê¸° {len(self.to_visit)}ê°œ")
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€ ë° ì•ˆì •ì„± í™•ë³´
                time.sleep(2)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
        except KeyboardInterrupt:
            logger.info("â¸ï¸  ì‚¬ìš©ì ì¤‘ë‹¨ - ìƒíƒœ ì €ì¥ ì¤‘...")
            self.save_state()
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            self.save_state()
        
        finally:
            self.save_state()
            logger.info(f"ğŸ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {page_index}ê°œ í˜ì´ì§€ ì €ì¥")
            logger.info(f"ğŸ“ˆ ë„ë©”ì¸ë³„ í†µê³„: {dict(self.domain_stats)}")

if __name__ == "__main__":
    crawler = EnhancedCrawler()
    crawler.run()