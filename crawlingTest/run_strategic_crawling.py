#!/usr/bin/env python3
"""
ì „ëµì  ë³´ì™„ í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- ê¸°ì¡´ 5,118ê°œ ë°ì´í„° + ìƒˆë¡œìš´ 2,000-3,000ê°œ ì¶”ê°€
- ë„ì„œê´€ ì–•ê²Œ, ê²Œì‹œíŒ ê¹Šê²Œ ì „ëµ
"""

import os
import sys
import asyncio
import time
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('strategic_execution.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def pre_crawling_check():
    """í¬ë¡¤ë§ ì „ ì‚¬ì „ ê²€ì‚¬"""
    logger.info("ğŸ” ì‚¬ì „ ê²€ì‚¬ ì‹œì‘")
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ í™•ì¸
    required_dirs = ['enhanced_output', 'strategic_output']
    for dir_name in required_dirs:
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)
            logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {dir_name}")
    
    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    existing_files = len([f for f in os.listdir('enhanced_output') if f.endswith('.txt')])
    logger.info(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {existing_files:,}ê°œ íŒŒì¼")
    
    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ (ê°„ì†Œí™”)
    import os
    import shutil
    
    # CPU ì½”ì–´ ìˆ˜ í™•ì¸
    try:
        cpu_count = os.cpu_count() or 4
    except:
        cpu_count = 4
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    try:
        total, used, free = shutil.disk_usage('.')
        free_gb = free // (1024**3)
    except:
        free_gb = 10  # ê¸°ë³¸ê°’
    
    logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:")
    logger.info(f"   CPU: {cpu_count}ì½”ì–´")
    logger.info(f"   ë””ìŠ¤í¬: {free_gb:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
    
    # ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸ (ë””ìŠ¤í¬ë§Œ)
    if free_gb < 2:  # 2GB
        logger.warning("âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: ìµœì†Œ 2GB í•„ìš”")
        return False
    
    logger.info("âœ… ì‚¬ì „ ê²€ì‚¬ ì™„ë£Œ")
    return True

def estimate_crawling_time():
    """í¬ë¡¤ë§ ì‹œê°„ ì¶”ì •"""
    logger.info("â±ï¸ í¬ë¡¤ë§ ì‹œê°„ ì¶”ì •")
    
    # ì¶”ì • íŒŒë¼ë¯¸í„°
    target_pages = 3000
    avg_page_time = 2.5  # í˜ì´ì§€ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    parallel_factor = 3  # ë³‘ë ¬ ì²˜ë¦¬ íš¨ê³¼
    
    estimated_seconds = (target_pages * avg_page_time) / parallel_factor
    estimated_hours = estimated_seconds / 3600
    
    logger.info(f"ğŸ“Š ì¶”ì • ê²°ê³¼:")
    logger.info(f"   ëª©í‘œ í˜ì´ì§€: {target_pages:,}ê°œ")
    logger.info(f"   ì˜ˆìƒ ì‹œê°„: {estimated_hours:.1f}ì‹œê°„")
    logger.info(f"   ì™„ë£Œ ì˜ˆì •: {datetime.now().strftime('%H:%M')} + {estimated_hours:.1f}h")
    
    return estimated_hours

async def run_strategic_crawling():
    """ì „ëµì  í¬ë¡¤ë§ ì‹¤í–‰"""
    logger.info("ğŸš€ ì „ëµì  ë³´ì™„ í¬ë¡¤ë§ ì‹œì‘")
    
    try:
        # í¬ë¡¤ëŸ¬ import ë° ì‹¤í–‰
        from strategic_crawler import StrategicCrawler
        
        crawler = StrategicCrawler()
        
        # í¬ë¡¤ë§ ì‹¤í–‰ (ìµœëŒ€ 3000í˜ì´ì§€)
        await crawler.run_strategic_crawling(max_pages=3000)
        
        logger.info("âœ… ì „ëµì  í¬ë¡¤ë§ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return False

def post_crawling_analysis():
    """í¬ë¡¤ë§ í›„ ë¶„ì„"""
    logger.info("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„")
    
    try:
        # ìƒˆë¡œ ìˆ˜ì§‘ëœ íŒŒì¼ ìˆ˜ í™•ì¸
        if os.path.exists('strategic_output'):
            new_files = len([f for f in os.listdir('strategic_output') if f.endswith('.txt')])
            logger.info(f"ğŸ†• ìƒˆë¡œ ìˆ˜ì§‘ëœ í˜ì´ì§€: {new_files:,}ê°œ")
        
        # ê¸°ì¡´ + ì‹ ê·œ ì´í•©
        existing_files = len([f for f in os.listdir('enhanced_output') if f.endswith('.txt')])
        total_files = existing_files + new_files
        
        logger.info(f"ğŸ“Š ì´ ë°ì´í„°ëŸ‰:")
        logger.info(f"   ê¸°ì¡´: {existing_files:,}ê°œ")
        logger.info(f"   ì‹ ê·œ: {new_files:,}ê°œ")
        logger.info(f"   ì´í•©: {total_files:,}ê°œ")
        
        # ê¶Œì¥ì‚¬í•­
        if total_files >= 8000:
            logger.info("âœ… ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - RAG ì„ë² ë”© ì§„í–‰ ê°€ëŠ¥")
        elif total_files >= 6000:
            logger.info("âš ï¸ ì¶”ê°€ í¬ë¡¤ë§ ê¶Œì¥ - ëª©í‘œì¹˜ì˜ 75% ìˆ˜ì§‘")
        else:
            logger.info("âŒ ì¶”ê°€ í¬ë¡¤ë§ í•„ìš” - ëª©í‘œì¹˜ ë¯¸ë‹¬")
        
        return new_files
        
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ¯ ëŒ€ì§„ëŒ€í•™êµ ì „ëµì  ë³´ì™„ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # ì‚¬ì „ ê²€ì‚¬
    if not pre_crawling_check():
        logger.error("âŒ ì‚¬ì „ ê²€ì‚¬ ì‹¤íŒ¨ - í¬ë¡¤ë§ ì¤‘ë‹¨")
        return False
    
    # ì‹œê°„ ì¶”ì •
    estimated_time = estimate_crawling_time()
    
    # ì‚¬ìš©ì í™•ì¸
    print(f"\nâ±ï¸ ì˜ˆìƒ ì†Œìš”ì‹œê°„: {estimated_time:.1f}ì‹œê°„")
    print("ğŸ“‹ í¬ë¡¤ë§ ì „ëµ:")
    print("   â€¢ ê¸°ì¡´ 5,118ê°œ ë°ì´í„° ìœ ì§€")
    print("   â€¢ ê²Œì‹œíŒ ê²Œì‹œê¸€ ì§‘ì¤‘ ìˆ˜ì§‘")
    print("   â€¢ ë„ì„œê´€ í˜ì´ì§€ ìµœì†Œ ìˆ˜ì§‘")
    print("   â€¢ ì „ìì±… ê²€ìƒ‰ í˜ì´ì§€ ì œì™¸")
    
    # ì‹¤í–‰ í™•ì¸ (ìë™ ì‹¤í–‰)
    print(f"\nğŸš€ í¬ë¡¤ë§ì„ ìë™ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("   ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    start_time = time.time()
    
    try:
        success = asyncio.run(run_strategic_crawling())
        
        if success:
            # ê²°ê³¼ ë¶„ì„
            new_files = post_crawling_analysis()
            
            elapsed_time = time.time() - start_time
            logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {elapsed_time/3600:.1f}ì‹œê°„")
            
            print("\n" + "=" * 60)
            print("âœ… ì „ëµì  í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ìƒˆë¡œ ìˆ˜ì§‘ëœ í˜ì´ì§€: {new_files:,}ê°œ")
            print(f"â±ï¸ ì†Œìš”ì‹œê°„: {elapsed_time/3600:.1f}ì‹œê°„")
            print("ğŸ“ ê²°ê³¼ ìœ„ì¹˜: strategic_output/")
            print("=" * 60)
            
            return True
        else:
            logger.error("âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨")
            return False
            
    except KeyboardInterrupt:
        logger.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        return False
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    main()