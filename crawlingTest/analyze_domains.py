#!/usr/bin/env python3
"""
ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë„ë©”ì¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import re
from collections import defaultdict

def analyze_crawled_data():
    """ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„"""
    output_dir = "enhanced_output"
    domain_stats = defaultdict(int)
    url_patterns = defaultdict(int)
    
    total_files = 0
    empty_files = 0
    
    print("ğŸ” ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ ì¤‘...")
    
    for filename in os.listdir(output_dir):
        if filename.endswith('.txt'):
            total_files += 1
            filepath = os.path.join(output_dir, filename)
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    if len(content.strip()) < 50:
                        empty_files += 1
                        continue
                    
                    # URLê³¼ ë„ë©”ì¸ ì¶”ì¶œ
                    url_match = re.search(r'\[URL\] (https?://[^\n]+)', content)
                    if url_match:
                        url = url_match.group(1)
                        
                        # ë„ë©”ì¸ ì¶”ì¶œ
                        domain_match = re.search(r'https?://([^/]+)', url)
                        if domain_match:
                            domain = domain_match.group(1)
                            domain_stats[domain] += 1
                            
                            # URL íŒ¨í„´ ë¶„ì„
                            if '/bbs/' in url and 'artclView.do' in url:
                                url_patterns['ê²Œì‹œíŒ_ê²Œì‹œê¸€'] += 1
                            elif '/bbs/' in url:
                                url_patterns['ê²Œì‹œíŒ_ëª©ë¡'] += 1
                            elif 'subview.do' in url:
                                url_patterns['ì„œë¸Œí˜ì´ì§€'] += 1
                            elif '/index.do' in url:
                                url_patterns['ë©”ì¸í˜ì´ì§€'] += 1
                            else:
                                url_patterns['ê¸°íƒ€'] += 1
                                
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {filename} - {e}")
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼:")
    print(f"ì´ íŒŒì¼ ìˆ˜: {total_files:,}ê°œ")
    print(f"ë¹ˆ íŒŒì¼ ìˆ˜: {empty_files:,}ê°œ")
    print(f"ìœ íš¨ íŒŒì¼ ìˆ˜: {total_files - empty_files:,}ê°œ")
    
    print(f"\nğŸŒ ë„ë©”ì¸ë³„ ë¶„í¬ (ìƒìœ„ 20ê°œ):")
    for domain, count in sorted(domain_stats.items(), key=lambda x: x[1], reverse=True)[:20]:
        print(f"  {domain}: {count:,}ê°œ")
    
    print(f"\nğŸ“ URL íŒ¨í„´ ë¶„í¬:")
    for pattern, count in sorted(url_patterns.items(), key=lambda x: x[1], reverse=True):
        print(f"  {pattern}: {count:,}ê°œ")
    
    # ë„ì„œê´€ í™•ì¸
    library_count = domain_stats.get('library.daejin.ac.kr', 0)
    print(f"\nğŸ“š ë„ì„œê´€ ë°ì´í„°: {library_count}ê°œ (ì´ë¯¸ ì œì™¸ë¨ âœ…)")
    
    return domain_stats, url_patterns

if __name__ == "__main__":
    analyze_crawled_data()