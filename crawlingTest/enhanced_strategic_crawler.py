#!/usr/bin/env python3
"""
í–¥ìƒëœ ì „ëµì  í¬ë¡¤ë§ ì‹œìŠ¤í…œ
- ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ì™„ì „ ìƒˆë¡œìš´ í¬ë¡¤ë§
- ê²Œì‹œíŒ ì¤‘ì‹¬ ê¹Šì´ ìš°ì„  íƒìƒ‰
- ë” ë§ì€ í•™ê³¼ í™ˆí˜ì´ì§€ ì»¤ë²„
"""

import os
import time
import json
import asyncio
import aiohttp
import requests
from urllib.parse import urlparse, urljoin, parse_qs
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, deque
import hashlib
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_strategic_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedStrategicCrawler:
    def __init__(self):
        # ì„±ëŠ¥ ì„¤ì • (M4 Pro ìµœì í™”)
        self.max_workers = 12
        self.max_concurrent_requests = 40
        self.max_selenium_instances = 6
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = "enhanced_strategic_output"
        self.state_file = "enhanced_strategic_crawler_state.json"
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        # í¬ë¡¤ë§ ìƒíƒœ (ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œ)
        self.visited = set()
        self.to_visit = deque()
        self.saved_texts = []
        self.saved_urls = []
        self.url_depths = {}
        self.domain_stats = defaultdict(int)
        self.priority_queue = deque()
        
        # ëŒ€í•™ í™ˆí˜ì´ì§€ ìš°ì„ ìˆœìœ„ URL (ëŒ€í­ í™•ì¥)
        self.seed_urls = [
            # ë©”ì¸ ëŒ€ì§„ëŒ€í•™êµ ì‚¬ì´íŠ¸ ë° ì£¼ìš” í˜ì´ì§€
            "https://www.daejin.ac.kr/sites/daejin/index.do",
            "https://www.daejin.ac.kr/daejin/index.do",
            "https://www.daejin.ac.kr/daejin/1135/subview.do",  # ê³µì§€ì‚¬í•­
            "https://www.daejin.ac.kr/daejin/1139/subview.do",  # ì¼ë°˜ì†Œì‹
            "https://www.daejin.ac.kr/daejin/1140/subview.do",  # í•™ì‚¬ê³µì§€
            "https://www.daejin.ac.kr/daejin/1141/subview.do",  # í–‰ì‚¬ì•ˆë‚´
            "https://www.daejin.ac.kr/daejin/1142/subview.do",  # ì±„ìš©ì •ë³´
            
            # ëŒ€í•™ êµ¬ì¡°
            "https://www.daejin.ac.kr/daejin/877/subview.do",  # ìƒìƒêµì–‘ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/869/subview.do",  # ëŒ€ìˆœì¢…í•™ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/870/subview.do",  # ì¸ë¬¸ì˜ˆìˆ ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/871/subview.do",  # ê¸€ë¡œë²Œì‚°ì—…í†µìƒëŒ€í•™
            "https://www.daejin.ac.kr/daejin/872/subview.do",  # ê³µê³µì¸ì¬ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/873/subview.do",  # ë³´ê±´ê³¼í•™ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/874/subview.do",  # AIìœµí•©ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/875/subview.do",  # ê³µê³¼ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/5566/subview.do", # êµ­ì œí˜‘ë ¥ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/876/subview.do",  # ë¯¸ë˜í‰ìƒêµìœ¡ìœµí•©ëŒ€í•™
            
            # AIìœµí•©ëŒ€í•™ (ìµœìš°ì„ )
            "https://ce.daejin.ac.kr/",                    # ì»´í“¨í„°ê³µí•™ì „ê³µ
            "https://ce.daejin.ac.kr/main.php",
            "https://ce.daejin.ac.kr/bbs/board.php?bo_table=notice",  # ê³µì§€ì‚¬í•­
            "https://ce.daejin.ac.kr/bbs/board.php?bo_table=job",     # ì·¨ì—…ì •ë³´
            "https://aidata.daejin.ac.kr/",                # AIë¹…ë°ì´í„°ì „ê³µ
            "https://security.daejin.ac.kr/",              # ìŠ¤ë§ˆíŠ¸ìœµí•©ë³´ì•ˆí•™ê³¼
            
            # ì¸ë¬¸ì˜ˆìˆ ëŒ€í•™
            "https://eng.daejin.ac.kr/",                   # ì˜ì–´ì˜ë¬¸í•™ê³¼
            "https://history.daejin.ac.kr/",               # ì—­ì‚¬ë¬¸í™”ì½˜í…ì¸ í•™ê³¼
            "https://literature.daejin.ac.kr/",            # ë¬¸ì˜ˆì½˜í…ì¸ ì°½ì‘í•™ê³¼
            "https://arte.daejin.ac.kr/",                  # í˜„ëŒ€ë¯¸ìˆ ì „ê³µ
            "https://design.daejin.ac.kr/",                # ì‹œê°ë””ìì¸í•™ê³¼
            "https://djfilm.daejin.ac.kr/",                # ì˜í™”ì˜ìƒí•™ê³¼
            "https://acting.daejin.ac.kr/",                # ì—°ê¸°ì˜ˆìˆ í•™ê³¼
            "https://music.daejin.ac.kr/",                 # ì‹¤ìš©ìŒì•…í•™ê³¼
            
            # ê¸€ë¡œë²Œì‚°ì—…í†µìƒëŒ€í•™
            "https://economics.daejin.ac.kr/",             # ê¸€ë¡œë²Œê²½ì œí•™ê³¼
            "https://sm.daejin.ac.kr/",                    # ê²½ì˜í•™ê³¼
            "https://intlbusiness.daejin.ac.kr/",          # êµ­ì œí†µìƒí•™ê³¼
            "https://camde.daejin.ac.kr/",                 # ì¤‘êµ­ì–´ë¬¸í™”í•™ê³¼
            
            # ê³µê³µì¸ì¬ëŒ€í•™
            "https://law.daejin.ac.kr/",                   # ê³µê³µì¸ì¬ë²•í•™ê³¼
            "https://pai.daejin.ac.kr/",                   # í–‰ì •ì •ë³´í•™ê³¼
            "https://welfare.daejin.ac.kr/",               # ì‚¬íšŒë³µì§€í•™ê³¼
            "https://child.daejin.ac.kr/",                 # ì•„ë™í•™ê³¼
            "https://media.daejin.ac.kr/",                 # ë¯¸ë””ì–´ì»¤ë®¤ë‹ˆì¼€ì´ì…˜í•™ê³¼
            
            # ë³´ê±´ê³¼í•™ëŒ€í•™
            "https://medical.daejin.ac.kr/",               # ì˜ìƒëª…ê³¼í•™ê³¼
            "https://nurse.daejin.ac.kr/",                 # ê°„í˜¸í•™ê³¼
            "https://sports.daejin.ac.kr/",                # ìŠ¤í¬ì¸ ê±´ê°•ê³¼í•™ê³¼
            "https://food.daejin.ac.kr/",                  # ì‹í’ˆì˜ì–‘í•™ê³¼
            "https://health.daejin.ac.kr/",                # ë³´ê±´ê²½ì˜í•™ê³¼
            
            # ê³µê³¼ëŒ€í•™
            "https://elec.daejin.ac.kr/",                  # ì „ê¸°ê³µí•™ì „ê³µ
            "https://cpe.daejin.ac.kr/",                   # í™”í•™ê³µí•™ì „ê³µ
            "https://arch.daejin.ac.kr/",                  # ê±´ì¶•ê³µí•™ê³¼
            "https://envir.daejin.ac.kr/",                 # ìŠ¤ë§ˆíŠ¸ê±´ì„¤í™˜ê²½ê³µí•™ê³¼
            "https://ie.daejin.ac.kr/",                    # ë°ì´í„°ê²½ì˜ì‚°ì—…ê³µí•™ê³¼
            "https://mech.daejin.ac.kr/",                  # ITê¸°ê³„ê³µí•™ê³¼
            
            # êµ­ì œí˜‘ë ¥ëŒ€í•™
            "https://korean.daejin.ac.kr/",                # í•œêµ­í•™ê³¼
            
            # ê¸°íƒ€ ì¤‘ìš” ê¸°ê´€
            "https://admission.daejin.ac.kr/",             # ì…í•™ì²˜
            "https://job.daejin.ac.kr/",                   # ëŒ€í•™ì¼ìë¦¬í”ŒëŸ¬ìŠ¤ì„¼í„°
            "https://ctl.daejin.ac.kr/",                   # êµìˆ˜í•™ìŠµì§€ì›ì„¼í„°
            "https://liberal.daejin.ac.kr/",               # ìƒìƒêµì–‘ëŒ€í•™
            "https://international.daejin.ac.kr/",         # êµ­ì œêµë¥˜ì›
            "https://dormitory.daejin.ac.kr/",             # ìƒí™œê´€
            "https://rotc.daejin.ac.kr/",                  # í•™êµ°ë‹¨
            "https://counseling.daejin.ac.kr/",            # í•™ìƒìƒí™œìƒë‹´ì„¼í„°
        ]
        
        # ë„ë©”ì¸ë³„ í¬ë¡¤ë§ ì „ëµ
        self.domain_strategies = {
            # ë©”ì¸ ì‚¬ì´íŠ¸ - ë§¤ìš° ê¹Šê²Œ
            'www.daejin.ac.kr': {'max_depth': 20, 'priority': 1000},
            
            # AIìœµí•©ëŒ€í•™ - ìµœìš°ì„ 
            'ce.daejin.ac.kr': {'max_depth': 25, 'priority': 950},
            'aidata.daejin.ac.kr': {'max_depth': 20, 'priority': 900},
            'security.daejin.ac.kr': {'max_depth': 20, 'priority': 900},
            
            # ì£¼ìš” í•™ê³¼ - ë†’ì€ ìš°ì„ ìˆœìœ„
            'economics.daejin.ac.kr': {'max_depth': 15, 'priority': 800},
            'sm.daejin.ac.kr': {'max_depth': 15, 'priority': 800},
            'law.daejin.ac.kr': {'max_depth': 15, 'priority': 800},
            'media.daejin.ac.kr': {'max_depth': 15, 'priority': 800},
            'medical.daejin.ac.kr': {'max_depth': 15, 'priority': 800},
            'nurse.daejin.ac.kr': {'max_depth': 15, 'priority': 800},
            
            # ê¸°íƒ€ í•™ê³¼ - ì¤‘ê°„ ìš°ì„ ìˆœìœ„
            'default': {'max_depth': 12, 'priority': 500}
        }
        
        # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ URL íŒ¨í„´
        self.high_priority_patterns = [
            (r'/bbs/.*/artclView\\.do', 100),      # ê²Œì‹œíŒ ê²Œì‹œë¬¼
            (r'/ce/2518/subview\\.do\\?enc=', 95), # ì»´ê³µê³¼ ê²Œì‹œê¸€
            (r'/bbs/.*/', 80),                     # ê²Œì‹œíŒ ëª©ë¡
            (r'/board/.*/', 75),                   # ê²Œì‹œíŒ ê´€ë ¨
            (r'/notice/', 70),                     # ê³µì§€ì‚¬í•­
            (r'/subview\\.do', 60),                # ì„œë¸Œí˜ì´ì§€
            (r'/curriculum/', 50),                 # êµìœ¡ê³¼ì •
            (r'/professor/', 50),                  # êµìˆ˜ì§„
            (r'/faculty/', 50),                    # êµìˆ˜ì§„
            (r'/student/', 45),                    # í•™ìƒ ê´€ë ¨
            (r'/admission/', 40),                  # ì…í•™ ê´€ë ¨
        ]
        
        # ì œì™¸í•  íŒ¨í„´ (ìµœì†Œí™”)
        self.exclude_patterns = [
            r'\\.(pdf|doc|hwp|zip|exe)$',
            r'/download\\.do',
            r'groupware\\.daejin\\.ac\\.kr',
            r'webmail\\.daejin\\.ac\\.kr',
            r'sso\\.daejin\\.ac\\.kr',
            r'#none$',
            r'#this$',
            r'javascript:',
        ]

    def should_crawl_url(self, url, depth):
        """URL í¬ë¡¤ë§ ì—¬ë¶€ ê²°ì • (ê¸°ì¡´ ë°ì´í„°ì™€ ë¬´ê´€í•˜ê²Œ)"""
        # ì´ë¯¸ ë°©ë¬¸í•œ URL
        if url in self.visited:
            return False
        
        # ì œì™¸ íŒ¨í„´
        for pattern in self.exclude_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # ë„ë©”ì¸ í™•ì¸
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # ëŒ€ì§„ëŒ€í•™êµ ë„ë©”ì¸ë§Œ í—ˆìš©
        if 'daejin.ac.kr' not in domain:
            return False
        
        # ë„ë©”ì¸ë³„ ê¹Šì´ ì œí•œ
        strategy = self.domain_strategies.get(domain, self.domain_strategies['default'])
        if depth > strategy['max_depth']:
            return False
        
        return True

    def get_url_priority(self, url):
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # ì‹œë“œ URLì€ ìµœê³  ìš°ì„ ìˆœìœ„
        if url in self.seed_urls:
            return 1000
        
        # íŒ¨í„´ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        for pattern, priority in self.high_priority_patterns:
            if re.search(pattern, url):
                return priority
        
        # ë„ë©”ì¸ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        parsed = urlparse(url)
        domain = parsed.netloc
        strategy = self.domain_strategies.get(domain, self.domain_strategies['default'])
        
        return strategy['priority']

    async def crawl_with_selenium(self, url, depth=0):
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
            
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(20)
            
            driver.get(url)
            time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            content = driver.page_source
            soup = BeautifulSoup(content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            text_content = self.extract_clean_text(soup)
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                if self.should_crawl_url(full_url, depth + 1):
                    priority = self.get_url_priority(full_url)
                    links.append((full_url, priority, depth + 1))
            
            driver.quit()
            
            return {
                'url': url,
                'content': text_content,
                'links': links,
                'depth': depth,
                'domain': urlparse(url).netloc,
                'length': len(text_content),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Selenium í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            try:
                driver.quit()
            except:
                pass
            return None

    def extract_clean_text(self, soup):
        """ê¹”ë”í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        text = soup.get_text()
        lines = [line.strip() for line in text.splitlines()]
        text = '\\n'.join(line for line in lines if line and len(line) > 2)
        
        return text

    def save_page_content(self, page_data):
        """í˜ì´ì§€ ë‚´ìš© ì €ì¥"""
        if not page_data or len(page_data['content'].strip()) < 200:
            return False
        
        filename = f"enhanced_strategic_page_{len(self.saved_texts):05d}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        content = f"[URL] {page_data['url']}\\n"
        content += f"[DEPTH] {page_data['depth']}\\n"
        content += f"[DOMAIN] {page_data['domain']}\\n"
        content += f"[TIMESTAMP] {page_data['timestamp']}\\n"
        content += f"[LENGTH] {page_data['length']}\\n\\n"
        content += page_data['content']
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.saved_texts.append(filepath)
            self.saved_urls.append(page_data['url'])
            self.domain_stats[page_data['domain']] += 1
            
            logger.info(f"ğŸ’¾ ì €ì¥: {filename} ({page_data['domain']}) - {page_data['length']}ì")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    async def run_enhanced_crawling(self, max_pages=5000):
        """í–¥ìƒëœ ì „ëµì  í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸš€ í–¥ìƒëœ ì „ëµì  í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"ğŸ¯ ì‹œë“œ URL: {len(self.seed_urls)}ê°œ")
        logger.info(f"ğŸ“Š ëª©í‘œ í˜ì´ì§€: {max_pages:,}ê°œ")
        
        # ì‹œë“œ URLë¡œ ì‹œì‘
        for url in self.seed_urls:
            priority = self.get_url_priority(url)
            self.priority_queue.append((url, 0, priority))
        
        processed = 0
        
        with ThreadPoolExecutor(max_workers=self.max_selenium_instances) as executor:
            while (self.priority_queue or self.to_visit) and processed < max_pages:
                # ìš°ì„ ìˆœìœ„ íì—ì„œ ê°€ì ¸ì˜¤ê¸°
                current_batch = []
                
                # ìš°ì„ ìˆœìœ„ í ìš°ì„  ì²˜ë¦¬
                while self.priority_queue and len(current_batch) < self.max_selenium_instances:
                    url, depth, priority = self.priority_queue.popleft()
                    if url not in self.visited:
                        current_batch.append((url, depth, priority))
                
                # ì¼ë°˜ íì—ì„œ ì¶”ê°€
                while self.to_visit and len(current_batch) < self.max_selenium_instances:
                    url, depth, priority = self.to_visit.popleft()
                    if url not in self.visited:
                        current_batch.append((url, depth, priority))
                
                if not current_batch:
                    break
                
                # ë³‘ë ¬ ì²˜ë¦¬
                futures = []
                for url, depth, priority in current_batch:
                    if url not in self.visited:
                        future = executor.submit(asyncio.run, self.crawl_with_selenium(url, depth))
                        futures.append(future)
                        self.visited.add(url)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for future in as_completed(futures):
                    try:
                        page_data = future.result()
                        if page_data:
                            # í˜ì´ì§€ ì €ì¥
                            if self.save_page_content(page_data):
                                processed += 1
                            
                            # ìƒˆ ë§í¬ë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¶„ë¥˜
                            for link_url, link_priority, link_depth in page_data['links']:
                                if link_url not in self.visited:
                                    if link_priority >= 70:  # ë†’ì€ ìš°ì„ ìˆœìœ„
                                        self.priority_queue.append((link_url, link_depth, link_priority))
                                    else:  # ì¼ë°˜ ìš°ì„ ìˆœìœ„
                                        self.to_visit.append((link_url, link_depth, link_priority))
                    
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
                # ì£¼ê¸°ì  ìƒíƒœ ë³´ê³ 
                if processed % 100 == 0:
                    logger.info(f"ğŸ“ˆ ì§„í–‰ìƒí™©: {processed}/{max_pages} í˜ì´ì§€ ì²˜ë¦¬")
                    logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ ìˆ˜ì§‘: {dict(self.domain_stats)}")
                    logger.info(f"ğŸ“‹ ëŒ€ê¸° ì¤‘: ìš°ì„ ìˆœìœ„ {len(self.priority_queue)}ê°œ, ì¼ë°˜ {len(self.to_visit)}ê°œ")
                
                # ì†ë„ ì¡°ì ˆ
                await asyncio.sleep(0.5)
        
        logger.info("âœ… í–¥ìƒëœ ì „ëµì  í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {processed}ê°œ í˜ì´ì§€")
        logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ í†µê³„: {dict(self.domain_stats)}")
        
        return processed

if __name__ == "__main__":
    crawler = EnhancedStrategicCrawler()
    result = asyncio.run(crawler.run_enhanced_crawling(max_pages=5000))
    print(f"í¬ë¡¤ë§ ì™„ë£Œ: {result}ê°œ í˜ì´ì§€ ìˆ˜ì§‘")