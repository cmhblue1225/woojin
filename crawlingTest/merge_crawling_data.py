#!/usr/bin/env python3
"""
ê¸°ì¡´ + ì‹ ê·œ í¬ë¡¤ë§ ë°ì´í„° í†µí•© ìŠ¤í¬ë¦½íŠ¸
RAG ì‹œìŠ¤í…œ ì„ë² ë”©ìš© ë°ì´í„° ì¤€ë¹„
"""

import os
import shutil
import json
import re
from datetime import datetime
from collections import defaultdict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CrawlingDataMerger:
    def __init__(self):
        self.existing_dir = "enhanced_output"
        self.strategic_dir = "strategic_output"
        self.merged_dir = "merged_output"
        self.stats = defaultdict(int)
        
    def create_merged_directory(self):
        """í†µí•© ë””ë ‰í† ë¦¬ ìƒì„±"""
        if os.path.exists(self.merged_dir):
            backup_dir = f"{self.merged_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            shutil.move(self.merged_dir, backup_dir)
            logger.info(f"ğŸ“ ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë°±ì—…: {backup_dir}")
        
        os.makedirs(self.merged_dir, exist_ok=True)
        logger.info(f"ğŸ“ í†µí•© ë””ë ‰í† ë¦¬ ìƒì„±: {self.merged_dir}")

    def analyze_file_content(self, filepath):
        """íŒŒì¼ ë‚´ìš© ë¶„ì„"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # URLê³¼ ë„ë©”ì¸ ì¶”ì¶œ
            url_match = re.search(r'\[URL\] (https?://[^\n]+)', content)
            domain_match = re.search(r'\[DOMAIN\] ([^\n]+)', content)
            length_match = re.search(r'\[LENGTH\] (\d+)', content)
            
            url = url_match.group(1) if url_match else ""
            domain = domain_match.group(1) if domain_match else ""
            length = int(length_match.group(1)) if length_match else len(content)
            
            # í’ˆì§ˆ ë¶„ì„
            quality_score = self.calculate_quality_score(content, url)
            
            return {
                'url': url,
                'domain': domain,
                'length': length,
                'quality_score': quality_score,
                'content': content
            }
            
        except Exception as e:
            logger.warning(f"âŒ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {filepath}: {e}")
            return None

    def calculate_quality_score(self, content, url):
        """ì»¨í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        
        # ê¸¸ì´ ì ìˆ˜ (100-2000ì: ìµœê³ ì )
        content_length = len(content)
        if 100 <= content_length <= 2000:
            score += 10
        elif 50 <= content_length < 100:
            score += 5
        elif content_length > 2000:
            score += 7
        
        # URL íŒ¨í„´ ì ìˆ˜
        if '/bbs/' in url and 'artclView.do' in url:
            score += 15  # ê²Œì‹œíŒ ê²Œì‹œê¸€
        elif '/bbs/' in url:
            score += 10  # ê²Œì‹œíŒ ëª©ë¡
        elif 'subview.do' in url:
            score += 8   # ì„œë¸Œí˜ì´ì§€
        elif '/index.do' in url:
            score += 5   # ë©”ì¸í˜ì´ì§€
        
        # ë„ë©”ì¸ ì ìˆ˜
        if 'ce.daejin.ac.kr' in url:
            score += 5   # ì»´ê³µê³¼ ìš°ëŒ€
        elif 'www.daejin.ac.kr' in url:
            score += 3   # ë©”ì¸ ì‚¬ì´íŠ¸
        elif any(dept in url for dept in ['law', 'eng', 'food', 'nurse']):
            score += 4   # ì£¼ìš” í•™ê³¼
        
        # ì „ìì±… í˜ì´ì§€ ê°ì 
        if 'ebook.daejin.ac.kr' in url:
            if any(pattern in url for pattern in ['search', 'keyword', 'category']):
                score -= 10  # ê²€ìƒ‰/ëª©ë¡ í˜ì´ì§€
            else:
                score -= 5   # ì¼ë°˜ ì „ìì±… í˜ì´ì§€
        
        # ì»¨í…ì¸  í’ˆì§ˆ í™•ì¸
        if 'ê³µì§€ì‚¬í•­' in content or 'ì•ˆë‚´' in content:
            score += 3
        if 'êµìœ¡ê³¼ì •' in content or 'êµìˆ˜' in content:
            score += 3
        if 'ì…í•™' in content or 'ëª¨ì§‘' in content:
            score += 2
        
        return max(0, score)  # ìŒìˆ˜ ë°©ì§€

    def filter_and_copy_files(self, source_dir, prefix):
        """íŒŒì¼ í•„í„°ë§ ë° ë³µì‚¬"""
        if not os.path.exists(source_dir):
            logger.warning(f"âš ï¸ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ì—†ìŒ: {source_dir}")
            return 0
        
        files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]
        copied_count = 0
        
        logger.info(f"ğŸ“Š {source_dir} ë¶„ì„ ì¤‘... ({len(files):,}ê°œ íŒŒì¼)")
        
        # í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        file_scores = []
        for filename in files:
            filepath = os.path.join(source_dir, filename)
            analysis = self.analyze_file_content(filepath)
            
            if analysis and analysis['quality_score'] > 0:
                file_scores.append((filename, analysis))
        
        # í’ˆì§ˆ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        file_scores.sort(key=lambda x: x[1]['quality_score'], reverse=True)
        
        # ê³ í’ˆì§ˆ íŒŒì¼ë§Œ ë³µì‚¬
        for filename, analysis in file_scores:
            # í’ˆì§ˆ í•„í„°ë§
            if analysis['quality_score'] < 5:  # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€
                continue
                
            # ì¤‘ë³µ URL ì œê±° (ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸)
            url_hash = hash(analysis['url'])
            if url_hash in self.processed_urls:
                self.stats['duplicates'] += 1
                continue
            
            self.processed_urls.add(url_hash)
            
            # íŒŒì¼ ë³µì‚¬
            source_path = os.path.join(source_dir, filename)
            target_filename = f"{prefix}_{copied_count:05d}.txt"
            target_path = os.path.join(self.merged_dir, target_filename)
            
            try:
                shutil.copy2(source_path, target_path)
                copied_count += 1
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats['total_files'] += 1
                self.stats[f'{prefix}_files'] += 1
                self.stats[analysis['domain']] += 1
                
                if copied_count % 500 == 0:
                    logger.info(f"   ë³µì‚¬ ì§„í–‰: {copied_count:,}ê°œ ì™„ë£Œ")
                    
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ë³µì‚¬ ì˜¤ë¥˜ {filename}: {e}")
        
        logger.info(f"âœ… {source_dir}: {copied_count:,}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ")
        return copied_count

    def create_metadata(self):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±"""
        metadata = {
            'merge_timestamp': datetime.now().isoformat(),
            'source_directories': {
                'existing': self.existing_dir,
                'strategic': self.strategic_dir
            },
            'statistics': dict(self.stats),
            'quality_criteria': {
                'min_quality_score': 5,
                'duplicate_removal': True,
                'ebook_filtering': True
            },
            'total_files': self.stats['total_files']
        }
        
        metadata_path = os.path.join(self.merged_dir, 'merge_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

    def merge_crawling_data(self):
        """í¬ë¡¤ë§ ë°ì´í„° í†µí•© ì‹¤í–‰"""
        logger.info("ğŸ”„ í¬ë¡¤ë§ ë°ì´í„° í†µí•© ì‹œì‘")
        
        # ì´ˆê¸°í™”
        self.processed_urls = set()
        self.create_merged_directory()
        
        # ê¸°ì¡´ ë°ì´í„° ë³µì‚¬ (ë†’ì€ í’ˆì§ˆë§Œ)
        existing_count = self.filter_and_copy_files(self.existing_dir, "existing")
        
        # ì‹ ê·œ ë°ì´í„° ë³µì‚¬
        strategic_count = self.filter_and_copy_files(self.strategic_dir, "strategic")
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        self.create_metadata()
        
        # ê²°ê³¼ ë³´ê³ 
        total_count = existing_count + strategic_count
        logger.info("âœ… ë°ì´í„° í†µí•© ì™„ë£Œ")
        logger.info(f"ğŸ“Š í†µí•© ê²°ê³¼:")
        logger.info(f"   ê¸°ì¡´ ë°ì´í„°: {existing_count:,}ê°œ")
        logger.info(f"   ì‹ ê·œ ë°ì´í„°: {strategic_count:,}ê°œ")
        logger.info(f"   ì¤‘ë³µ ì œê±°: {self.stats['duplicates']:,}ê°œ")
        logger.info(f"   ì´ íŒŒì¼: {total_count:,}ê°œ")
        logger.info(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {self.merged_dir}/")
        
        # ë„ë©”ì¸ë³„ í†µê³„ (ìƒìœ„ 10ê°œ)
        domain_stats = {k: v for k, v in self.stats.items() 
                       if k not in ['total_files', 'existing_files', 'strategic_files', 'duplicates']}
        top_domains = sorted(domain_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        
        logger.info(f"ğŸŒ ì£¼ìš” ë„ë©”ì¸ (ìƒìœ„ 10ê°œ):")
        for domain, count in top_domains:
            logger.info(f"   {domain}: {count:,}ê°œ")
        
        return total_count

if __name__ == "__main__":
    merger = CrawlingDataMerger()
    total_files = merger.merge_crawling_data()
    
    print("\n" + "=" * 60)
    print("âœ… í¬ë¡¤ë§ ë°ì´í„° í†µí•© ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ íŒŒì¼ ìˆ˜: {total_files:,}ê°œ")
    print("ğŸ“ í†µí•© ë°ì´í„° ìœ„ì¹˜: merged_output/")
    print("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: RAG ì‹œìŠ¤í…œ ì„ë² ë”© ì‹¤í–‰")
    print("=" * 60)