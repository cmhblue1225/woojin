#!/usr/bin/env python3
"""
ì „ëµì  ë³´ì™„ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
- ê¸°ì¡´ ë°ì´í„° í™œìš© + ëˆ„ë½ ì˜ì—­ ì§‘ì¤‘ í¬ë¡¤ë§
- ë„ì„œê´€ ì–•ê²Œ, ê²Œì‹œíŒ ê¹Šê²Œ ì „ëµ êµ¬í˜„
"""

import os
import time
import json
import asyncio
import aiohttp
import requests
from urllib.parse import urlparse, urljoin, parse_qs
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import hashlib
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('strategic_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class StrategicCrawler:
    def __init__(self):
        # M4 Pro ìµœëŒ€ ì„±ëŠ¥ ì„¤ì •
        self.max_workers = 16
        self.max_concurrent_requests = 50
        self.max_selenium_instances = 8
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = "strategic_output"
        self.state_file = "strategic_crawler_state.json"
        self.existing_data_dir = "enhanced_output"
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        # í¬ë¡¤ë§ ìƒíƒœ
        self.visited = set()
        self.to_visit = set()
        self.existing_urls = set()  # ê¸°ì¡´ í¬ë¡¤ë§ëœ URL
        self.saved_texts = []
        self.saved_urls = []
        self.url_depths = {}
        self.domain_stats = defaultdict(int)
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        self.load_existing_data()
        
        # ëŒ€í­ í™•ì¥ëœ ìš°ì„ ìˆœìœ„ URL (ëª¨ë“  í•™ê³¼ ë° ê¸°ê´€ í¬í•¨)
        self.priority_targets = [
            # ë©”ì¸ ëŒ€ì§„ëŒ€í•™êµ ì‚¬ì´íŠ¸
            "https://www.daejin.ac.kr/sites/daejin/index.do",
            "https://www.daejin.ac.kr/daejin/index.do",
            "https://www.daejin.ac.kr/daejin/877/subview.do",  # ìƒìƒêµì–‘ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/869/subview.do",  # ëŒ€ìˆœì¢…í•™ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/870/subview.do",  # ì¸ë¬¸ì˜ˆìˆ ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/871/subview.do",  # ê¸€ë¡œë²Œì‚°ì—…í†µìƒëŒ€í•™
            "https://www.daejin.ac.kr/daejin/872/subview.do",  # ê³µê³µì¸ì¬ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/873/subview.do",  # ë³´ê±´ê³¼í•™ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/874/subview.do",  # AIìœµí•©ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/875/subview.do",  # ê³µê³¼ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/5566/subview.do", # êµ­ì œí˜‘ë ¥ëŒ€í•™
            "https://www.daejin.ac.kr/daejin/876/subview.do",  # ë¯¸ë˜í‰ìƒêµìœ¡ìœµí•©ëŒ€í•™
            
            # ì „ì²´ í•™ê³¼ í™ˆí˜ì´ì§€ - ì¸ë¬¸ì˜ˆìˆ ëŒ€í•™
            "https://eng.daejin.ac.kr/",                   # ì˜ì–´ì˜ë¬¸í•™ê³¼
            "https://history.daejin.ac.kr/",               # ì—­ì‚¬ë¬¸í™”ì½˜í…ì¸ í•™ê³¼
            "https://literature.daejin.ac.kr/",            # ë¬¸ì˜ˆì½˜í…ì¸ ì°½ì‘í•™ê³¼
            "https://arte.daejin.ac.kr/",                  # í˜„ëŒ€ë¯¸ìˆ ì „ê³µ
            "https://design.daejin.ac.kr/",                # ì‹œê°ë””ìì¸í•™ê³¼
            "https://djfilm.daejin.ac.kr/",                # ì˜í™”ì˜ìƒí•™ê³¼
            "https://acting.daejin.ac.kr/",                # ì—°ê¸°ì˜ˆìˆ í•™ê³¼
            "https://music.daejin.ac.kr/",                 # ì‹¤ìš©ìŒì•…í•™ê³¼
            
            # ê¸€ë¡œë²Œì‚°ì—…í†µìƒëŒ€í•™
            "https://economics.daejin.ac.kr/",             # ê¸€ë¡œë²Œê²½ì œí•™ê³¼
            "https://sm.daejin.ac.kr/",                    # ê²½ì˜í•™ê³¼
            "https://intlbusiness.daejin.ac.kr/",          # êµ­ì œí†µìƒí•™ê³¼
            "https://camde.daejin.ac.kr/",                 # ì¤‘êµ­ì–´ë¬¸í™”í•™ê³¼
            
            # ê³µê³µì¸ì¬ëŒ€í•™
            "https://law.daejin.ac.kr/",                   # ê³µê³µì¸ì¬ë²•í•™ê³¼
            "https://pai.daejin.ac.kr/",                   # í–‰ì •ì •ë³´í•™ê³¼
            "https://welfare.daejin.ac.kr/",               # ì‚¬íšŒë³µì§€í•™ê³¼
            "https://child.daejin.ac.kr/",                 # ì•„ë™í•™ê³¼
            "https://media.daejin.ac.kr/",                 # ë¯¸ë””ì–´ì»¤ë®¤ë‹ˆì¼€ì´ì…˜í•™ê³¼
            "https://library.daejin.ac.kr/",               # ë¬¸í—Œì •ë³´í•™ê³¼
            
            # ë³´ê±´ê³¼í•™ëŒ€í•™
            "https://medical.daejin.ac.kr/",               # ì˜ìƒëª…ê³¼í•™ê³¼
            "https://nurse.daejin.ac.kr/",                 # ê°„í˜¸í•™ê³¼
            "https://sports.daejin.ac.kr/",                # ìŠ¤í¬ì¸ ê±´ê°•ê³¼í•™ê³¼
            "https://food.daejin.ac.kr/",                  # ì‹í’ˆì˜ì–‘í•™ê³¼
            "https://health.daejin.ac.kr/",                # ë³´ê±´ê²½ì˜í•™ê³¼
            
            # AIìœµí•©ëŒ€í•™
            "https://ce.daejin.ac.kr/",                    # ì»´í“¨í„°ê³µí•™ì „ê³µ
            "https://aidata.daejin.ac.kr/",                # AIë¹…ë°ì´í„°ì „ê³µ
            "https://security.daejin.ac.kr/",              # ìŠ¤ë§ˆíŠ¸ìœµí•©ë³´ì•ˆí•™ê³¼
            
            # ê³µê³¼ëŒ€í•™
            "https://elec.daejin.ac.kr/",                  # ì „ê¸°ê³µí•™ì „ê³µ
            "https://cpe.daejin.ac.kr/",                   # í™”í•™ê³µí•™ì „ê³µ
            "https://arch.daejin.ac.kr/",                  # ê±´ì¶•ê³µí•™ê³¼
            "https://envir.daejin.ac.kr/",                 # ìŠ¤ë§ˆíŠ¸ê±´ì„¤í™˜ê²½ê³µí•™ê³¼
            "https://ie.daejin.ac.kr/",                    # ë°ì´í„°ê²½ì˜ì‚°ì—…ê³µí•™ê³¼
            "https://mech.daejin.ac.kr/",                  # ITê¸°ê³„ê³µí•™ê³¼
            
            # êµ­ì œí˜‘ë ¥ëŒ€í•™
            "https://korean.daejin.ac.kr/",                # í•œêµ­í•™ê³¼
            
            # ê¸°íƒ€ ì¤‘ìš” ê¸°ê´€
            "https://admission.daejin.ac.kr/",             # ì…í•™ì²˜
            "https://job.daejin.ac.kr/",                   # ëŒ€í•™ì¼ìë¦¬í”ŒëŸ¬ìŠ¤ì„¼í„°
            "https://ctl.daejin.ac.kr/",                   # êµìˆ˜í•™ìŠµì§€ì›ì„¼í„°
            "https://liberal.daejin.ac.kr/",               # ìƒìƒêµì–‘ëŒ€í•™
            "https://international.daejin.ac.kr/",         # êµ­ì œêµë¥˜ì›
            "https://dormitory.daejin.ac.kr/",             # ìƒí™œê´€
            "https://rotc.daejin.ac.kr/",                  # í•™êµ°ë‹¨
            "https://counseling.daejin.ac.kr/",            # í•™ìƒìƒí™œìƒë‹´ì„¼í„°
            
            # ì—°êµ¬ê¸°ê´€
            "https://tech.daejin.ac.kr/",                  # ì‚°í•™í˜‘ë ¥ë‹¨
            "https://djss.daejin.ac.kr/",                  # ëŒ€ì§„ì§€ì—­ìƒìƒì„¼í„°
            "https://hcc.daejin.ac.kr/",                   # ê³µë™ê¸°ê¸°ì„¼í„°
            
            # ë„ì„œê´€ (ì œí•œì )
            "https://library.daejin.ac.kr/main_main.mir",
        ]
        
        # ë„ë©”ì¸ë³„ ì „ëµì  ê¹Šì´ ì„¤ì • (ëŒ€í­ í™•ì¥)
        self.domain_strategies = {
            # ë„ì„œê´€ ì œí•œì 
            'library.daejin.ac.kr': {'max_depth': 2, 'priority': 'low'},
            'ebook.daejin.ac.kr': {'max_depth': 1, 'filter_patterns': [r'product/list', r'search']},
            
            # ë©”ì¸ ì‚¬ì´íŠ¸ ë§¤ìš° ê¹Šê²Œ
            'www.daejin.ac.kr': {'max_depth': 25, 'priority': 'highest'},
            
            # AIìœµí•©ëŒ€í•™ (ìµœìš°ì„ )
            'ce.daejin.ac.kr': {'max_depth': 30, 'priority': 'highest'},
            'aidata.daejin.ac.kr': {'max_depth': 25, 'priority': 'highest'},
            'security.daejin.ac.kr': {'max_depth': 25, 'priority': 'highest'},
            
            # ì¸ë¬¸ì˜ˆìˆ ëŒ€í•™
            'eng.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'history.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'literature.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'arte.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'design.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'djfilm.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'acting.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'music.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            
            # ê¸€ë¡œë²Œì‚°ì—…í†µìƒëŒ€í•™
            'economics.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'sm.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'intlbusiness.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'camde.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            
            # ê³µê³µì¸ì¬ëŒ€í•™
            'law.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'pai.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'welfare.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'child.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'media.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            
            # ë³´ê±´ê³¼í•™ëŒ€í•™
            'medical.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'nurse.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'sports.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'food.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'health.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            
            # ê³µê³¼ëŒ€í•™
            'elec.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'cpe.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'arch.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'envir.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'ie.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            'mech.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            
            # êµ­ì œí˜‘ë ¥ëŒ€í•™
            'korean.daejin.ac.kr': {'max_depth': 20, 'priority': 'high'},
            
            # ê¸°íƒ€ ì¤‘ìš” ê¸°ê´€
            'admission.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'job.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'ctl.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'liberal.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'international.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'dormitory.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'rotc.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'counseling.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            
            # ì—°êµ¬ê¸°ê´€
            'tech.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'djss.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            'hcc.daejin.ac.kr': {'max_depth': 15, 'priority': 'high'},
            
            # ê¸°ë³¸ê°’ (ìƒˆë¡œìš´ ë„ë©”ì¸)
            'default': {'max_depth': 15, 'priority': 'medium'}
        }
        
        # ìš°ì„ ìˆœìœ„ íŒ¨í„´ (ê²Œì‹œíŒ ì¤‘ì‹¬)
        self.priority_patterns = [
            (r'/bbs/.*/artclView\.do', 100),      # ê²Œì‹œíŒ ê²Œì‹œë¬¼ (ìµœê³  ìš°ì„ ìˆœìœ„)
            (r'/ce/2518/subview\.do\?enc=', 95),  # ì»´ê³µê³¼ ê²Œì‹œê¸€
            (r'/bbs/.*/', 80),                    # ê²Œì‹œíŒ ëª©ë¡
            (r'/board/.*/', 75),                  # ê²Œì‹œíŒ ê´€ë ¨
            (r'/notice/', 70),                    # ê³µì§€ì‚¬í•­
            (r'/subview\.do', 60),                # ì„œë¸Œí˜ì´ì§€
            (r'/curriculum/', 50),                # êµìœ¡ê³¼ì •
            (r'/professor/', 50),                 # êµìˆ˜ì§„
            (r'/faculty/', 50),                   # êµìˆ˜ì§„
            (r'/student/', 45),                   # í•™ìƒ ê´€ë ¨
            (r'/admission/', 40),                 # ì…í•™ ê´€ë ¨
            (r'/employment/', 40),                # ì·¨ì—… ê´€ë ¨
            (r'/index\.do', 30),                  # ë©”ì¸í˜ì´ì§€
        ]
        
        # ì œì™¸í•  íŒ¨í„´ (ìµœì†Œí™”)
        self.exclude_patterns = [
            # ì „ìì±… ê²€ìƒ‰ í˜ì´ì§€ë§Œ ì œì™¸ (ì¼ë°˜ í˜ì´ì§€ëŠ” í—ˆìš©)
            r'ebook\.daejin\.ac\.kr.*search',
            r'ebook\.daejin\.ac\.kr.*keyword=',
            
            # ì‹œìŠ¤í…œ í˜ì´ì§€ ì œì™¸
            r'groupware\.daejin\.ac\.kr',
            r'sso\.daejin\.ac\.kr/login',
            r'webmail\.daejin\.ac\.kr',
            
            # ë‹¤ìš´ë¡œë“œ íŒŒì¼ë§Œ ì œì™¸
            r'\.pdf$',
            r'\.doc$',
            r'\.hwp$',
            r'/download\.do',
            
            # ì¤‘ë³µ ë°©ì§€ (ì•µì»¤ ë§í¬)
            r'#this$',
            r'#none$',
            r'#link$',
        ]

    def load_existing_data(self):
        """ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ"""
        logger.info("ğŸ” ê¸°ì¡´ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        if not os.path.exists(self.existing_data_dir):
            logger.warning(f"ê¸°ì¡´ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.existing_data_dir}")
            return
        
        count = 0
        for filename in os.listdir(self.existing_data_dir):
            if filename.endswith('.txt'):
                filepath = os.path.join(self.existing_data_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        url_match = re.search(r'\[URL\] (https?://[^\n]+)', content)
                        if url_match:
                            url = url_match.group(1).strip()
                            self.existing_urls.add(url)
                            count += 1
                except Exception as e:
                    logger.warning(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {filename} - {e}")
        
        logger.info(f"âœ… ê¸°ì¡´ URL {count:,}ê°œ ë¡œë“œ ì™„ë£Œ")

    def is_excluded_url(self, url):
        """URL ì œì™¸ ì—¬ë¶€ ê²€ì‚¬"""
        for pattern in self.exclude_patterns:
            if re.search(pattern, url):
                return True
        return False

    def get_url_priority(self, url):
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # ê¸°ì¡´ì— í¬ë¡¤ë§ëœ URLì€ ë‚®ì€ ìš°ì„ ìˆœìœ„
        if url in self.existing_urls:
            return 0
        
        # ì œì™¸ íŒ¨í„´ ê²€ì‚¬
        if self.is_excluded_url(url):
            return 0
        
        # ìš°ì„ ìˆœìœ„ íƒ€ê²Ÿì— í¬í•¨ëœ ê²½ìš°
        if url in self.priority_targets:
            return 1000
        
        # íŒ¨í„´ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        for pattern, priority in self.priority_patterns:
            if re.search(pattern, url):
                return priority
        
        return 10  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„

    def get_domain_strategy(self, domain):
        """ë„ë©”ì¸ë³„ ì „ëµ ë°˜í™˜"""
        return self.domain_strategies.get(domain, self.domain_strategies['default'])

    def should_crawl_url(self, url, depth):
        """URL í¬ë¡¤ë§ ì—¬ë¶€ ê²°ì •"""
        # ì´ë¯¸ ë°©ë¬¸í•œ URL
        if url in self.visited or url in self.existing_urls:
            return False
        
        # ì œì™¸ íŒ¨í„´
        if self.is_excluded_url(url):
            return False
        
        # ë„ë©”ì¸ ì „ëµ í™•ì¸
        parsed = urlparse(url)
        domain = parsed.netloc
        strategy = self.get_domain_strategy(domain)
        
        # ìŠ¤í‚µ ë„ë©”ì¸
        if strategy.get('skip', False):
            return False
        
        # ê¹Šì´ ì œí•œ
        max_depth = strategy['max_depth']
        if depth > max_depth:
            return False
        
        # ì „ìì±… í•„í„°ë§
        if 'ebook.daejin.ac.kr' in domain:
            filter_patterns = strategy.get('filter_patterns', [])
            for pattern in filter_patterns:
                if re.search(pattern, url):
                    return depth <= 1  # ì „ìì±…ì€ ë§¤ìš° ì–•ê²Œë§Œ
        
        return True

    async def crawl_with_selenium(self, url, depth=0):
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
            
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(15)
            
            driver.get(url)
            time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            content = driver.page_source
            soup = BeautifulSoup(content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            text_content = self.extract_clean_text(soup)
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                if self.should_crawl_url(full_url, depth + 1):
                    priority = self.get_url_priority(full_url)
                    if priority > 0:
                        links.append((full_url, priority))
            
            driver.quit()
            
            return {
                'url': url,
                'content': text_content,
                'links': links,
                'depth': depth,
                'domain': urlparse(url).netloc,
                'length': len(text_content),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Selenium í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            try:
                driver.quit()
            except:
                pass
            return None

    def extract_clean_text(self, soup):
        """ê¹”ë”í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        text = soup.get_text()
        lines = [line.strip() for line in text.splitlines()]
        text = '\n'.join(line for line in lines if line and len(line) > 2)
        
        return text

    def save_page_content(self, page_data):
        """í˜ì´ì§€ ë‚´ìš© ì €ì¥"""
        if not page_data or len(page_data['content'].strip()) < 100:
            return False
        
        filename = f"strategic_page_{len(self.saved_texts):05d}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        content = f"[URL] {page_data['url']}\n"
        content += f"[DEPTH] {page_data['depth']}\n"
        content += f"[DOMAIN] {page_data['domain']}\n"
        content += f"[TIMESTAMP] {page_data['timestamp']}\n"
        content += f"[LENGTH] {page_data['length']}\n\n"
        content += page_data['content']
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.saved_texts.append(filepath)
            self.saved_urls.append(page_data['url'])
            self.domain_stats[page_data['domain']] += 1
            
            logger.info(f"ğŸ’¾ ì €ì¥: {filename} ({page_data['domain']}) - {page_data['length']}ì")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    def save_state(self):
        """ìƒíƒœ ì €ì¥"""
        state = {
            'visited': list(self.visited),
            'to_visit': list(self.to_visit),
            'saved_urls': self.saved_urls,
            'url_depths': self.url_depths,
            'domain_stats': dict(self.domain_stats),
            'timestamp': datetime.now().isoformat()
        }
        
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ìƒíƒœ ì €ì¥ ì˜¤ë¥˜: {e}")

    async def run_strategic_crawling(self, max_pages=3000):
        """ì „ëµì  ë³´ì™„ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸš€ ì „ëµì  ë³´ì™„ í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"ğŸ“Š ê¸°ì¡´ URL: {len(self.existing_urls):,}ê°œ")
        logger.info(f"ğŸ¯ ìš°ì„ ìˆœìœ„ íƒ€ê²Ÿ: {len(self.priority_targets)}ê°œ")
        
        # ìš°ì„ ìˆœìœ„ íƒ€ê²Ÿìœ¼ë¡œ ì‹œì‘
        for url in self.priority_targets:
            if url not in self.existing_urls:
                priority = self.get_url_priority(url)
                self.to_visit.add((url, 0, priority))
        
        processed = 0
        
        with ThreadPoolExecutor(max_workers=self.max_selenium_instances) as executor:
            while self.to_visit and processed < max_pages:
                # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
                current_batch = sorted(list(self.to_visit), key=lambda x: x[2], reverse=True)[:self.max_selenium_instances]
                self.to_visit.clear()
                
                # ë³‘ë ¬ ì²˜ë¦¬
                futures = []
                for url, depth, priority in current_batch:
                    if url not in self.visited:
                        future = executor.submit(asyncio.run, self.crawl_with_selenium(url, depth))
                        futures.append(future)
                        self.visited.add(url)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for future in as_completed(futures):
                    try:
                        page_data = future.result()
                        if page_data:
                            # í˜ì´ì§€ ì €ì¥
                            if self.save_page_content(page_data):
                                processed += 1
                            
                            # ìƒˆ ë§í¬ ì¶”ê°€
                            for link_url, link_priority in page_data['links']:
                                if link_url not in self.visited and link_url not in self.existing_urls:
                                    self.to_visit.add((link_url, page_data['depth'] + 1, link_priority))
                    
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
                # ì£¼ê¸°ì  ìƒíƒœ ì €ì¥
                if processed % 50 == 0:
                    self.save_state()
                    logger.info(f"ğŸ“ˆ ì§„í–‰ìƒí™©: {processed}/{max_pages} í˜ì´ì§€ ì²˜ë¦¬")
                    logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ ìˆ˜ì§‘: {dict(self.domain_stats)}")
                
                # ì†ë„ ì¡°ì ˆ
                await asyncio.sleep(1)
        
        # ìµœì¢… ìƒíƒœ ì €ì¥
        self.save_state()
        
        logger.info("âœ… ì „ëµì  í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {processed}ê°œ í˜ì´ì§€")
        logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ í†µê³„: {dict(self.domain_stats)}")

if __name__ == "__main__":
    crawler = StrategicCrawler()
    asyncio.run(crawler.run_strategic_crawling(max_pages=3000))