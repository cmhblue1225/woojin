
import os
import time
import json
import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import pdfplumber
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Selenium ì„¤ì •
def create_driver():
    options = Options()
    options.headless = True
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    return webdriver.Chrome(options=options)

# ì‹œì‘ URL ëª©ë¡
start_urls = [
    "https://www.daejin.ac.kr/sites/daejin/index.do",
    "https://sm.daejin.ac.kr/sm/index.do",
    "https://swcg.daejin.ac.kr/swcg/index.do",
    "https://ce.daejin.ac.kr/ce/index.do",
    "https://eng.daejin.ac.kr/eng/index.do",
    "https://law.daejin.ac.kr/law/index.do",
]

# ìƒíƒœ ì €ì¥ íŒŒì¼
state_file = "crawler_state.json"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

# ë°©ë¬¸í•œ URL, ë°©ë¬¸í•  URL, ì €ì¥ëœ í…ìŠ¤íŠ¸ ê´€ë¦¬
visited = set()
to_visit = set(start_urls)
saved_texts = []
saved_urls = []

# ìƒíƒœ ë¡œë“œ
def load_state():
    global visited, to_visit, saved_texts, saved_urls
    if os.path.exists(state_file):
        with open(state_file, "r", encoding="utf-8") as f:
            state = json.load(f)
            visited.update(state.get("visited", []))
            to_visit.update(state.get("to_visit", []))
            saved_texts.extend(state.get("saved_texts", []))
            saved_urls.extend(state.get("saved_urls", []))
            print(f"ğŸ“‚ ìƒíƒœ ë¡œë“œ: {len(visited)}ê°œì˜ ë°©ë¬¸ URL, {len(to_visit)}ê°œì˜ ëŒ€ê¸° URL")

# ìƒíƒœ ì €ì¥
def save_state():
    state = {
        "visited": list(visited),
        "to_visit": list(to_visit),
        "saved_texts": saved_texts,
        "saved_urls": saved_urls
    }
    with open(state_file, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False)
    print(f"ğŸ’¾ ìƒíƒœ ì €ì¥: {len(visited)}ê°œì˜ ë°©ë¬¸ URL, {len(to_visit)}ê°œì˜ ëŒ€ê¸° URL")

# ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
def is_valid_url(url):
    try:
        parsed = urlparse(url)
        if re.search(r'\.(pdf|jpg|png|gif|doc|docx|zip)$', url, re.IGNORECASE):
            return False
        return parsed.scheme in ("http", "https") and "daejin.ac.kr" in parsed.netloc
    except:
        return False

# í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜
def clean_text(html, url):
    if url.lower().endswith('.pdf'):
        try:
            with pdfplumber.open(html) as pdf:
                text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                return text.strip()
        except Exception as e:
            log_error(url, f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ""
    
    try:
        soup = BeautifulSoup(html, "html.parser")
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for tag in soup.select("header, nav, footer, .sitemap, .quickmenu, script, style, noscript"):
            tag.extract()
        for tag in soup.find_all(class_=re.compile("menu|nav|sitemap|footer|quickmenu|popup", re.I)):
            tag.extract()
        text = soup.get_text(separator="\n", strip=True)
        text = re.sub(r'\n\s*\n', '\n', text).strip()
        return text if text else ""
    except Exception as e:
        log_error(url, f"HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
        return ""

# íŒŒì¼ ì €ì¥ í•¨ìˆ˜
def save_to_file(url, text, idx):
    filename = os.path.join(output_dir, f"page_{idx}.txt")
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"[URL] {url}\n\n{text}")
    saved_texts.append(text)
    saved_urls.append(url)

# ì—ëŸ¬ ë¡œê¹… í•¨ìˆ˜
def log_error(url, error):
    with open(os.path.join(output_dir, "error_log.txt"), "a", encoding="utf-8") as f:
        f.write(f"[ERROR] {url}: {str(error)}\n")

# ì¤‘ë³µ ì½˜í…ì¸  ê°ì§€
def is_duplicate(text):
    if not saved_texts:
        return False
    vectorizer = TfidfVectorizer().fit_transform([text] + saved_texts)
    similarities = cosine_similarity(vectorizer[0:1], vectorizer[1:])[0]
    return any(similarity > 0.95 for similarity in similarities)

# ê²Œì‹œíŒ ê²Œì‹œë¬¼ ë§í¬ ì¶”ì¶œ
def extract_board_links(soup, base_url):
    board_links = set()
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        if re.search(r'/(bbs|artclView\.do|subview\.do)', href, re.IGNORECASE):
            next_url = urljoin(base_url, href)
            if is_valid_url(next_url) and next_url not in visited:
                board_links.add(next_url)
    return board_links

# ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§
def crawl_page(url, driver):
    if url in visited or not is_valid_url(url):
        return set(), None, None

    try:
        print(f"ğŸ“„ í¬ë¡¤ë§ ì¤‘: {url}")
        if url.lower().endswith('.pdf'):
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(url, headers=headers, timeout=10, stream=True)
            response.raise_for_status()
            content = response.content
        else:
            driver.get(url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            content = driver.page_source

        text = clean_text(content, url)
        if not text:
            print(f"âš ï¸ ë¹ˆ í…ìŠ¤íŠ¸: {url}")
            log_error(url, "ë¹ˆ í…ìŠ¤íŠ¸ ë°˜í™˜")
            return set(), None, None

        if is_duplicate(text):
            print(f"âš ï¸ ì¤‘ë³µ ì½˜í…ì¸ : {url}")
            log_error(url, "ì¤‘ë³µ ì½˜í…ì¸  ê°ì§€")
            return set(), None, None

        new_links = set()
        if not url.lower().endswith('.pdf'):
            soup = BeautifulSoup(content, "html.parser")
            new_links.update(extract_board_links(soup, url))
            for tag in soup.find_all("a", href=True):
                next_url = urljoin(url, tag["href"])
                if is_valid_url(next_url) and next_url not in visited:
                    new_links.add(next_url)

        return new_links, text, url

    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {url} â†’ {e}")
        log_error(url, e)
        return set(), None, None

# ë©”ì¸ í¬ë¡¤ë§ ë£¨í”„
def main():
    global visited, to_visit
    load_state()
    idx = len(saved_urls)
    max_workers = 4  # ìŠ¤ë ˆë“œ ìˆ˜ (ì‹œìŠ¤í…œ ì„±ëŠ¥ì— ë”°ë¼ ì¡°ì •)

    while to_visit:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            drivers = [create_driver() for _ in range(max_workers)]
            try:
                url_batch = list(to_visit)[:max_workers]
                to_visit.difference_update(url_batch)
                futures = {executor.submit(crawl_page, url, drivers[i]): url for i, url in enumerate(url_batch)}

                for future in as_completed(futures):
                    url = futures[future]
                    new_links, text, crawled_url = future.result()
                    if text and crawled_url:
                        visited.add(crawled_url)
                        save_to_file(crawled_url, text, idx)
                        idx += 1
                        print(f"âœ… ì €ì¥ ì™„ë£Œ: {crawled_url} (page_{idx}.txt)")
                    to_visit.update(new_links - visited)
                    save_state()

            finally:
                for driver in drivers:
                    driver.quit()

        time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {idx}ê°œì˜ í˜ì´ì§€ ì €ì¥ë¨.")

if __name__ == "__main__":
    main()
