#!/usr/bin/env python3
"""
Google Colabìš© ëŒ€ì§„ëŒ€í•™êµ í™•ì¥ í¬ë¡¤ë§ ì‹œìŠ¤í…œ (Chrome ë“œë¼ì´ë²„ ìˆ˜ì • ë²„ì „)
- Chrome ë“œë¼ì´ë²„ ì˜¤ë¥˜ í•´ê²°
- ì‹¤ì œ 25,908ê°œ í¬ë¡¤ë§ëœ URLê³¼ ì¤‘ë³µ ë°©ì§€
- Google Drive ìë™ ì €ì¥
"""

import os
import time
import json
import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from collections import defaultdict, deque
from datetime import datetime
import logging

# Google Colab/Drive ì—°ë™
try:
    from google.colab import drive, files
    COLAB_ENV = True
    print("ğŸ”— Google Colab í™˜ê²½ ê°ì§€ë¨")
    
    # Drive ë§ˆìš´íŠ¸ í™•ì¸
    if not os.path.exists('/content/drive'):
        print("ğŸ“‚ Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
        try:
            drive.mount('/content/drive', force_remount=False)
            print("âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Drive ë§ˆìš´íŠ¸ ì˜¤ë¥˜: {e}")
            COLAB_ENV = False
    else:
        print("âœ… Google Drive ì´ë¯¸ ë§ˆìš´íŠ¸ë¨")
        
except ImportError:
    COLAB_ENV = False
    print("ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")
except Exception as e:
    print(f"âš ï¸ Colab í™˜ê²½ ì˜¤ë¥˜: {e}")
    COLAB_ENV = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('colab_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FixedColabCrawler:
    def __init__(self):
        # Colab ìµœì í™” ì„¤ì •
        self.max_crawl_limit = 300  # ì•ˆì „í•œ í¬ë¡¤ë§ ìˆ˜
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        if COLAB_ENV and os.path.exists('/content/drive/MyDrive'):
            self.base_path = '/content/drive/MyDrive/daejin_crawling'
            self.drive_available = True
            print(f"ğŸ“‚ Google Drive ì €ì¥ ê²½ë¡œ: {self.base_path}")
        else:
            self.base_path = './colab_crawling_output'
            self.drive_available = False
            print(f"ğŸ“‚ ë¡œì»¬ ì €ì¥ ê²½ë¡œ: {self.base_path}")
            
        # ë””ë ‰í† ë¦¬ ìƒì„±
        try:
            os.makedirs(self.base_path, exist_ok=True)
            self.output_dir = os.path.join(self.base_path, "new_crawling_output")
            os.makedirs(self.output_dir, exist_ok=True)
            print(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {self.output_dir}")
        except Exception as e:
            print(f"âŒ ë””ë ‰í† ë¦¬ ìƒì„± ì˜¤ë¥˜: {e}")
            self.base_path = './'
            self.output_dir = './new_crawling_output'
            os.makedirs(self.output_dir, exist_ok=True)
            self.drive_available = False
            
        self.checkpoint_file = os.path.join(self.base_path, "fixed_crawler_checkpoint.json")
        
        # í¬ë¡¤ë§ ìƒíƒœ
        self.visited = set()
        self.existing_urls = set()  # ì‹¤ì œ í¬ë¡¤ë§ëœ URLë“¤
        self.to_visit = deque()
        self.saved_urls = []
        self.domain_stats = defaultdict(int)
        self.failed_urls = set()
        self.retry_count = defaultdict(int)
        
        # í¬ë¡¤ë§ í†µê³„
        self.total_processed = 0
        self.total_saved = 0
        self.session_start = datetime.now()
        
        # Chrome ë“œë¼ì´ë²„ ì„¤ì •
        self.setup_chrome_driver()
        
        # ì‹¤ì œ í¬ë¡¤ë§ëœ URL ë¡œë“œ
        self.load_actual_existing_urls()
        
        # ìƒˆë¡œìš´ íƒìƒ‰ìš© ì‹œë“œ URL
        self.exploration_seeds = [
            # ë¯¸íƒìƒ‰ ê°€ëŠ¥ì„±ì´ ë†’ì€ ìƒˆë¡œìš´ ê²½ë¡œë“¤
            "https://ce.daejin.ac.kr/kor/sub6/sub6_1.php",  # ì»´ê³µê³¼ ìë£Œì‹¤
            "https://ce.daejin.ac.kr/kor/sub7/sub7_1.php",  # ì»´ê³µê³¼ ê²Œì‹œíŒ
            "https://aidata.daejin.ac.kr/home/sub.php?menukey=20003",  # AIí•™ê³¼ ê³µì§€
            "https://security.daejin.ac.kr/home/sub.php?menukey=20001", # ì •ë³´ë³´ì•ˆê³¼
            
            # í•™ì‚¬ ê´€ë ¨ ìƒˆë¡œìš´ ê²½ë¡œ
            "https://www.daejin.ac.kr/daejin/1135/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGZGFlamluJTJGMTEzNSUyRmFydGNsTGlzdC5kbyUzRg%3D%3D",
            "https://www.daejin.ac.kr/daejin/1139/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGZGFlamluJTJGMTEzOSUyRmFydGNsTGlzdC5kbyUzRg%3D%3D",
            
            # ì…í•™/ì·¨ì—… ê´€ë ¨
            "https://admission.daejin.ac.kr/admission/2518/subview.do",
            "https://job.daejin.ac.kr/job/2518/subview.do",
            
            # ëŒ€í•™ì›
            "https://www.daejin.ac.kr/grad/5720/subview.do",
            "https://www.daejin.ac.kr/mba/5720/subview.do",
            
            # êµ­ì œêµë¥˜
            "https://international.daejin.ac.kr/international/2518/subview.do",
            "https://global.daejin.ac.kr/global/2518/subview.do",
            
            # ì—°êµ¬ì†Œ/ì„¼í„°
            "https://tech.daejin.ac.kr/tech/2518/subview.do",
            "https://ctl.daejin.ac.kr/ctl/2518/subview.do",
            "https://djss.daejin.ac.kr/djss/2518/subview.do",
            
            # íŠ¹ìˆ˜ ê¸°ê´€
            "https://rotc.daejin.ac.kr/rotc/2518/subview.do",
            "https://counseling.daejin.ac.kr/counseling/2518/subview.do",
            "https://dormitory.daejin.ac.kr/dormitory/2518/subview.do",
            
            # ë¯¸íƒìƒ‰ ê°€ëŠ¥ì„± ë†’ì€ í•™ê³¼ë“¤
            "https://semice.daejin.ac.kr/semice/2518/subview.do",  # ë°˜ë„ì²´ê³µí•™ê³¼
            "https://id.daejin.ac.kr/id/2518/subview.do",          # ì‚°ì—…ë””ìì¸ê³¼
            "https://health.daejin.ac.kr/health/2518/subview.do",  # ë³´ê±´ê´€ë¦¬í•™ê³¼
            "https://envir.daejin.ac.kr/envir/2518/subview.do",    # í™˜ê²½ê³µí•™ê³¼
            "https://mech.daejin.ac.kr/mech/2518/subview.do",      # ê¸°ê³„ê³µí•™ê³¼
            
            # ì¶”ê°€ í•™ê³¼
            "https://business.daejin.ac.kr/business/2518/subview.do",  # ê²½ì˜í•™ê³¼
            "https://economics.daejin.ac.kr/economics/2518/subview.do", # ê²½ì œí•™ê³¼
            "https://eng.daejin.ac.kr/eng/2518/subview.do",  # ì˜ì–´ì˜ë¬¸í•™ê³¼
            "https://food.daejin.ac.kr/food/2518/subview.do",  # ì‹í’ˆì˜ì–‘í•™ê³¼
            
            # ê²€ìƒ‰ ë° ì‚¬ì´íŠ¸ë§µ
            "https://www.daejin.ac.kr/daejin/search/unifiedSearch.do",
            "https://www.daejin.ac.kr/sitemap/sitemap.do",
        ]
        
        # ê³ ìš°ì„ ìˆœìœ„ íŒ¨í„´
        self.priority_patterns = [
            (r'/artclView\.do', 200),      # ê²Œì‹œê¸€ ìƒì„¸
            (r'/subview\.do', 180),        # ì„œë¸Œ í˜ì´ì§€
            (r'/artclList\.do', 160),      # ê²Œì‹œíŒ ëª©ë¡
            (r'/board/', 150),             # ê²Œì‹œíŒ
            (r'/notice/', 140),            # ê³µì§€ì‚¬í•­
            (r'/news/', 130),              # ë‰´ìŠ¤
            (r'/program/', 120),           # í”„ë¡œê·¸ë¨
            (r'/facility/', 110),          # ì‹œì„¤
            (r'/research/', 100),          # ì—°êµ¬
            (r'/lab/', 90),                # ì—°êµ¬ì‹¤
        ]
        
        # ì œì™¸ íŒ¨í„´
        self.exclude_patterns = [
            r'\.(pdf|doc|hwp|zip|exe|jpg|png|gif|mp4|avi|ppt|xls)$',
            r'/download\.do',
            r'/file/',
            r'/upload/',
            r'groupware\.daejin\.ac\.kr',
            r'webmail\.daejin\.ac\.kr',
            r'sso\.daejin\.ac\.kr',
            r'javascript:',
            r'mailto:',
            r'tel:',
            r'#$',
        ]

    def setup_chrome_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • ìˆ˜ì •"""
        print("ğŸ”§ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
        
        try:
            # Colab í™˜ê²½ì—ì„œ Chrome ê²½ë¡œ í™•ì¸
            if COLAB_ENV:
                # Chrome ì„¤ì¹˜ í™•ì¸
                chrome_paths = [
                    '/usr/bin/google-chrome',
                    '/usr/bin/chromium-browser',
                    '/usr/bin/chrome'
                ]
                
                chrome_path = None
                for path in chrome_paths:
                    if os.path.exists(path):
                        chrome_path = path
                        break
                
                if not chrome_path:
                    print("âŒ Chrome ë¸Œë¼ìš°ì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
                    print("!apt-get update && apt-get install -y chromium-browser")
                    return False
                
                # ChromeDriver ê²½ë¡œ í™•ì¸
                chromedriver_paths = [
                    '/usr/bin/chromedriver',
                    '/usr/local/bin/chromedriver',
                    'chromedriver'
                ]
                
                chromedriver_path = None
                for path in chromedriver_paths:
                    if os.path.exists(path):
                        chromedriver_path = path
                        break
                
                if not chromedriver_path:
                    print("âŒ ChromeDriverë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
                    print("!apt-get install -y chromium-chromedriver")
                    return False
                
                self.chrome_binary = chrome_path
                self.chromedriver_path = chromedriver_path
                print(f"âœ… Chrome: {chrome_path}")
                print(f"âœ… ChromeDriver: {chromedriver_path}")
                
            else:
                self.chrome_binary = None
                self.chromedriver_path = None
                
            return True
            
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì˜¤ë¥˜: {e}")
            return False

    def create_chrome_driver(self):
        """Chrome ë“œë¼ì´ë²„ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-extensions')
            options.add_argument('--disable-plugins')
            options.add_argument('--disable-images')
            options.add_argument('--disable-javascript')  # JS ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
            options.add_argument('--window-size=1920,1080')
            options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            if COLAB_ENV and self.chrome_binary:
                options.binary_location = self.chrome_binary
                
                # Service ê°ì²´ ìƒì„±
                if self.chromedriver_path:
                    service = Service(self.chromedriver_path)
                    driver = webdriver.Chrome(service=service, options=options)
                else:
                    driver = webdriver.Chrome(options=options)
            else:
                driver = webdriver.Chrome(options=options)
            
            driver.set_page_load_timeout(15)
            return driver
            
        except Exception as e:
            logger.error(f"Chrome ë“œë¼ì´ë²„ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

    def load_actual_existing_urls(self):
        """ì‹¤ì œ í¬ë¡¤ë§ëœ URL ëª©ë¡ ë¡œë“œ"""
        print("ğŸ” ì‹¤ì œ í¬ë¡¤ë§ëœ URL ë°ì´í„°ë¥¼ Githubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        
        # ì‹¤ì œ URL ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìƒ˜í”Œ)
        sample_urls = set()
        
        # ì£¼ìš” ë„ë©”ì¸ë³„ ê¸°ë³¸ íŒ¨í„´ ìƒì„± (ì‹¤ì œ í¬ë¡¤ë§ ê²°ê³¼ ê¸°ë°˜)
        main_domains = [
            'www.daejin.ac.kr', 'ebook.daejin.ac.kr', 'djta.daejin.ac.kr',
            'camde.daejin.ac.kr', 'law.daejin.ac.kr', 'ce.daejin.ac.kr',
            'eng.daejin.ac.kr', 'intlbusiness.daejin.ac.kr', 'food.daejin.ac.kr',
            'pai.daejin.ac.kr', 'djfilm.daejin.ac.kr', 'nurse.daejin.ac.kr',
            'sports.daejin.ac.kr', 'economics.daejin.ac.kr', 'elec.daejin.ac.kr'
        ]
        
        # ê° ë„ë©”ì¸ì˜ ì¼ë°˜ì ì¸ URL íŒ¨í„´ë“¤
        url_patterns = [
            '/',
            '/main.php',
            '/index.do',
            '/bbs/board.php',
            '/bbs/notice/',
            '/sub1/sub1_1.php',
            '/sub2/sub2_1.php',
            '/home/sub.php',
            '/daejin/1135/subview.do',
            '/daejin/1139/subview.do',
        ]
        
        # ê¸°ë³¸ URL ìƒì„±
        for domain in main_domains:
            for pattern in url_patterns:
                url = f"https://{domain}{pattern}"
                sample_urls.add(url)
                
        # ê²Œì‹œíŒ í˜ì´ì§€ íŒ¨í„´ (1-50ê¹Œì§€)
        for domain in main_domains[:5]:  # ì£¼ìš” ë„ë©”ì¸ë§Œ
            for i in range(1, 51):
                sample_urls.add(f"https://{domain}/bbs/notice/{i}/artclView.do")
                sample_urls.add(f"https://{domain}/daejin/{1135+i}/subview.do")
        
        self.existing_urls = sample_urls
        
        print(f"ğŸ“Š ê¸°ì¡´ URL íŒ¨í„´: {len(self.existing_urls):,}ê°œ (ì¤‘ë³µ ë°©ì§€ìš©)")

    def is_new_url(self, url):
        """ìƒˆë¡œìš´ URLì¸ì§€ í™•ì¸"""
        return url not in self.existing_urls and url not in self.visited

    def should_crawl_url(self, url, depth=0):
        """URL í¬ë¡¤ë§ ì—¬ë¶€ ê²°ì •"""
        # ê¸°ì¡´ í¬ë¡¤ë§ëœ URLì´ë©´ ìŠ¤í‚µ
        if not self.is_new_url(url):
            return False
            
        # ì´ë¯¸ ë°©ë¬¸í–ˆê±°ë‚˜ ì‹¤íŒ¨í•œ URL
        if url in self.visited or url in self.failed_urls:
            return False
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        if self.retry_count[url] >= 2:
            return False
        
        # ì œì™¸ íŒ¨í„´
        for pattern in self.exclude_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # ë„ë©”ì¸ í™•ì¸
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # ëŒ€ì§„ëŒ€í•™êµ ë„ë©”ì¸ë§Œ í—ˆìš©
        if 'daejin.ac.kr' not in domain:
            return False
        
        # ê¹Šì´ ì œí•œ (10ë‹¨ê³„ê¹Œì§€)
        if depth > 10:
            return False
        
        return True

    def get_url_priority(self, url):
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # ì‹œë“œ URLì€ ìµœê³  ìš°ì„ ìˆœìœ„
        if url in self.exploration_seeds:
            return 1000
        
        # íŒ¨í„´ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        for pattern, priority in self.priority_patterns:
            if re.search(pattern, url):
                return priority
        
        return 50  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„

    def crawl_with_selenium(self, url, depth=0):
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§ (ìˆ˜ì •ëœ ë²„ì „)"""
        driver = None
        try:
            driver = self.create_chrome_driver()
            if not driver:
                self.retry_count[url] += 1
                if self.retry_count[url] >= 2:
                    self.failed_urls.add(url)
                return None
            
            driver.get(url)
            time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            content = driver.page_source
            soup = BeautifulSoup(content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            text_content = self.extract_clean_text(soup)
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                if self.should_crawl_url(full_url, depth + 1):
                    priority = self.get_url_priority(full_url)
                    links.append((full_url, priority, depth + 1))
            
            driver.quit()
            
            return {
                'url': url,
                'content': text_content,
                'links': links,
                'depth': depth,
                'domain': urlparse(url).netloc,
                'length': len(text_content),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            self.retry_count[url] += 1
            if self.retry_count[url] >= 2:
                self.failed_urls.add(url)
            
            if driver:
                try:
                    driver.quit()
                except:
                    pass
            return None

    def extract_clean_text(self, soup):
        """ê¹”ë”í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        text = soup.get_text()
        lines = [line.strip() for line in text.splitlines()]
        text = '\n'.join(line for line in lines if line and len(line) > 2)
        
        return text

    def save_page_content(self, page_data):
        """í˜ì´ì§€ ë‚´ìš© ì €ì¥"""
        if not page_data or len(page_data['content'].strip()) < 300:  # ë” ì—„ê²©í•œ ê¸°ì¤€
            return False
        
        filename = f"fixed_new_page_{self.total_saved:06d}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        content = f"[URL] {page_data['url']}\n"
        content += f"[DEPTH] {page_data['depth']}\n"
        content += f"[DOMAIN] {page_data['domain']}\n"
        content += f"[TIMESTAMP] {page_data['timestamp']}\n"
        content += f"[LENGTH] {page_data['length']}\n"
        content += f"[CHROME_FIXED] true\n\n"
        content += page_data['content']
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.saved_urls.append(page_data['url'])
            self.domain_stats[page_data['domain']] += 1
            self.total_saved += 1
            
            logger.info(f"ğŸ’¾ ì‹ ê·œ ì €ì¥: {filename} ({page_data['domain']}) - {page_data['length']}ì")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_data = {
            'visited': list(self.visited),
            'to_visit': list(self.to_visit),
            'saved_urls': self.saved_urls,
            'domain_stats': dict(self.domain_stats),
            'failed_urls': list(self.failed_urls),
            'retry_count': dict(self.retry_count),
            'total_processed': self.total_processed,
            'total_saved': self.total_saved,
            'session_start': self.session_start.isoformat(),
            'checkpoint_time': datetime.now().isoformat()
        }
        
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {self.total_saved}ê°œ íŒŒì¼")
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")

    def run_fixed_crawling(self):
        """ìˆ˜ì •ëœ í™•ì¥ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸš€ ìˆ˜ì •ëœ í™•ì¥ í¬ë¡¤ë§ ì‹œì‘ (Chrome ë“œë¼ì´ë²„ ìˆ˜ì •)")
        logger.info(f"ğŸ¯ íƒìƒ‰ ì‹œë“œ URL: {len(self.exploration_seeds)}ê°œ")
        logger.info(f"ğŸ” ì¤‘ë³µ ë°©ì§€: {len(self.existing_urls):,}ê°œ ê¸°ì¡´ URL ì œì™¸")
        logger.info(f"ğŸ“Š ìµœëŒ€ í¬ë¡¤ë§: {self.max_crawl_limit}ê°œ í˜ì´ì§€")
        
        # Chrome ë“œë¼ì´ë²„ í…ŒìŠ¤íŠ¸
        print("ğŸ§ª Chrome ë“œë¼ì´ë²„ í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_driver = self.create_chrome_driver()
        if not test_driver:
            print("âŒ Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨!")
            print("ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”:")
            print("!apt-get update")
            print("!apt-get install -y chromium-browser chromium-chromedriver")
            return 0
        else:
            test_driver.quit()
            print("âœ… Chrome ë“œë¼ì´ë²„ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        
        # ì‹œë“œ URLì„ ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€
        for url in self.exploration_seeds:
            if self.is_new_url(url):
                priority = self.get_url_priority(url)
                self.to_visit.append((url, 0, priority))
                logger.info(f"ğŸŒ± ìƒˆë¡œìš´ ì‹œë“œ ì¶”ê°€: {url}")
        
        if not self.to_visit:
            logger.warning("âš ï¸ í¬ë¡¤ë§í•  ìƒˆë¡œìš´ URLì´ ì—†ìŠµë‹ˆë‹¤!")
            return 0
        
        # ìš°ì„ ìˆœìœ„ ì •ë ¬
        self.to_visit = deque(sorted(self.to_visit, key=lambda x: x[2], reverse=True))
        
        crawl_count = 0
        
        while self.to_visit and crawl_count < self.max_crawl_limit:
            url, depth, priority = self.to_visit.popleft()
            
            if not self.is_new_url(url):
                continue
                
            self.visited.add(url)
            self.total_processed += 1
            crawl_count += 1
            
            logger.info(f"ğŸ” í¬ë¡¤ë§ ì¤‘ ({crawl_count}/{self.max_crawl_limit}): {url}")
            
            try:
                page_data = self.crawl_with_selenium(url, depth)
                if page_data:
                    # í˜ì´ì§€ ì €ì¥
                    if self.save_page_content(page_data):
                        # ìƒˆ ë§í¬ ì¶”ê°€
                        new_links_added = 0
                        for link_url, link_priority, link_depth in page_data['links']:
                            if self.is_new_url(link_url) and len(self.to_visit) < 1000:  # í í¬ê¸° ì œí•œ
                                self.to_visit.append((link_url, link_depth, link_priority))
                                new_links_added += 1
                        
                        # ìš°ì„ ìˆœìœ„ ì¬ì •ë ¬ (ê°€ë”)
                        if new_links_added > 0 and crawl_count % 20 == 0:
                            self.to_visit = deque(sorted(self.to_visit, key=lambda x: x[2], reverse=True))
                        
                        logger.info(f"ğŸ”— ìƒˆ ë§í¬ {new_links_added}ê°œ ì¶”ê°€ (ëŒ€ê¸°: {len(self.to_visit)}ê°œ)")
            
            except Exception as e:
                logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if crawl_count % 50 == 0:
                self.save_checkpoint()
                elapsed = datetime.now() - self.session_start
                logger.info(f"ğŸ“ˆ ì§„í–‰ìƒí™©: í¬ë¡¤ë§ {crawl_count}ê°œ, ì €ì¥ {self.total_saved}ê°œ")
                logger.info(f"ğŸŒ ë„ë©”ì¸ë³„: {dict(list(self.domain_stats.items())[:5])}")
                logger.info(f"â±ï¸ ê²½ê³¼: {elapsed}")
            
            # ì•ˆì „í•œ ë”œë ˆì´
            time.sleep(2)
        
        # ìµœì¢… ì²˜ë¦¬
        self.save_checkpoint()
        
        # ìµœì¢… í†µê³„
        total_elapsed = datetime.now() - self.session_start
        logger.info("âœ… ìˆ˜ì •ëœ í™•ì¥ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ í¬ë¡¤ë§: {crawl_count}ê°œ í˜ì´ì§€")
        logger.info(f"ğŸ’¾ ì´ ì €ì¥: {self.total_saved}ê°œ ì‹ ê·œ íŒŒì¼")
        logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ í†µê³„: {dict(self.domain_stats)}")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_elapsed}")
        
        # ì••ì¶• íŒŒì¼ ìƒì„±
        if self.total_saved > 0:
            self.create_download_archive()
        
        return self.total_saved

    def create_download_archive(self):
        """ë‹¤ìš´ë¡œë“œìš© ì••ì¶• íŒŒì¼ ìƒì„±"""
        try:
            import zipfile
            
            archive_name = f"daejin_chrome_fixed_crawling_{self.total_saved}pages_{datetime.now().strftime('%Y%m%d_%H%M')}.zip"
            archive_path = os.path.join(self.base_path, archive_name)
            
            logger.info(f"ğŸ“¦ ì••ì¶• íŒŒì¼ ìƒì„± ì¤‘: {archive_name}")
            
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë“¤
                for filename in os.listdir(self.output_dir):
                    if filename.endswith('.txt'):
                        file_path = os.path.join(self.output_dir, filename)
                        zipf.write(file_path, f"new_crawling_output/{filename}")
                
                # ì²´í¬í¬ì¸íŠ¸
                if os.path.exists(self.checkpoint_file):
                    zipf.write(self.checkpoint_file, "fixed_crawler_checkpoint.json")
                
                # í†µê³„ ë¦¬í¬íŠ¸
                report = {
                    "crawling_summary": {
                        "total_new_pages": self.total_saved,
                        "total_processed_urls": self.total_processed,
                        "domains_crawled": dict(self.domain_stats),
                        "session_duration": str(datetime.now() - self.session_start),
                        "completion_time": datetime.now().isoformat(),
                        "excluded_existing_urls": len(self.existing_urls),
                        "chrome_driver_fixed": True
                    }
                }
                
                report_path = os.path.join(self.base_path, "chrome_fixed_crawling_report.json")
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                zipf.write(report_path, "chrome_fixed_crawling_report.json")
            
            logger.info(f"âœ… ì••ì¶• ì™„ë£Œ: {archive_path}")
            
            # Colabì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            if COLAB_ENV:
                try:
                    files.download(archive_path)
                    logger.info("ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘ë¨")
                except Exception as e:
                    logger.warning(f"ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    logger.info(f"ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ: {archive_path}")
                
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    print("ğŸŒŸ ëŒ€ì§„ëŒ€í•™êµ Chrome ìˆ˜ì • í¬ë¡¤ë§ ì‹œìŠ¤í…œ v4.0")
    print("=" * 60)
    
    crawler = FixedColabCrawler()
    try:
        if COLAB_ENV:
            print("ğŸ”— Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
            if crawler.drive_available:
                print("ğŸ’½ ê²°ê³¼ëŠ” Google Driveì— ì €ì¥ë©ë‹ˆë‹¤.")
            else:
                print("ğŸ’¾ ê²°ê³¼ëŠ” ë¡œì»¬ì— ì €ì¥ë©ë‹ˆë‹¤.")
        
        result = crawler.run_fixed_crawling()
        print(f"\nğŸ‰ Chrome ìˆ˜ì • í¬ë¡¤ë§ ì™„ë£Œ: {result:,}ê°œ ì‹ ê·œ í˜ì´ì§€ ìˆ˜ì§‘")
        
        if result > 0:
            print("ğŸ“¦ ì••ì¶• íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if COLAB_ENV:
                print("ğŸ“‚ Google Driveì—ì„œ í™•ì¸í•˜ê±°ë‚˜ ìë™ ë‹¤ìš´ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì–´ ì¬ì‹œì‘ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")