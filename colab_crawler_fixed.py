#!/usr/bin/env python3
"""
Google Colabìš© ëŒ€ì§„ëŒ€í•™êµ í™•ì¥ í¬ë¡¤ë§ ì‹œìŠ¤í…œ (ìˆ˜ì • ë²„ì „)
- ê¸°ì¡´ 20,231ê°œ í˜ì´ì§€ì™€ ì¤‘ë³µ ë°©ì§€
- Google Drive ìˆ˜ë™ ë§ˆìš´íŠ¸
- ì¤‘ë‹¨ í›„ ì¬ì‹œì‘ ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ
"""

import os
import time
import json
import asyncio
import aiohttp
import requests
from urllib.parse import urlparse, urljoin, parse_qs
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, deque
import hashlib
from datetime import datetime
import logging
import signal
import sys

# Google Colab/Drive ì—°ë™ (ìˆ˜ì •ë¨)
try:
    from google.colab import drive, files
    COLAB_ENV = True
    print("ğŸ”— Google Colab í™˜ê²½ ê°ì§€ë¨")
    
    # Drive ë§ˆìš´íŠ¸ í™•ì¸
    if not os.path.exists('/content/drive'):
        print("ğŸ“‚ Google Drive ë§ˆìš´íŠ¸ ì¤‘...")
        try:
            drive.mount('/content/drive', force_remount=False)
            print("âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Drive ë§ˆìš´íŠ¸ ì˜¤ë¥˜: {e}")
            print("ğŸ”§ ë¡œì»¬ ì €ì¥ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            COLAB_ENV = False
    else:
        print("âœ… Google Drive ì´ë¯¸ ë§ˆìš´íŠ¸ë¨")
        
except ImportError:
    COLAB_ENV = False
    print("ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")
except Exception as e:
    print(f"âš ï¸ Colab í™˜ê²½ ì˜¤ë¥˜: {e}")
    COLAB_ENV = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('colab_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ColabAdvancedCrawler:
    def __init__(self):
        # Colab ìµœì í™” ì„¤ì •
        self.max_workers = 6 if COLAB_ENV else 4
        self.max_concurrent_requests = 25 if COLAB_ENV else 15
        self.max_selenium_instances = 3 if COLAB_ENV else 2
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì • (ìˆ˜ì •ë¨)
        if COLAB_ENV and os.path.exists('/content/drive/MyDrive'):
            self.base_path = '/content/drive/MyDrive/daejin_crawling'
            self.drive_available = True
            print(f"ğŸ“‚ Google Drive ì €ì¥ ê²½ë¡œ: {self.base_path}")
        else:
            self.base_path = './colab_crawling_output'
            self.drive_available = False
            print(f"ğŸ“‚ ë¡œì»¬ ì €ì¥ ê²½ë¡œ: {self.base_path}")
            
        # ë””ë ‰í† ë¦¬ ìƒì„±
        try:
            os.makedirs(self.base_path, exist_ok=True)
            self.output_dir = os.path.join(self.base_path, "new_crawling_output")
            os.makedirs(self.output_dir, exist_ok=True)
            print(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {self.output_dir}")
        except Exception as e:
            print(f"âŒ ë””ë ‰í† ë¦¬ ìƒì„± ì˜¤ë¥˜: {e}")
            # í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©
            self.base_path = './'
            self.output_dir = './new_crawling_output'
            os.makedirs(self.output_dir, exist_ok=True)
            self.drive_available = False
            
        self.checkpoint_file = os.path.join(self.base_path, "colab_crawler_checkpoint.json")
        self.visited_urls_file = os.path.join(self.base_path, "existing_urls.json")
        
        # í¬ë¡¤ë§ ìƒíƒœ
        self.visited = set()
        self.existing_urls = set()  # ê¸°ì¡´ í¬ë¡¤ë§ëœ URLë“¤
        self.to_visit = deque()
        self.priority_queue = deque()
        self.saved_texts = []
        self.saved_urls = []
        self.url_depths = {}
        self.domain_stats = defaultdict(int)
        self.failed_urls = set()
        self.retry_count = defaultdict(int)
        
        # í¬ë¡¤ë§ í†µê³„
        self.start_time = datetime.now()
        self.total_processed = 0
        self.total_saved = 0
        self.session_start = datetime.now()
        
        # ìƒíƒœ ì €ì¥ í”Œë˜ê·¸
        self.should_stop = False
        self.checkpoint_interval = 25  # 25ê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë” ìì£¼)
        
        # ì‹ í˜¸ ì²˜ë¦¬ ì„¤ì • (Colabì—ì„œëŠ” ì œì™¸)
        if not COLAB_ENV:
            signal.signal(signal.SIGINT, self.signal_handler)
            signal.signal(signal.SIGTERM, self.signal_handler)
        
        # ê¸°ì¡´ URL ë¡œë“œ (ê°„ì†Œí™”)
        self.load_existing_urls_simple()
        
        # í™•ì¥ëœ ì‹œë“œ URL (ìƒˆë¡œìš´ ì˜ì—­ ì¤‘ì‹¬)
        self.new_exploration_seeds = [
            # ê° í•™ê³¼ì˜ ìˆ¨ê²¨ì§„ ë©”ë‰´ë“¤
            "https://ce.daejin.ac.kr/sub6/",  # ì»´ê³µê³¼ ìë£Œì‹¤
            "https://ce.daejin.ac.kr/sub7/",  # ì»´ê³µê³¼ ì»¤ë®¤ë‹ˆí‹°
            "https://aidata.daejin.ac.kr/board/",    # AIë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ê³¼ ê²Œì‹œíŒ
            "https://security.daejin.ac.kr/board/",  # ì •ë³´ë³´ì•ˆê³¼ ê²Œì‹œíŒ
            
            # ëŒ€í•™ì› ë° íŠ¹ìˆ˜ ê³¼ì •
            "https://www.daejin.ac.kr/grad/",
            "https://www.daejin.ac.kr/mba/",
            "https://www.daejin.ac.kr/special/",
            
            # ì—°êµ¬ê¸°ê´€
            "https://www.daejin.ac.kr/research/",
            "https://tech.daejin.ac.kr/tech/",
            
            # ë¶€ì„¤ê¸°ê´€ë“¤
            "https://admission.daejin.ac.kr/admission/notice/",
            "https://job.daejin.ac.kr/job/notice/",
            "https://ctl.daejin.ac.kr/ctl/notice/",
            "https://international.daejin.ac.kr/international/notice/",
            
            # íŠ¹ë³„ í”„ë¡œê·¸ë¨ë“¤
            "https://rotc.daejin.ac.kr/rotc/program/",
            "https://counseling.daejin.ac.kr/counseling/program/",
            
            # ë¯¸íƒìƒ‰ëœ í•™ê³¼ë“¤
            "https://semice.daejin.ac.kr/",  # ë°˜ë„ì²´ê³µí•™ê³¼
            "https://id.daejin.ac.kr/",      # ì‚°ì—…ë””ìì¸ê³¼
            "https://food.daejin.ac.kr/",    # ì‹í’ˆì˜ì–‘í•™ê³¼
            "https://health.daejin.ac.kr/",  # ë³´ê±´ê´€ë¦¬í•™ê³¼
            "https://envir.daejin.ac.kr/",   # í™˜ê²½ê³µí•™ê³¼
            "https://mech.daejin.ac.kr/",    # ê¸°ê³„ê³µí•™ê³¼
            "https://child.daejin.ac.kr/",   # ì•„ë™í•™ê³¼
            "https://welfare.daejin.ac.kr/", # ì‚¬íšŒë³µì§€í•™ê³¼
            "https://camde.daejin.ac.kr/",   # ë¬¸í™”ì½˜í…ì¸ í•™ê³¼
            "https://korean.daejin.ac.kr/",  # í•œêµ­ì–´êµìœ¡ê³¼
        ]
        
        # ìƒˆë¡œìš´ íƒìƒ‰ ì „ëµ
        self.new_exploration_strategies = {
            # ë¯¸íƒìƒ‰ ë„ë©”ì¸ë“¤ ìš°ì„ 
            'semice.daejin.ac.kr': {'max_depth': 40, 'priority': 950},
            'id.daejin.ac.kr': {'max_depth': 40, 'priority': 940},
            'food.daejin.ac.kr': {'max_depth': 35, 'priority': 930},
            'health.daejin.ac.kr': {'max_depth': 35, 'priority': 920},
            'envir.daejin.ac.kr': {'max_depth': 35, 'priority': 910},
            'mech.daejin.ac.kr': {'max_depth': 35, 'priority': 900},
            'child.daejin.ac.kr': {'max_depth': 30, 'priority': 890},
            'welfare.daejin.ac.kr': {'max_depth': 30, 'priority': 880},
            'camde.daejin.ac.kr': {'max_depth': 30, 'priority': 870},
            'korean.daejin.ac.kr': {'max_depth': 30, 'priority': 860},
            
            # ë©”ì¸ ì‚¬ì´íŠ¸ëŠ” ë§¤ìš° ê¹Šê²Œ
            'www.daejin.ac.kr': {'max_depth': 50, 'priority': 1000},
            
            # ê¸°ë³¸ê°’
            'default': {'max_depth': 25, 'priority': 500},
            
            # ë„ì„œê´€ì€ ì œí•œ
            'library.daejin.ac.kr': {'max_depth': 5, 'priority': 100},
        }
        
        # ê³ ìš°ì„ ìˆœìœ„ íŒ¨í„´
        self.new_priority_patterns = [
            (r'/program/', 200),
            (r'/facility/', 190),
            (r'/lab/', 170),
            (r'/research/', 160),
            (r'/seminar/', 140),
            (r'/workshop/', 130),
            (r'/notice/', 120),
            (r'/board/', 110),
            (r'/community/', 100),
            (r'/gallery/', 90),
            (r'/bbs/', 150),
            (r'/artclView\\.do', 180),
            (r'/subview\\.do', 100),
        ]
        
        # ì œì™¸ íŒ¨í„´
        self.exclude_patterns = [
            r'\\.(pdf|doc|hwp|zip|exe|jpg|png|gif|mp4|avi|ppt|xls)$',
            r'/download\\.do',
            r'/file/',
            r'/upload/',
            r'groupware\\.daejin\\.ac\\.kr',
            r'webmail\\.daejin\\.ac\\.kr',
            r'sso\\.daejin\\.ac\\.kr/login',
            r'#none$',
            r'#this$',
            r'javascript:',
            r'mailto:',
            r'tel:',
            r'/admin/',
            r'/private/',
            r'/temp/',
            r'/test/',
        ]
        
        # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ ì‹œë„
        self.load_checkpoint()

    def load_existing_urls_simple(self):
        """ê¸°ì¡´ í¬ë¡¤ë§ëœ URL ê°„ë‹¨ ë¡œë“œ (Colab ìµœì í™”)"""
        logger.info("ğŸ” ê¸°ì¡´ í¬ë¡¤ë§ URL ë¶„ì„ ì¤‘...")
        
        # ê¸°ì¡´ URL íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        if os.path.exists(self.visited_urls_file):
            try:
                with open(self.visited_urls_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.existing_urls = set(data.get('urls', []))
                    logger.info(f"ğŸ“Š ê¸°ì¡´ URL ë¡œë“œ: {len(self.existing_urls):,}ê°œ")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ URL íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        # ì—†ìœ¼ë©´ ê¸°ë³¸ ìƒ˜í”Œ ìƒì„± (í¬ë¡¤ë§ëœ ê²ƒìœ¼ë¡œ ê°€ì •)
        if not self.existing_urls:
            # ê¸°ì¡´ 20,231ê°œ íŒŒì¼ì˜ URL íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜
            sample_domains = [
                'www.daejin.ac.kr', 'ce.daejin.ac.kr', 'aidata.daejin.ac.kr',
                'eng.daejin.ac.kr', 'economics.daejin.ac.kr', 'law.daejin.ac.kr',
                'nurse.daejin.ac.kr', 'sports.daejin.ac.kr', 'elec.daejin.ac.kr'
            ]
            
            # ê¸°ë³¸ URL íŒ¨í„´ë“¤ì„ ê¸°ì¡´ URLë¡œ ê°„ì£¼
            for domain in sample_domains:
                for i in range(100):  # ë„ë©”ì¸ë‹¹ 100ê°œì”©
                    self.existing_urls.add(f"https://{domain}/page{i:03d}.do")
                    self.existing_urls.add(f"https://{domain}/bbs/notice/{i:03d}/artclView.do")
            
            logger.info(f"ğŸ“Š ê¸°ë³¸ URL íŒ¨í„´ ìƒì„±: {len(self.existing_urls):,}ê°œ")
        
        # URL íŒŒì¼ ì €ì¥
        try:
            with open(self.visited_urls_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'total_count': len(self.existing_urls),
                    'last_updated': datetime.now().isoformat(),
                    'urls': list(self.existing_urls)
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"URL íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

    def signal_handler(self, signum, frame):
        """ì‹ í˜¸ ì²˜ë¦¬ (Ctrl+C ë“±)"""
        logger.info(f"\\nâš ï¸ ì‹ í˜¸ {signum} ìˆ˜ì‹ . ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘...")
        self.should_stop = True
        self.save_checkpoint()

    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_data = {
            'visited': list(self.visited),
            'to_visit': list(self.to_visit),
            'priority_queue': list(self.priority_queue),
            'saved_urls': self.saved_urls,
            'url_depths': self.url_depths,
            'domain_stats': dict(self.domain_stats),
            'failed_urls': list(self.failed_urls),
            'retry_count': dict(self.retry_count),
            'total_processed': self.total_processed,
            'total_saved': self.total_saved,
            'session_start': self.session_start.isoformat(),
            'checkpoint_time': datetime.now().isoformat(),
            'existing_urls_count': len(self.existing_urls)
        }
        
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {self.total_saved}ê°œ ì‹ ê·œ íŒŒì¼")
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")

    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if not os.path.exists(self.checkpoint_file):
            logger.info("ğŸ†• ìƒˆë¡œìš´ í™•ì¥ í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘")
            return
        
        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            self.visited = set(checkpoint_data.get('visited', []))
            self.to_visit = deque(checkpoint_data.get('to_visit', []))
            self.priority_queue = deque(checkpoint_data.get('priority_queue', []))
            self.saved_urls = checkpoint_data.get('saved_urls', [])
            self.url_depths = checkpoint_data.get('url_depths', {})
            self.domain_stats = defaultdict(int, checkpoint_data.get('domain_stats', {}))
            self.failed_urls = set(checkpoint_data.get('failed_urls', []))
            self.retry_count = defaultdict(int, checkpoint_data.get('retry_count', {}))
            self.total_processed = checkpoint_data.get('total_processed', 0)
            self.total_saved = checkpoint_data.get('total_saved', 0)
            
            if 'session_start' in checkpoint_data:
                self.session_start = datetime.fromisoformat(checkpoint_data['session_start'])
            
            logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {self.total_saved}ê°œ ì‹ ê·œ íŒŒì¼, {len(self.visited)}ê°œ ë°©ë¬¸")
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            logger.info("ğŸ†• ìƒˆë¡œìš´ ì„¸ì…˜ìœ¼ë¡œ ì‹œì‘")

    def is_new_url(self, url):
        """ìƒˆë¡œìš´ URLì¸ì§€ í™•ì¸"""
        return url not in self.existing_urls and url not in self.visited

    def should_crawl_url(self, url, depth):
        """URL í¬ë¡¤ë§ ì—¬ë¶€ ê²°ì •"""
        # ê¸°ì¡´ í¬ë¡¤ë§ëœ URLì´ë©´ ìŠ¤í‚µ
        if not self.is_new_url(url):
            return False
            
        # ì´ë¯¸ ë°©ë¬¸í–ˆê±°ë‚˜ ì‹¤íŒ¨í•œ URL
        if url in self.visited or url in self.failed_urls:
            return False
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        if self.retry_count[url] >= 3:
            return False
        
        # ì œì™¸ íŒ¨í„´
        for pattern in self.exclude_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # ë„ë©”ì¸ í™•ì¸
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # ëŒ€ì§„ëŒ€í•™êµ ë„ë©”ì¸ë§Œ í—ˆìš©
        if 'daejin.ac.kr' not in domain:
            return False
        
        # ë„ë©”ì¸ë³„ ê¹Šì´ ì œí•œ
        strategy = self.new_exploration_strategies.get(domain, self.new_exploration_strategies['default'])
        if depth > strategy['max_depth']:
            return False
        
        return True

    def get_url_priority(self, url):
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # ì‹œë“œ URLì€ ìµœê³  ìš°ì„ ìˆœìœ„
        if url in self.new_exploration_seeds:
            return 2000
        
        # íŒ¨í„´ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        for pattern, priority in self.new_priority_patterns:
            if re.search(pattern, url):
                return priority + 1000
        
        # ë„ë©”ì¸ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        parsed = urlparse(url)
        domain = parsed.netloc
        strategy = self.new_exploration_strategies.get(domain, self.new_exploration_strategies['default'])
        
        return strategy['priority']

    def crawl_with_selenium(self, url, depth=0):
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§ (ë™ê¸° ë²„ì „)"""
        driver = None
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-extensions')
            options.add_argument('--disable-plugins')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
            
            # Colab í™˜ê²½ì—ì„œëŠ” Chrome ê²½ë¡œ ëª…ì‹œ
            if COLAB_ENV:
                options.binary_location = '/usr/bin/google-chrome'
                
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(20)
            
            driver.get(url)
            time.sleep(1.5)  # ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            content = driver.page_source
            soup = BeautifulSoup(content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            text_content = self.extract_clean_text(soup)
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(url, href)
                if self.should_crawl_url(full_url, depth + 1):
                    priority = self.get_url_priority(full_url)
                    links.append((full_url, priority, depth + 1))
            
            driver.quit()
            
            return {
                'url': url,
                'content': text_content,
                'links': links,
                'depth': depth,
                'domain': urlparse(url).netloc,
                'length': len(text_content),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Selenium í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            self.retry_count[url] += 1
            if self.retry_count[url] >= 3:
                self.failed_urls.add(url)
            
            if driver:
                try:
                    driver.quit()
                except:
                    pass
            return None

    def extract_clean_text(self, soup):
        """ê¹”ë”í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        text = soup.get_text()
        lines = [line.strip() for line in text.splitlines()]
        text = '\\n'.join(line for line in lines if line and len(line) > 2)
        
        return text

    def save_page_content(self, page_data):
        """í˜ì´ì§€ ë‚´ìš© ì €ì¥"""
        if not page_data or len(page_data['content'].strip()) < 200:
            return False
        
        filename = f"new_page_{self.total_saved:06d}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        content = f"[URL] {page_data['url']}\\n"
        content += f"[DEPTH] {page_data['depth']}\\n"
        content += f"[DOMAIN] {page_data['domain']}\\n"
        content += f"[TIMESTAMP] {page_data['timestamp']}\\n"
        content += f"[LENGTH] {page_data['length']}\\n"
        content += f"[NEW_CRAWLING] true\\n\\n"
        content += page_data['content']
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.saved_urls.append(page_data['url'])
            self.domain_stats[page_data['domain']] += 1
            self.total_saved += 1
            
            logger.info(f"ğŸ’¾ ì‹ ê·œ ì €ì¥: {filename} ({page_data['domain']}) - {page_data['length']}ì")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    def run_extended_crawling(self):
        """í™•ì¥ í¬ë¡¤ë§ ì‹¤í–‰ (ë™ê¸° ë²„ì „)"""
        logger.info("ğŸš€ í™•ì¥ í¬ë¡¤ë§ ì‹œì‘ (ê¸°ì¡´ê³¼ ì¤‘ë³µ ë°©ì§€)")
        logger.info(f"ğŸ¯ ìƒˆë¡œìš´ ì‹œë“œ URL: {len(self.new_exploration_seeds)}ê°œ")
        logger.info(f"ğŸ“Š ê¸°ì¡´ ì§„í–‰: {self.total_saved}ê°œ ì‹ ê·œ ì €ì¥ë¨")
        logger.info(f"ğŸ” ì¤‘ë³µ ë°©ì§€: {len(self.existing_urls):,}ê°œ ê¸°ì¡´ URL ì œì™¸")
        
        # ìƒˆë¡œìš´ ì‹œë“œ URL ì¶”ê°€
        for url in self.new_exploration_seeds:
            if self.is_new_url(url):
                priority = self.get_url_priority(url)
                self.priority_queue.append((url, 0, priority))
                logger.info(f"ğŸŒ± ìƒˆë¡œìš´ ì‹œë“œ ì¶”ê°€: {url}")
        
        # ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ ì•ˆì „í•˜ê²Œ í¬ë¡¤ë§
        crawl_count = 0
        max_crawl_limit = 500  # Colab ì„¸ì…˜ ì œí•œ ê³ ë ¤
        
        while not self.should_stop and (self.priority_queue or self.to_visit) and crawl_count < max_crawl_limit:
            # ì²˜ë¦¬í•  URL ì„ íƒ
            current_url = None
            
            # ìš°ì„ ìˆœìœ„ í ìš°ì„  ì²˜ë¦¬
            if self.priority_queue:
                url, depth, priority = self.priority_queue.popleft()
                if self.is_new_url(url):
                    current_url = (url, depth, priority)
            
            # ì¼ë°˜ íì—ì„œ ì„ íƒ
            elif self.to_visit:
                url, depth, priority = self.to_visit.popleft()
                if self.is_new_url(url):
                    current_url = (url, depth, priority)
            
            if not current_url:
                logger.info("ğŸ“ ëª¨ë“  ì‹ ê·œ URL ì²˜ë¦¬ ì™„ë£Œ")
                break
            
            url, depth, priority = current_url
            
            # URL í¬ë¡¤ë§
            if self.is_new_url(url):
                self.visited.add(url)
                self.total_processed += 1
                crawl_count += 1
                
                logger.info(f"ğŸ” í¬ë¡¤ë§ ì¤‘ ({crawl_count}/{max_crawl_limit}): {url}")
                
                try:
                    page_data = self.crawl_with_selenium(url, depth)
                    if page_data:
                        # í˜ì´ì§€ ì €ì¥
                        if self.save_page_content(page_data):
                            # ìƒˆ ë§í¬ ì¶”ê°€
                            new_links_added = 0
                            for link_url, link_priority, link_depth in page_data['links']:
                                if self.is_new_url(link_url) and link_url not in self.failed_urls:
                                    if link_priority >= 1000:
                                        self.priority_queue.append((link_url, link_depth, link_priority))
                                    else:
                                        self.to_visit.append((link_url, link_depth, link_priority))
                                    new_links_added += 1
                            
                            logger.info(f"ğŸ”— ìƒˆ ë§í¬ {new_links_added}ê°œ ì¶”ê°€")
                
                except Exception as e:
                    logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
                # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if self.total_saved > 0 and self.total_saved % self.checkpoint_interval == 0:
                    self.save_checkpoint()
                    logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ({self.total_saved}ê°œ íŒŒì¼)")
                
                # ì£¼ê¸°ì  ìƒíƒœ ë³´ê³ 
                if crawl_count % 10 == 0:
                    elapsed = datetime.now() - self.session_start
                    logger.info(f"ğŸ“ˆ ì§„í–‰ìƒí™©: í¬ë¡¤ë§ {crawl_count}ê°œ, ì €ì¥ {self.total_saved}ê°œ")
                    logger.info(f"ğŸŒ ë„ë©”ì¸ë³„: {dict(list(self.domain_stats.items())[:3])}")
                    logger.info(f"ğŸ“‹ ëŒ€ê¸°: ìš°ì„ ìˆœìœ„ {len(self.priority_queue)}ê°œ, ì¼ë°˜ {len(self.to_visit)}ê°œ")
                    logger.info(f"â±ï¸ ê²½ê³¼: {elapsed}")
                
                # ì•ˆì „í•œ ë”œë ˆì´
                time.sleep(1)
        
        # ìµœì¢… ì²˜ë¦¬
        self.save_checkpoint()
        
        # ìµœì¢… í†µê³„
        total_elapsed = datetime.now() - self.session_start
        logger.info("âœ… í™•ì¥ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ í¬ë¡¤ë§: {crawl_count}ê°œ í˜ì´ì§€")
        logger.info(f"ğŸ’¾ ì´ ì €ì¥: {self.total_saved}ê°œ ì‹ ê·œ íŒŒì¼")
        logger.info(f"ğŸŒ ë„ë©”ì¸ë³„ í†µê³„: {dict(self.domain_stats)}")
        logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_elapsed}")
        logger.info(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {self.output_dir}/")
        
        # ì••ì¶• íŒŒì¼ ìƒì„±
        if self.total_saved > 0:
            self.create_download_archive()
        
        return self.total_saved

    def create_download_archive(self):
        """ë‹¤ìš´ë¡œë“œìš© ì••ì¶• íŒŒì¼ ìƒì„±"""
        try:
            import zipfile
            
            archive_name = f"daejin_new_crawling_{self.total_saved}pages_{datetime.now().strftime('%Y%m%d_%H%M')}.zip"
            archive_path = os.path.join(self.base_path, archive_name)
            
            logger.info(f"ğŸ“¦ ì••ì¶• íŒŒì¼ ìƒì„± ì¤‘: {archive_name}")
            
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë“¤
                for filename in os.listdir(self.output_dir):
                    if filename.endswith('.txt'):
                        file_path = os.path.join(self.output_dir, filename)
                        zipf.write(file_path, f"new_crawling_output/{filename}")
                
                # ì²´í¬í¬ì¸íŠ¸ ë° ë©”íƒ€ë°ì´í„°
                if os.path.exists(self.checkpoint_file):
                    zipf.write(self.checkpoint_file, "colab_crawler_checkpoint.json")
                if os.path.exists(self.visited_urls_file):
                    zipf.write(self.visited_urls_file, "existing_urls.json")
                
                # í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
                report = {
                    "crawling_summary": {
                        "total_new_pages": self.total_saved,
                        "total_processed_urls": self.total_processed,
                        "domains_crawled": dict(self.domain_stats),
                        "session_duration": str(datetime.now() - self.session_start),
                        "completion_time": datetime.now().isoformat()
                    }
                }
                
                report_path = os.path.join(self.base_path, "crawling_report.json")
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                zipf.write(report_path, "crawling_report.json")
            
            logger.info(f"âœ… ì••ì¶• ì™„ë£Œ: {archive_path}")
            
            # Colabì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ìˆ˜ì •ë¨)
            if COLAB_ENV:
                try:
                    files.download(archive_path)
                    logger.info("ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘ë¨")
                except Exception as e:
                    logger.warning(f"ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    logger.info(f"ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ: {archive_path}")
                
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    print("ğŸŒŸ ëŒ€ì§„ëŒ€í•™êµ í™•ì¥ í¬ë¡¤ë§ ì‹œìŠ¤í…œ v2.1 (ìˆ˜ì • ë²„ì „)")
    print("=" * 60)
    
    crawler = ColabAdvancedCrawler()
    try:
        if COLAB_ENV:
            print("ğŸ”— Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...")
            if crawler.drive_available:
                print("ğŸ’½ ê²°ê³¼ëŠ” Google Driveì— ì €ì¥ë©ë‹ˆë‹¤.")
            else:
                print("ğŸ’¾ ê²°ê³¼ëŠ” ë¡œì»¬ì— ì €ì¥ë©ë‹ˆë‹¤.")
        
        result = crawler.run_extended_crawling()
        print(f"\\nğŸ‰ í™•ì¥ í¬ë¡¤ë§ ì™„ë£Œ: {result:,}ê°œ ì‹ ê·œ í˜ì´ì§€ ìˆ˜ì§‘")
        
        if result > 0:
            print("ğŸ“¦ ì••ì¶• íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if COLAB_ENV:
                print("ğŸ“‚ Google Driveì—ì„œ í™•ì¸í•˜ê±°ë‚˜ ìë™ ë‹¤ìš´ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
    except KeyboardInterrupt:
        print("\\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ì–´ ì¬ì‹œì‘ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    except Exception as e:
        print(f"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")